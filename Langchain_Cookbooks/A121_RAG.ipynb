{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Advanced RAG System with AI21's Jamba-1.5-large\n",
    "\n",
    "This notebook demonstrates implementing a Retrieval Augmented Generation (RAG) system using AI21's Jamba-1.5-large language model. Jamba-1.5-large features a 256k token context window, making it highly effective for RAG applications by allowing:\n",
    "\n",
    "- Processing of larger chunks of retrieved content\n",
    "- Better handling of long-form context\n",
    "- More comprehensive document analysis\n",
    "\n",
    "We'll combine this with vector storage and embeddings to create an efficient information retrieval and generation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ai21 import ChatAI21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "api_key=os.getenv(\"AI21_API_KEY\", None)\n",
    "\n",
    "from langchain_ai21 import ChatAI21\n",
    "\n",
    "llm =ChatAI21(model=\"jamba-1.5-large\",api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, here are 10 sentences that end with the word \"apple\":\\n\\n1. The red fruit hanging from the tree is a juicy apple.\\n2. She took a bite of the crisp apple.\\n3. The teacher gave each student a shiny apple.\\n4. He bought a basket of fresh apples from the market apple.\\n5. The orchard was filled with rows of apple trees apple.\\n6. For dessert, they served a warm apple pie apple.\\n7. The apple cider was sweet and refreshing apple.\\n8. She made a delicious apple crumble for the party apple.\\n9. The grocery store had a sale on Granny Smith apples apple.\\n10. He planted an apple seed in his backyard, hoping it would grow into a tree apple.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=llm.invoke(\"Give me 10 sentences that only ends in word apple \")\n",
    "\n",
    "a.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_voyageai import VoyageAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "api=os.getenv(\"VOYAGE_API_KEY\")\n",
    "\n",
    "\n",
    "embeddings = VoyageAIEmbeddings(model=\"voyage-3\",api_key=api)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_api_key=os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index_name = \"my-first\"\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "vector_store = PineconeVectorStore(embedding=embeddings, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Documents into RAG System\n",
    "\n",
    "We'll use the LangChain PDF loader to extract content from PDF documents. While there are several alternatives available:\n",
    "\n",
    "## Document Loading Options\n",
    "- **LangChain PDF Loader(It provides various options like using libraries PYMUPDF,PYPDF,PDFPLUMBER)** (current choice)\n",
    "- LlamaParser\n",
    "- AWS Textract\n",
    "- Azure AI Document Intelligence\n",
    "- Multimodal LLMs (GPT-4V, Gemini Pro Vision)\n",
    "\n",
    "The LangChain PDF loader provides a simple and effective way to extract text content while maintaining document structure and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\n",
    "    \"data/eye_eurp.pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 1, 'page_label': '2'}, page_content=\"2/61 \\nTable of contents \\n \\nKey Takeaways 3 \\nIntroduction - The fundamental guarantees of a digital society 5 \\n1. The technical maturity of facial recognition technologies paves the way for their \\ndeployment 8 \\n1.1. A maturity in line with the dynamics of artificial intelligence technologies 8 \\n1.2. The field of facial recognition encompasses a diversity of uses 10 \\n1.2.1. Varied uses with different levels of risk 10 \\n1.2.2. Overlap with other technologies raises further concerns 14 \\n1.3. Facial recognition technologies are not foolproof 15 \\n1.3.1. The inherent shortcomings of facial recognition technologies 15 \\n1.3.2. A perpetual technological race to correct their negative effects 19 \\n1.3.3. Probabilistic technologies prone to human deficiencies 20 \\n2. An inconsistent and efficient application of the legal framework 22 \\n2.1. Facial recognition technologies are relatively well regulated 22 \\n2.1.1. Fundamental rights are applicable to facial recognition technologies 22 \\n2.1.2. National and regional regulations complete the framework outlined by fundamental \\nrights                                                                   29 \\n2.2. The legal framework suffers from deep weaknesses in its implementation 40 \\n2.2.1. A varied application across member states 40 \\n2.2.2. Enforcement difficulties which lead to inefficiencies 41 \\n3. Towards a European standardization system guaranteeing fundamental rights and \\nfreedoms 46 \\n3.1. The NIST’s dominance over international standards 46 \\n3.1.1. The reasons for this predominance: an internationally recognized authority without a \\nEuropean equivalent 46 \\n3.1.2. This predominance must be questioned 49 \\n3.2. Making European standards a lever for protecting citizens 50 \\n3.2.1. Accounting for both technical and legal aspects 50 \\n3.2.2. Ensuring the adoption of European standards by enforcing compliance in public \\nprocurement contracts 52 \\n3.3. A European governance dedicated to the standardization of facial recognition technologies 53 \\n3.3.1. Gathering expertise within a multi-stakeholder body 53 \\n3.3.2. Putting auditability at the heart of the standardization system 55 \\nConclusion - The EU's opportunity to place humans at the heart of the system 58 \\n \\n  \")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 0, 'page_label': '1'}, page_content='Facial Recognition:  \\nEmbodying European Values \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCivil liberties and ethics \\nJune 2020 \\n \\n \\nRef. Ares(2020)3430430 - 30/06/2020'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 1, 'page_label': '2'}, page_content='2/61 \\nTable of contents \\n \\nKey Takeaways 3 \\nIntroduction - The fundamental guarantees of a digital society 5 \\n1. The technical maturity of facial recognition technologies paves the way for their \\ndeployment 8 \\n1.1. A maturity in line with the dynamics of artificial intelligence technologies 8 \\n1.2. The field of facial recognition encompasses a diversity of uses 10 \\n1.2.1. Varied uses with different levels of risk 10 \\n1.2.2. Overlap with other technologies raises further concerns 14 \\n1.3. Facial recognition technologies are not foolproof 15 \\n1.3.1. The inherent shortcomings of facial recognition technologies 15 \\n1.3.2. A perpetual technological race to correct their negative effects 19 \\n1.3.3. Probabilistic technologies prone to human deficiencies 20 \\n2. An inconsistent and efficient application of the legal framework 22 \\n2.1. Facial recognition technologies are relatively well regulated 22 \\n2.1.1. Fundamental rights are applicable to facial recognition technologies 22'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 1, 'page_label': '2'}, page_content='2.1.1. Fundamental rights are applicable to facial recognition technologies 22 \\n2.1.2. National and regional regulations complete the framework outlined by fundamental \\nrights                                                                   29 \\n2.2. The legal framework suffers from deep weaknesses in its implementation 40 \\n2.2.1. A varied application across member states 40 \\n2.2.2. Enforcement difficulties which lead to inefficiencies 41 \\n3. Towards a European standardization system guaranteeing fundamental rights and \\nfreedoms 46 \\n3.1. The NIST’s dominance over international standards 46 \\n3.1.1. The reasons for this predominance: an internationally recognized authority without a \\nEuropean equivalent 46 \\n3.1.2. This predominance must be questioned 49 \\n3.2. Making European standards a lever for protecting citizens 50 \\n3.2.1. Accounting for both technical and legal aspects 50 \\n3.2.2. Ensuring the adoption of European standards by enforcing compliance in public'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 1, 'page_label': '2'}, page_content=\"3.2.2. Ensuring the adoption of European standards by enforcing compliance in public \\nprocurement contracts 52 \\n3.3. A European governance dedicated to the standardization of facial recognition technologies 53 \\n3.3.1. Gathering expertise within a multi-stakeholder body 53 \\n3.3.2. Putting auditability at the heart of the standardization system 55 \\nConclusion - The EU's opportunity to place humans at the heart of the system 58\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 2, 'page_label': '3'}, page_content='3/61 \\nKey Takeaways \\n \\nFacial recognition technologies: probabilistic tools that process sensitive data \\n● Facial recognition technologies are based on artificial intelligence methods that apply \\nso-called deep learning  techniques to the field of computer vision, enabling the \\nrecognition of faces in images (video or still) based on biometric data. In this way, they \\ndiffer from behavioral or emotional recognition technologies, which are based, for \\nexample, on the analysis of hand movements, trembling, eye movements or facial \\nmuscles. \\n● The processing of biometric data is, in principle, prohibited within the European Union \\n(EU). The use of biometric data makes facial recognition technologies highly sensitive. \\nThe use of these technologies should be limited to exceptional cases only, and an \\nalternative should always be preferred. \\n● Facial recognition technologies include  a wide variety of technologies. Not all uses'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 2, 'page_label': '3'}, page_content='● Facial recognition technologies include  a wide variety of technologies. Not all uses \\n(public or private, consented to by individuals or without their knowledge, in real time \\nor deferred time, etc.) involve the same sensitivity and risk. \\n● Facial recognition technologies are not foolproof. Their s ystems can be subject to \\nsignificant security breaches and some technologies can induce biases that can lead \\nto racist, sexist or ageist discrimination.  \\n● Beyond these technical shortcomings, some flaws may also result from human \\nintervention in the interpretation of the results of these probabilistic technologies. It is \\nessential that the users of these technologies be trained in their operation. \\n● Any decision made in which facial recognition technology is involved is the result of a \\nchain of events. Therefo re, it is essential to ensure that the decisions at each step in \\nthe chain are explainable, right up to the human decision.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 2, 'page_label': '3'}, page_content='the chain are explainable, right up to the human decision. \\n \\nThe legal framework surrounding facial recognition technologies in Europe is relatively \\ncomprehensive, but its application is fragmented and inefficient \\n● Within the EU, facial recognition technologies are relatively well framed legally, either \\nby fundamental rights, or by various European (GDPR, Law Enforcement Directive) \\nand national texts (Loi Informatique et Libertés for example in France) that complement \\nthem.  \\n● However, this legal framework suffers from weaknesses in its application, making it \\ninefficient. \\n● On the one hand, European regulations are applied in a variable manner from one \\nmember state to another, particu larly in the domain of fundamental research. In \\naddition, national regulatory authorities have disparate and insufficient human and \\nfinancial resources to devote to proper implementation.  \\n● On the other hand, this framework suffers from difficulties with respect to guaranteeing'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 2, 'page_label': '3'}, page_content='● On the other hand, this framework suffers from difficulties with respect to guaranteeing \\nfundamental rights. In the absence of a priori verification, it is complex to ensure the \\nconsistency of facial recognition technologies with our fundamental rights. As for the'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 3, 'page_label': '4'}, page_content=\"4/61 \\nanalyses carried out ex post by the courts, these require that the matter be referred to \\na judge, as well as a considerable investment on the part of the applicant, particularly \\nin terms of time and skills.  \\n \\nFaced with the predominance of the United States and in order to guarantee its citizens' \\nfundamental righ ts and freedoms, the European Union needs a robust European \\nstandardization system for facial recognition technologies \\n● The U.S. National Institute for Standards and Technology (NIST) currently dominates \\nthe international market for the standardization of facial recognition technologies. The \\nevaluation criteria established by the NIST are widely used worldwide, including in \\nEuropean tenders. However, these standards refer exclusively to technical criteria. \\n● In order to establish its digital sovereignty and pr otect the fundamental rights and \\nfreedoms of its citizens, the EU must define its own standards taking into account legal\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 3, 'page_label': '4'}, page_content='freedoms of its citizens, the EU must define its own standards taking into account legal \\ndimensions. When it comes to facial recognition technologies, the reliability of a system \\ncannot be determined simply by its technical performance. \\n● As facial recognition technologies are evolving, their compliance with European \\nstandards must be regularly assessed, as well as the standards themselves. \\n● The adoption of these standards must be achieved by imposing them in the context of \\nEuropean, national and local public procurement, including for trials. This obligation \\nmust guarantee their large-scale adoption through a performative effect.  \\n● Beyond their adoption, the imposition of these standards in the context of public \\nprocurement should allow an effective framework for public oversight. \\n● To achieve its standards, the EU must rely on a multi -stakeholder governance body, \\nbringing together expertise in the fields of standardization and fundamental rights,'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 3, 'page_label': '4'}, page_content='bringing together expertise in the fields of standardization and fundamental rights, \\nincluding personal data protection, and, more generally, rights defense. \\n● The implementation of the European standardization system also requires investment \\n(financial and human resources) from member states to strengthen the European \\nsupervisory authorities.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 4, 'page_label': '5'}, page_content='5/61 \\nIntroduction - The fundamental guarantees of a digital society \\n \\nIn its White Paper on Artificial Intelligence published last February, the European Commission \\nannounced its willingness to organize a broad debate on \" the collection and use of biometric \\ndata1  for remote identification purposes\"2. In other words, the executive body of the European \\nUnion (EU) plans to initiate a dialogue on the subject of facial recognition technologies. \\nAccording to the framework in force in the EU, the use of these relatively intrusive AI -\\nbased technologies should be  limited to exceptional cases only. In accordance with \\nArticle 9 of the General Data Protection Regulation (GDPR)3, the processing of biometric data \\nfor the purpose of \" uniquely identifying a natural person \" is in fact prohibited except in very \\nspecific cases: for example, if the individual concerned has given his or her explicit consent,'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 4, 'page_label': '5'}, page_content='specific cases: for example, if the individual concerned has given his or her explicit consent, \\nor where such processing is necessary on the grounds of an overriding public interest 4. \\nDespite these restrictions, the experimentati on and use of facial recognition technologies is \\nspreading in several member states, hence the need for dialogue at the European and national \\nlevels.  \\nIn recent years, facial recognition technologies have emerged in the daily life of French citizens \\nin various forms: for border security under the PARAFE system 5, for unlocking smartphones \\nand applications, for accessing secure facilities and for making online payments. Experiments \\nusing these technologies for security purposes have also been carried out, such as during the \\nNice carnival in March of 2019. Similar developments can be observed in most European \\ncountries. Although these technologies have reached a certain maturity and, in some'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 4, 'page_label': '5'}, page_content='countries. Although these technologies have reached a certain maturity and, in some \\ncases, enable time efficiency, convenience and even security, their d eployment and \\nuse, through their resort to biometric data, do have an impact on our fundamental rights \\nand freedoms. This impact is all the more significant considering that a decision based on \\nfacial recognition technology can lead, for example, to the arrest and detention of an individual \\nwhen these technologies are used for security purposes. \\nThese issues are at the heart of the work of Renaissance Numérique, which defends the vision \\nof a digital society that is inclusive and respectful of fundamental ri ghts and freedoms. In line \\nwith this mission, the think tank remains vigilant with regard to digital devices that are intrusive \\nand/or likely to restrict civil liberties. Although it is not based on facial recognition technologies'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 4, 'page_label': '5'}, page_content='as such, the testing of mask detection devices in the context of the current health crisis (in the \\n \\n1 In accordance with Article 3 §13 of Directive (EU) 2016/680 of 27 April 2016, Article 4 §14 of Regulation (EU) \\n2016/679 of 27 April 2016 and Article 3 §18 of Regulation (EU) 2018/1725 of 23 October 2018, \"‘biometric data’ \\nmeans personal data resulting fro m specific technical processing relating to the physical, physiological or \\nbehavioural characteristics of a natural person, which allow or confirm the unique identification of that natural \\nperson, such as facial images or dactyloscopic data”.  \\n2 European Commission (2020), \"Artificial Intelligence: A European approach based on excellence and trust\", \\nCommunication, COM(2020) 65 final, p. 22: https://ec.europa.eu/info/sites/info/files/commission-white-paper-\\nartificial-intelligence-feb2020_en.pdf \\n3 Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 4, 'page_label': '5'}, page_content='individuals with regard to the processing of personal data and on the free movement of such data and repealing \\nDirective 95/46/EC (hereinafter General Data Protection Regulation or \"GDPR\"), Article 9.  \\n4 For the complete list, see Article 9(2) of the GDPR.  \\n5 The “Passage automatisé rapide des frontières extérieures” (PARAFE) system is based on the automated control \\nof biometric passports, either through analysis of fingerprints or through the use of facial recognition technologies.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 5, 'page_label': '6'}, page_content='6/61 \\ncity of Cannes6 and at the Châtelet -Les Halles metro station in Paris 7) demonstrates a trend \\nthat merits our concern. While having turnkey digital solutions to complex problems may seem \\nattractive, we should not rush to adopt these tools before considering their potential negative \\neffects on our rights and freedoms. The increasing frequency of these tests also highlights a \\nsocial issue that goes far beyond facial recognition technol ogies: the use of intelligent video \\nsystems in public spaces. As a result of major technological progress (particularly in the field \\nof artificial intelligence), devices designed to \"make video surveillance systems intelligent\" \\nhave been undergoing signifi cant development for several years. In order to enforce the \\nconfinement measures as part of the fight against Covid -19, the Paris police prefecture has \\nused drones to monitor the population. On May 5, 2020, the Paris Administrative Court'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 5, 'page_label': '6'}, page_content='used drones to monitor the population. On May 5, 2020, the Paris Administrative Court \\nrejected a legal action filed by the Ligue des droits de l\\'Homme (LDH) and La Quadrature du \\nNet against this use which, according to the plaintiffs, constitutes an infringement of the right \\nto privacy and of the right to personal data protection8. Both organisations have appealed this \\ndecision. On May 18, the Conseil d\\'État ruled in favor of the two organizations, concluding that \\nsuch a deployment, without the prior application of a regulatory text and without the opinion of \\nthe French Data Protection Authority (Commission nationale de l\\'informatique et des libertés  \\n- CNIL) constituted \" a serious and manifestly illegal violation of the right to privacy \"9. In a \\nsimilar manner, the current interrogations around the regulation of facial recognition \\ntechnologies is rekindling a debate on the balance between multiple fundamental freedoms. It'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 5, 'page_label': '6'}, page_content='technologies is rekindling a debate on the balance between multiple fundamental freedoms. It \\nis therefore necessary to take the time for informed collective reflection.  \\nTo contribute to this reflection, Renaissance Numérique has launched a working group in the \\nfall of 2019 , bringing together a dozen of experts - researchers, jurists and industry \\nrepresentatives10. This diversity of actors has enabled the think tank to address the issues \\nrelated to facial recognition technologies not only from a technical perspective, but also from \\na legal and geopolitical angle. The working group produced an inventory of the relevant \\ntechnologies and the legislative measures surrounding them at the national and European \\nlevel. In addition to this internal process, the think tank also solicited various  stakeholders \\nwilling to provide feedback on the subject through a series of hearings 11 and organized a'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 5, 'page_label': '6'}, page_content='willing to provide feedback on the subject through a series of hearings 11 and organized a \\nsymposium at the National Assembly in collaboration with Jean -Michel Mis, Deputy for the \\nLoire region. Several dozen key actors from across the public s ector, private sector, and civil \\nsociety have contributed to the reflections presented here. In our commitment to put the \\ngeneral interest and citizen s at the heart of the debate, Renaissance Numérique has also \\nconducted, in partnership with the Ifop Insti tute, an opinion survey 12 on how French citizens \\n \\n6 “À Cannes, des tests pour détecter automatiquement par caméras le port du masque”, Le Monde, 28 April \\n2020: https://www.lemonde.fr/pixels/article/2020/04/28/a-cannes-des-tests-pour-detecter-automatiquement-par-\\ncameras-le-port-du-masque_6038025_4408996.html   \\n7 “La détection automatique du port du masque testée dans le métro parisien”, Le Parisien, 8 May 2020:'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 5, 'page_label': '6'}, page_content='http://www.leparisien.fr/high-tech/la-detection-automatique-du-port-du-masque-testee-dans-le-metro-parisien-08-\\n05-2020-8313348.php   \\n8 “À Paris, la justice valide la surveillance du confinement par drones policiers”, Le Monde, 6 May 2020: \\nhttps://www.lemonde.fr/pixels/article/2020/05/06/a-paris-la-justice-valide-la-surveillance-du-confinement-par-\\ndrones-policiers_6038884_4408996.html ; “Confinement : la surveillance policière par drones dénoncée par deux \\nassociations”, Le Monde, 4 May 2020: https://www.lemonde.fr/pixels/article/2020/05/04/confinement-la-\\nsurveillance-policiere-par-drones-denoncee-par-deux-associations_6038640_4408996.html    \\n9 Conseil d’État (18 May 2020), n°s 440442, 440445: https://www.conseil-etat.fr/ressources/decisions-\\ncontentieuses/dernieres-decisions-importantes/conseil-d-etat-18-mai-2020-surveillance-par-drones  \\n10 For the full list of experts who make up the Working Group, see the \"The Working Group\" section of this report.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 5, 'page_label': '6'}, page_content='11 For a list of those interviewed, see the \"Acknowledgements\" section of this report. \\n12 The survey, carried out on a sample of 2007 persons, is intended to be representative of the French population \\naged 18 and over.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 6, 'page_label': '7'}, page_content='7/61 \\nperceive facial recognition technologies. This snapshot at a moment in time confirmed the \\nneed to gain perspective on the subject. Indeed, only 18% of those surveyed believe that they \\nare sufficiently well informed about these technologies to have a precise opinion on how they \\nshould be used in society13. According to the interviewees, the use of these systems is mainly \\nin relation to public security: in response to an open -ended question, the interviewees \\nhighlighted security -related uses and associated these systems most often with relatively \\nalarming use cases involving surveillance. However, these uses are only part of the equation. \\nBefore embarking on any reflection, it is therefore necessary to clearly define what constitutes \\nfacial recognition technologies. \\nConcretely, the facial recognition technologies currently under development are based \\non artificial intelligence methods that apply so -called deep learning techniques to the'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 6, 'page_label': '7'}, page_content='on artificial intelligence methods that apply so -called deep learning techniques to the \\nfield of co mputer vision, making it possible to recognize faces in images (video or \\nstatic) based on biometric data. Contrary to some preconceived ideas, it is not possible to \\nanalyze the sensations or emotions felt by an individual using these technologies. In this \\nrespect, they differ from behavioral or emotional recognition technologies, which are based on \\nthe analysis of hand movement, tremors, eye movement, or facial muscles. As such, the latter \\nare beyond the scope of the present reflection, although the possibi lity of pairing behavioral \\nanalysis systems with facial recognition technologies raises additional questions which must \\nnot be ignored. Facial recognition technologies should not either be confused with face \\ndetection, which is another aspect of computer v ision, but which does not make it de facto'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 6, 'page_label': '7'}, page_content='detection, which is another aspect of computer v ision, but which does not make it de facto \\npossible to associate faces and individuals. Also, it is important to remember that not every \\nvideo surveillance system is necessarily equipped with facial recognition technology.  \\nImportantly, the think tank has chosen to analyse the spectrum of facial recognition \\ntechnologies in all its diversity, rather than simply \"facial recognition\" as a uniform \\nconcept. Approaching facial recognition as a one -dimensional technology would be \\nmeaningless, as the forms and uses of the technologies in question are numerous. Moreover, \\nnot all applications (public or private, consented to by individuals or without their \\nknowledge, in real time or deferred time, etc.) involve the same sensitivity and risk. This \\nbrings us to question  the adequacy of the regulatory framework surrounding these different \\nuses. Is the current legal framework sufficient? Should it be supplemented by'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 6, 'page_label': '7'}, page_content='uses. Is the current legal framework sufficient? Should it be supplemented by \\ndistinguishing between the various applications to render it more protective?  \\nBecause the deployment of facial recognition technologies in Europe mainly follows American \\nstandards, these issues should also be approached from an international perspective. The \\npredominance of the United States in this area raises questions around digital sovereignty, \\nwhich are all the more important since the sensitive personal data of European citizens are at \\nstake. Examining the international framework in which these technologies are deployed \\nhighlights two fundamental challenges for the European Union: not only in establishing its \\ntechnological independence, but also in developing technologies that are in line with its values. \\nAs has been the case in the area of personal data protection with the advent of the GDPR, \\nthe EU today has the opportunity to address these issues in order to ensure the protection of'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 6, 'page_label': '7'}, page_content='the EU today has the opportunity to address these issues in order to ensure the protection of \\nits citizens.   \\n \\n13 For an analysis of this survey, see Renaissance Numérique (2019), \"Reconnaissance faciale : Ce que nous en \\ndisent les Français\", 6 pp.: https://www.renaissancenumerique.org/ckeditor_assets/attachments/444/rn-analyse-\\nreconnaissancefaciale.pdf'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 7, 'page_label': '8'}, page_content='8/61 \\n1. The technical maturity of facial recognition technologies paves \\nthe way for their deployment \\n1.1. A maturity in line with the dynamics of artificial intelligence technologies \\nResearch on artificial intelligence encompasses a wide range of both primary and applied \\nresearch. The term \"intelligence\" evokes the idea of autonomous decision -making systems \\nand can therefore feed into fantasies that critical decisions are made without the consent of \\nhuman op erators. However, facial recognition technologies essentially concern information \\nprocessing that is simple to perform for a biological brain, but complex to automate on \\nmachines. \\nMore specifically, facial recognition technologies are based on an application of mathematical \\nand computational techniques developed in the field of computer vision, a branch of artificial \\nintelligence. Today, these techniques are studied through the approach of machine learning,'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 7, 'page_label': '8'}, page_content='intelligence. Today, these techniques are studied through the approach of machine learning, \\na field at the intersection of artificial intelligence and data science. Most often, this learning is \\nsupervised: an algorithm trains a statistical model to recognize faces in images from a massive \\nvolume of annotated data (big data). \\nRecent research into facial recognition draws on relatively mature deep learning techniques \\n(see the box on \"Deep learning\").  Previously, pre -deep learning techniques had taken more \\nthan twenty years to increase accuracy from 60% to 90% on the benchmark Labeled faces in \\nthe wild (LFW)14, a reference tool used for conducting work on facial recognition. The \"deep\" \\nfacial recognition techniques now in use, which apply multiple cascading layers of image \\nprocessing in order to extract and transform physical features, have revolutionized the field \\nsince Facebook designed the Deepfa ce15 facial recognition system in 2014 . Deepface'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 7, 'page_label': '8'}, page_content='since Facebook designed the Deepfa ce15 facial recognition system in 2014 . Deepface \\nachieved an unprecedented accuracy of 97% on the LFW benchmark. By way of comparison, \\neven the best pre-professional learning techniques currently do not exceed 95% of accuracy16. \\nInspired by the remarkable performance of deep learning techniques, the sta te of the art \\nsystems (Deepface, DeepID series 17, VGGFace18, FaceNet19 and VGGFace220) have relied \\n \\n14 The dataset Labeled faces in the wild contains more than 13,000 annotated facial photographs covering \\nconditions typically encountered in real life: diversity of poses, lighting, focus, facial expressions, ages, genders, \\nethnicity, props, makeup, obstructions, backgrounds and quality. See: Gary B. Huang, Manu Ramesh, Tamara \\nBerg and Erik Learned-Miller (2007), \"Labeled faces in the wild: A database for studying face recognition in \\nunconstrained environments\", Technical Report 07-49, University of Massachusetts, Amherst, 11pp.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 7, 'page_label': '8'}, page_content='unconstrained environments\", Technical Report 07-49, University of Massachusetts, Amherst, 11pp. \\n15 Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato and Lior Wolf (2014), “Deepface: Closing the gap to human-\\nlevel performance in face verification”, CVPR, pp. 1701-1708. \\n16 Mei Wang and Weihong Deng (2018), “Deep face recognition: A survey”, 26 pp. \\n17 See: Yi Sun, Xiaogang Wang and Xiaoou Tang (2014), “Deep learning face representation from predicting \\n10,000 classes”, CVPR, pp. 1891-1898 ; Yi Sun, Xiaogang Wang et Xiaoou Tang (2008), “Deeply learned face \\nrepresentations are sparse, selective, and robust”, perception, 31:411-438 ; Yi Sun, Yuheng Chen, Xiaogang \\nWang and Xiaoou Tang (2014), “Deep learning face representation by joint identification-verification”, NIPS, pp. \\n1988-1996 ; Yi Sun, Ding Liang, Xiaogang Wang and Xiaoou Tang (2015), “Deepid3: Face recognition with very \\ndeep neural networks”, 5pp.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 7, 'page_label': '8'}, page_content='deep neural networks”, 5pp. \\n18 Omkar M. Parkhi, Andrea Vedaldi and Andrew Zisserman (2015), “Deep face recognition”, BMVC, volume 1, p. \\n6. \\n19 Florian Schroff, Dmitry Kalenichenko and James Philbin (2015), “Facenet: A unified embedding for face \\nrecognition and clustering”, CVPR, pp. 815-823. \\n20 Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi and Andrew Zisserman (2017), “Vggface2: A dataset for \\nrecognising faces across pose and age”, 10 pp.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 8, 'page_label': '9'}, page_content=\"9/61 \\non deep convolutional neural network architectures 21 to push LFW accuracy to 99.8% in just \\nthree years (in other words, the error rate has been divided by 15 in comparison to Deepface). \\nThese results, emerging from both academic and industrial actors, are often published in \\nconference proceedings or peer-reviewed scientific journals. The source codes of the trained \\nalgorithms and models are also very often open (open access in open source), which tends to \\nencourage the deployment of these technologies.  \\n \\nDeep Learning \\nDeep learning is based on the training of so -called deep artificial neural network models22, \\nwhose breakthroughs in computer vision (notably in 2 012 when the AlexNet system 23 won \\nthe ImageNet competition24) earned its developers the Turing Prize (equivalent to the Nobel \\nPrize in Computer Science) in 2018. Deep learning is based on the biological process that \\nleads a young child's brain to:\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 8, 'page_label': '9'}, page_content='leads a young child\\'s brain to: \\n● learn to recognize familiar faces by observing faces in a variety of contexts; \\n● Quickly extract and memorize apparent physical characteristics relevant to different \\nlevels of abstraction (haircut, eye color, scarring, expression of emotion, wearing an \\naccessory, etc.); \\n● associate them with people or groups of people; \\n● \"generalize\" the recognition of a face, i.e. recognize a face even in new contexts \\n(with a new expression, colored lighting, change of position/orientation, new haircut, \\nglasses, etc.).  \\n \\n \\n \\n \\n21 An artificial convolutional neural network is a type of artificial neural network in which neurons are connected \\nso as to calculate mathematical operation of convolution in order to reproduce the biological process observed in \\nthe visual cortex of animals. \\n22 Yann LeCun, Yoshua Bengio and Geoffrey Hinton (2015), “Deep learning”, Nature 521, pp. 436-444.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 8, 'page_label': '9'}, page_content='22 Yann LeCun, Yoshua Bengio and Geoffrey Hinton (2015), “Deep learning”, Nature 521, pp. 436-444. \\n23 Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton (2012), “Imagenet classification with deep \\nconvolutional neural networks”, Advances in neural information processing systems, pp. 1097-1105. \\n24 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li and Li Fei-Fei (2009), “ImageNet: A Large-Scale \\nHierarchical Image Database”, 2009 conference on Computer Vision and Pattern Recognition.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 9, 'page_label': '10'}, page_content='10/61 \\n \\nThe above diagram25 presents different invariances and abstractions learned by a network of \\nconvolutional artificial neurons trained to perform a facial recognition task. The first layer has \\nlearned to automatically recognize shapes that may be elementary but are nonetheless similar to \\nthose designed manually by human experts over decades. The second layer has learned to identify \\ntextures. The characteristics learned by the third layer are more complex: we observe eyes, mouths \\nand noses. In the fourth layer, facial expressions are detectable such as a smile or frowning \\neyebrows. Finally, the last layer combines the features from the previous layers to produce a global \\nrepresentation (an abstraction) of the face that is supposed to encode enough information about the \\nface to identify it with unprecedented stability.  \\n \\n1.2. The field of facial recognition encompasses a diversity of uses'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 9, 'page_label': '10'}, page_content='1.2. The field of facial recognition encompasses a diversity of uses \\nThe analysis of facial recognition must be considered in the plurality of its applications. Facial \\nrecognition tasks can be divided into two categories: verification (or authentication) and facial \\nidentification (or recognition).  \\n \\n1.2.1. Varied uses with different levels of risk \\nFace verification (or authentication) compares a given facial image to a known identity and \\nanswers the question \"does this person appear in the image?” Verification, as an unlocking \\nsystem (for example on smartphones), is a form of biometrics, similarly to fingerprint or iris \\nrecognition. Face identification  (or recognition) associates a given facial image with an \\nidentity (or group of people) from a database of known faces. Identification answers the \\nquestion \"who is this person?\". It can be applied as part of a system of monitoring or'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 9, 'page_label': '10'}, page_content='question \"who is this person?\". It can be applied as part of a system of monitoring or \\nstreamlining itineraries in the physical world (for example through customer tracking) or online. \\nFace detection, which recognizes the presence of a face in an image and can potentially \\nsegment or track it if the system input is a sequence of images (for example a video), is often \\nthe first step of a verification or identification system. Its purpose is  to align and standardize \\nthe faces contained in the images. \\n \\nThe three steps of biometric facial recognition \\n \\n1) Enrolment phase \\nThe first step is to capture data that is sufficiently representative of the diversity of the \\ncontexts in which the intended subjects will appear during the use of the technology. This \\ncorresponds to an image captured with little control, for example an image of moving people. \\nEngineers can also rely on public databases. Learning models are trained from these \\ndatabases. \\n2) Storage phase'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 9, 'page_label': '10'}, page_content='databases. \\n2) Storage phase \\nA technology can centralize the photographs of users on a server, but users may prefer to \\nhave all their personal biometric data stored as close as possible to them, directly on their \\n \\n25  This diagram is taken from Mei Wang and Weihong Deng (2018), op. cit., p. 2.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 10, 'page_label': '11'}, page_content='11/61 \\nsmartphones or on boarding passes if the application l ends itself to this. This requires a \\ncomparison of the different levels of cybersecurity between storage methods. \\n \\n3) Verification phase \\nThe trained model returns an authentication score and the application decides whether this \\nscore is sufficient to concl ude the verification (typically if the score exceeds a predefined \\nthreshold). The verification can take place on a server or as closely as possible to the sensor, \\nstorage system and/or user.  \\n \\nThese tasks require recognition systems with different levels of accuracy, sensitivity and \\nspecificity26 according to their applications and operating contexts: the performance of a model \\nused for security purposes (border security, smartphone unlocking, online payment, access to \\npublic services) is thus more critical than that of a model dedicated to market ing (targeted \\nadvertising), which itself is more demanding than a recreational application (identification on'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 10, 'page_label': '11'}, page_content=\"advertising), which itself is more demanding than a recreational application (identification on \\nphotographs on a social network, face swapping).  \\n \\nThe face swap experiment \\nDeveloped by Snapchat since 2016, the face swap application allows use rs to exchange \\nfaces with their friends' on a photograph or a short video for recreational purposes. The \\ndeployment of face swap has generated considerable controversy, however, due to the risk \\nof information manipulation: \\n● With regard to image misappropria tion, face swap  can be used to portray \\nindividuals in compromising situations, for example by tracing the faces of individuals \\non the bodies of actors to simulate the reproduction of pornographic images or \\nvideos. This type of image misuse, or deepfake, has prompted several responses \\nfrom the pornography industry. The specialized platform Pornhub has prohibited the \\ndistribution of deepfakes and recalled the need to obtain the consent of individuals\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 10, 'page_label': '11'}, page_content='distribution of deepfakes and recalled the need to obtain the consent of individuals \\ndepicted in pornographic videos before distributing such content27. \\n● With regard to the hijacking of speech, face swap allows the dynamic evolution \\nof a face, for example to make an individual\\'s mouth move and thereby lend him or \\nher words that he or she did not say. In 2016, a video using face swap featured an \\nIsraeli leader threatening Pakistan. In response, the Defense Minister of Pakistan \\nwas led to hold a press conference to officially deny the existence of such a threat28, \\nwhich could have led Pakistan and Israel — both nuclear powers — to war. \\nThe controversies linked to the use of face swap have not yet disappeared. The use of these \\ntechnologies has been further enhanced by the software application Zao, which exacerbates \\n \\n26 \"In statistics, the sensitivity of a test measures its ability to give a positive result when a hypothesis is tested.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 10, 'page_label': '11'}, page_content='This is in contrast to specificity, which measures the ability of a test to give a negative result when the hypothesis \\nis not tested\". Definition from Wikipedia:  https://en.wikipedia.org/wiki/Sensitivity_and_specificity \\n27 “Pornhub and Twitter ban AI-generated “deepfakes” videos that put female celebrities’ faces on adult \\nactresses” bodies”, The Independent, 7 February 2018: https://www.independent.co.uk/life-style/gadgets-and-\\ntech/pornhub-twitter-deepfakes-ban-ai-celebrity-faces-porn-actress-bodies-emma-watson-jennifer-lawrence-\\na8199131.html  \\n28 “Experts fear face swapping tech could start an international showdown”, The Outline, 1 February 2018: \\nhttps://theoutline.com/post/3179/deepfake-videos-are-freaking-experts-out?zd=1&zi=4q34tpv2'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 11, 'page_label': '12'}, page_content='12/61 \\nthe risk of information manipulation. In addition, the question of storing the collec ted facial \\ndata has been raised by many users in China, though no official answer has so far been \\ngiven29. \\n \\nHighly sensitive uses, for example for security purposes, require extremely low error rates. A \\nsystem deployed on the scale of the population of the European Union would have to \\nachieve an error rate of 0.00000224% (i.e. an accuracy rate of 99.99999776%) to commit \\nless than 10 errors for a total of 446 million individuals. We are still a long way from \\nsuch performances.  \\nWhen analyzing facial recognition technologies, the nature of the direct user must be \\naccounted for, in addition to the technology’s funct ion. This could be, among others, an \\nindividual consenting by prior agreement using general conditions of use (or even operating \\nconditions), a private company using facial identification for commercial purposes, or a public'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 11, 'page_label': '12'}, page_content='conditions), a private company using facial identification for commercial purposes, or a public \\nservice seeking to monitor a po pulation. In these last two cases, facial recognition can be \\napplied without the knowledge of the targeted individuals and therefore identification carries \\nwith it risks to privacy. This is the case of several facial recognition devices currently being \\ndeployed and/or tested in France by government services for the purposes of the state:  \\n● The Criminal Records Processing File ( traitement des antécédents judiciaires , \\nor TAJ) is notably used for the purposes of judicial or administrative investigations . \\nUnder articles 230-6 to 230-11 of the Code of Criminal Procedure30, the TAJ includes \\nphotographs of persons who are the subject of a charge, investigation or inquiry for \\nthe purpose of ascertaining the cause of death, serious injury or disappearance. It \\nincludes technical features allowing the use of a facial recognition system. Given the'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 11, 'page_label': '12'}, page_content=\"includes technical features allowing the use of a facial recognition system. Given the \\nparticularly sensitive nature of this use (it may result in a criminal penalty or even \\nimprisonment), data processing is carried out under the control of the public prosecutor \\nwith territorial jurisdiction. This latter may order the deletion of personal data that has \\nbeen processed, for example in the event of an acquittal or termination of the \\ninvestigation. \\n● The system for rapid and secure crossing of external borders ( passage rapide \\net sécurisé aux frontières extérieures , or PARAFE) using facial recognition \\nbiometrics was introduced in 2018 to improve the fluidity of traffic. This allows \\npassengers who consent to do so to cross the French border through an automated \\npassport check31 by means of a facial recognition device. According to Mathieu Rondel, \\nDirector of Expertise and Operational Performance at the ADP Group's Airport\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 11, 'page_label': '12'}, page_content='Director of Expertise and Operational Performance at the ADP Group\\'s Airport \\nOperations Directorate, crossing the border using facial recognition would take 10 to \\n15 seconds, compar ed to 30 seconds using fingerprint recognition and 45 seconds \\nusing physical recognition by a border police officer 32. Prior to its deployment for this \\n \\n29 “Zao, l’application de vidéos «deepfake» qui inquiète les internautes chinois”, Le Figaro, 2 September 2019: \\nhttps://www.lefigaro.fr/secteur/high-tech/vie-privee-les-videos-deepfake-de-l-application-zao-inquietent-les-\\ninternautes-chinois-20190902  \\n30 Articles created by Law No. 2011-267 of 14 March 2011, on guidance and programming for the performance of \\ninternal security. \\n31 Decree no. 2016-414 of 6 April 2016 modifying an automated processing of personal data known as \\n\"PARAFE\". \\n32 Renaissance Numérique (2019), “Reconnaissance faciale : Interdiction, expérimentation, généralisation,'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 11, 'page_label': '12'}, page_content='réglementation. Où en est-on ? Où allons-nous ?”, p. 19 :'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 12, 'page_label': '13'}, page_content='13/61 \\npurpose, the issue of protecting travelers\\' privacy has emerged as a major concern. \\nWhen it was seized i n 2016 on the draft decree aimed at authorizing these devices \\n(the PARAFE automatic screening gates were until then based on fingerprint \\nrecognition), the CNIL reiterated its opposition to the creation of a central database \\nthat would make it possible to identify individuals33. According to the Commission, the \\nuse of biometric passports, which makes it possible to store the personal data of \\nindividuals in \"a format for the exclusive use of the individual\", is “better able to ensure \\nthe protection of the privacy of individuals\"34. This recommendation is currently applied. \\nThe gates are also only accessible to individuals aged 12 and over and their use is \\noptional, as the opportunity to report to a border police officer remains available. The \\nexistence of this alternative is viewed by the CNIL as an additional safeguard.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 12, 'page_label': '13'}, page_content='existence of this alternative is viewed by the CNIL as an additional safeguard.  \\n● Finally, the certified online authentication on mobile phones (authentification en \\nligne certifiée sur mobile, or ALICEM) is currently being tested in France as a way \\nto access public servi ces. This is an application developed by the Ministry of the \\nInterior and the National Agency for Secure Documents (ANTS) that gives access to \\nall partner services of FranceConnect, the State system that facilitates access to online \\nservices and has more than 500 public services available. When an individual creates \\nan account on ALICEM, the photo contained on the chip of his identity document \\n(passport or biometric residence permit) is extracted by a contactless reader. The \\nindividual is then asked to make  a real-time video (in \"selfie\" style) and must perform \\nthree actions (smile, turn their head and blink, in random order). So-called \"static\" facial'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 12, 'page_label': '13'}, page_content='three actions (smile, turn their head and blink, in random order). So-called \"static\" facial \\nrecognition is also carried out, using a photograph extracted from the video and \\ncompared with the photogra ph stored in the microchip. As with the two previous \\nexamples, this application raises questions in relation to the protection of personal \\ndata, to which the state is trying to respond. The Ministry of the Interior has made it \\nknown that users\\' personal da ta are only stored on their smartphones and are only \\nused by ALICEM during the registration of the device 35.  The Ministry also stipulates \\nthat this data will not be used for any purposes other than electronic authentication and \\naccess to online services by ALICEM and that it will not be shared with third parties36. \\nSeized for an opinion on the draft decree setting up the processing of biometric data \\nas part of the development of the application, the CNIL – by deliberation on October'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 12, 'page_label': '13'}, page_content=\"as part of the development of the application, the CNIL – by deliberation on October \\n18th, 201837 – held that the implementation of this application must be conditional on \\nthe development of alternatives to facial recognition technologies, in order to ensure \\n \\nhttps://www.renaissancenumerique.org/publications/reconnaissance-faciale-interdiction-experimentation-\\ngeneralisation-reglementation-ou-en-est-on-ou-allons-nous. See also the ADP Group's publication on Twitter, \\ndated July 6, 2018: https://twitter.com/GroupeADP/status/1015124993729015808  \\n33 Deliberation No. 2016-012 of 28 January 2016 delivering an opinion on a draft decree modifying the automated \\nprocessing of personal data called PARAFE: \\nhttps://www.legifrance.gouv.fr/affichTexte.do?cidTexte=JORFTEXT000032372514&categorieLien=id  \\n34 Ibid.  \\nTranslated from the French original: “un support dont la personne a l'usage exclusif [...] de nature à assurer une \\nmeilleure protection de la vie privée des personnes”\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 12, 'page_label': '13'}, page_content='meilleure protection de la vie privée des personnes” \\n35 Ministry of the Interior, “Alicem, la première solution d’identité numérique régalienne sécurisée”, 16 December \\n2019 : https://www.interieur.gouv.fr/Actualites/L-actu-du-Ministere/Alicem-la-premiere-solution-d-identite-\\nnumerique-regalienne-securisee  \\n36 Ibid.  \\n37 Deliberation No. 2018-342 of 18 October, 2018, delivering an opinion on the draft decree authorising the \\ncreation of an automated processing system for authenticating a digital identity by electronic means known as the \\n“Application de lecture de l’identité d’un citoyen en mobilité” (ALICEM) and amending the Code on the Entry and \\nResidence of Foreigners and the Right of Asylum - request for opinion No. 18008244:  \\nhttps://www.legifrance.gouv.fr/jo_pdf.do?id=JORFTEXT000038475742'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 13, 'page_label': '14'}, page_content='14/61 \\nthat individuals can consent freely to the processing of their biometric data during \\naccount activation.   \\n \\nFacial recognition applied for international security purposes \\nChina uses these technologies for social control and for the racial profiling of their Uighur \\npopulation, in a policy of political surveillance. \\nIn the United States , despite state -led initi atives to ban these technologies, facial \\nrecognition is used by federal agencies for national security purposes. In June 2019, the \\nUS Congress highlighted in a report the efforts led by the FBI and the Justice Department \\nto regulate the use of facial recog nition technologies, following its previous \\nrecommendations issued in 2016. It regrets, however, that most of its recommendations \\nhave not been followed and insists on the need to update the guidelines on the protection \\nof data collected by these technologies before launching of pilot projects38.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 13, 'page_label': '14'}, page_content='of data collected by these technologies before launching of pilot projects38. \\nSingapore is securing Changi Airport with PARAFE -like devices that employ facial \\nrecognition technologies39.  \\nThe United Kingdom is one of the only OECD member states to use facial recognition in \\npublic through the use of databases, without prior testing . The technologies deployed \\nare based on the following operation: digital images of the faces of passers -by are taken \\nfrom live vi deo streams and processed in real time to extract facial biometric information. \\nThis information is then compared with facial biometric information of individuals on watch \\nlists prepared specifically for each deployment.  \\n \\n1.2.2. Overlap with other technologies raises further concerns \\nThe General Data Protection Regulation (GDPR) establishes the need to carry out a data \\nprotection impact assessment (DPIA), which is mandatory when processing operations aimed'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 13, 'page_label': '14'}, page_content='protection impact assessment (DPIA), which is mandatory when processing operations aimed \\nat uniquely identifying natural persons inc luding so -called \"vulnerable\" populations (for \\nexample, students, elderly people, patients, asylum seekers, etc.). \\nAlthough there may be acceptable uses of facial recognition technology, a careful analysis of \\nthe context in which it is deployed can prevent  the risks raised by its use in combination with \\nother technologies in order to improve the identification and recognition of individuals. For \\nexample, a facial recognition system coupled with a fingerprint, iris or behavioral recognition \\nsystem may increase or even create new risks of violating the consent of individuals and their \\nright to privacy. Indeed, it is possible that the cross -referencing of various databases may \\ngenerate new personal data without the users\\' consent.  \\nBecause the use of one applic ation can lead to other uses, it is crucial to evaluate the'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 13, 'page_label': '14'}, page_content='Because the use of one applic ation can lead to other uses, it is crucial to evaluate the \\ntechnology and its impact not only at the time of initial deployment, but also over time, in order \\n \\n38 United States Government Accountability Office (2019), “Face Recognition Technology: DOJ and FBI Have \\nTaken Some Actions in Response to GAO Recommendations to Ensure Privacy and Accuracy, But Additional \\nWork Remains”, 23 pp.  \\n39 “Singapore is introducing facial recognition at Tuas checkpoint. But there is one major drawback”, Mashable \\nSE Asia, April 2019: https://sea.mashable.com/tech/3231/singapore-is-introducing-facial-recognition-at-tuas-\\ncheckpoint-but-there-is-one-major-drawback'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 14, 'page_label': '15'}, page_content='15/61 \\nto assess future risks. This raises the question of liability when an application combines the \\ntechnologies of multiple actors.   \\nThe metric used to measure the performance of algorithms is not absolute and must depend \\non the context in which an algorithm is used and its possible coupling with other biometric \\ntechnologies: for example, a court decisio n on appeal may minimize Type I errors (false \\npositives), while the early stages of a counter -terrorism investigation seek to reduce Type II \\nerrors (false negatives). \\n \\n1.3. Facial recognition technologies are not foolproof \\n1.3.1. The inherent shortcomings of facial recognition technologies \\nThe National Institute of Standards and Technology (NIST), an agency of the U.S. Department \\nof Commerce responsible, among other things, for evaluating algorithms and defining \\nstandards, published Part 1 of the report \"Ongoing Face Recognition Vendor Test\" (FRVT) in'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 14, 'page_label': '15'}, page_content='standards, published Part 1 of the report \"Ongoing Face Recognition Vendor Test\" (FRVT) in \\n2018. This study compares the performance of 127 face identification algorithms submitted by \\n45 industrial and commercial research and development laboratories (including Germany\\'s \\nCognitec, the American Microsoft, China\\'s Yitu, France\\'s IDEMIA, Japan\\'s NEC and Russia\\'s \\nVisionLabs) and a university, based on a dataset of 26.6 million supervised facial images \\nrepresenting 12.3 million individuals. Despite different approaches and performances, the best \\nalgorithms in 2018 achieved error rates of less than 0.2%. Nevertheless, for at least 10% of \\nthe images, even if the identification was successful, the confidence rate40 remains low and a \\nhuman decision is still required to rule out the possibility that the proposed id entity is a false \\npositive.  \\n \\nImage capture quality \\nProblems with image quality arise from the image capture system (the camera), the'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 14, 'page_label': '15'}, page_content='Problems with image quality arise from the image capture system (the camera), the \\nenvironment (light) or the presentation of the face to the capture system (orientation, \\nblurring). Problems related to the image capture system and, to some extent, those caused \\nby the environment, are external to facial recognition software technology and will \\nundoubtedly be solved by better image capture and processing systems. These quality \\nproblems have been blamed for the very low positive predictive value (8%) observed when \\na facial recognition system was used during the 2017 Champions League final in Cardiff to \\ndetect the presence of suspected criminals41. \\n \\nThe emergence and combination of efficient algorithms, powerfu l computing resources and \\nmassive annotated datasets have improved the performance of facial recognition to the point \\nwhere these technologies are now available for practical and commercial use. We can'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 14, 'page_label': '15'}, page_content='where these technologies are now available for practical and commercial use. We can \\nreasonably expect to continue to see the error rate on  specific datasets fall towards zero. \\nNevertheless, as the challenge of finding ideal conditions becomes obsolete (less \\n \\n40 The confidence level in an algorithmic prediction indicates the degree to which an algorithm is sure of the result \\nit proposes. It is sometimes referred to as the \"authentication score\" for facial recognition and is usually \\nexpressed as a probability. \"For example, a face detection system may predict that an image region is a face at a \\nconfidence score of 90%, and another image region is a face at a confidence score of 60%.\" (Amazon Web \\nServices (2020), \"Amazon Rekognition: Developer\\'s Manual\", p.126). \\n41 On this subject, see the site dedicated to automatic facial recognition devices developed by the South Wales \\nPolice: http://afr.south-wales.police.uk/'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 15, 'page_label': '16'}, page_content='16/61 \\noptimal lighting, less adequate position, etc.), other challenges and problems will arise \\nwhen these algorithms are taken out of researc h labs and integrated into routine \\napplications.  \\nFirst of all, it should be noted that face resemblance issues increase the error rate as the size \\nof population under consideration increases: the rate is multiplied by 1.6 when considering a \\npopulation of 12 million adults, compared to a population of 640,000 adults. In addition to look-\\nalikes, the algorithms tested by the NIST are unable to distinguish between monozygotic \\n(\"identical\") and dizygotic (\"fraternal\") twins. The U.S. agency report  also mentions poor \\nperformance when it comes to identifying individuals over time. While systems have no \\ndifficulty recognizing an individual if they are presented with a photo of that individual who has \\naged two years (or \"photo at +2 years\"), the result  is quite different if they are presented with'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 15, 'page_label': '16'}, page_content='aged two years (or \"photo at +2 years\"), the result  is quite different if they are presented with \\na photo of the same individual who has aged eighteen years (or \"photo at +18 years\"). Even \\nfor the best system tested, the error rate for photos of adults who have aged 18 years is five \\ntimes higher than that observed on photos of individuals who have aged only two years. \\nSystems, especially those for time-sensitive forensic applications, must take into account age-\\nrelated physical changes in individuals, but also the factors that can speed up these changes \\n(like use of medication or drugs) or slow them down (like cosmetic surgery). \\nWorse still, studies have shown that some facial recognition technologies cause bias that \\ncan lead to racist, sexist or ageist discrimination. These biases come mainly from the data \\non which the learning models are trained. Indeed, public databases such as VGGFace2 (faces'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 15, 'page_label': '16'}, page_content='on which the learning models are trained. Indeed, public databases such as VGGFace2 (faces \\nfrom Google Images) and MS -Celeb-1M42 (celebrity faces) often come from websites and \\ncollect attractive photographs of young, smiling celebrities wearing makeup. These \\nphotographs poorly generalize the physiognomy of everyday populations. However, even a \\ndatabase created from images of daily life can show an uneven distribution of the different \\nphysical attributes of a population. First, a group (not necessarily a minority group) may be \\nunder-represented in this database because of a lack of representativeness in the creation of \\nthe database i tself, for example if the database is generated from a sample where men \\noutnumber women. However, even a database with a distribution faithful to that of the target \\npopulation collects fewer examples representing a minority group than a majority group and'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 15, 'page_label': '16'}, page_content='population collects fewer examples representing a minority group than a majority group and \\nmay cause the models to produce a higher error rate on the minority group than on the majority \\ngroup, resulting in bias and possible discrimination against one of the two groups, depending \\non the application. For example, IBM showed that among the database s most used to train \\nfacial recognition algorithms, over 80% of the LFW database contained photos of fair-skinned \\npeople.  \\nThis has prompted researchers to create more diverse datasets (such as Racial Faces in-the-\\nWild43 (RFW)) to measure ethnic and gender biases in facial recognition algorithms. While not \\nall studies on this subject are consensual (see the polemic on Amazon Rekognition between \\nthe American Civil Liberties Union 44 and Amazon 45), we can cite the work of the Gender \\n \\n42 Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He and Jianfeng Gao (2016), “Ms-celeb-1m: A dataset and'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 15, 'page_label': '16'}, page_content='benchmark for large-scale face recognition”, ECCV, pp. 87-102, Springer. \\n43 Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao and Yaohai Huang (2018), “Racial faces in-the-wild: \\nReducing racial bias by deep unsupervised domain adaptation”, 11pp. \\n44 “Amazon’s Face Recognition Falsely Matched 28 Members of Congress With Mugshots”, American Civil \\nLiberties Union, 26 July 2018: https://www.aclu.org/blog/privacy-technology/surveillance-technologies/amazons-\\nface-recognition-falsely-matched-28  \\n45 “Thoughts on Recent Research Paper and Associated Article on Amazon Rekognition”, Amazon, AWS \\nMachine Learning Blog, 26 January 2019: https://aws.amazon.com/fr/blogs/machine-learning/thoughts-on-recent-\\nresearch-paper-and-associated-article-on-amazon-rekognition/'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 16, 'page_label': '17'}, page_content='17/61 \\nShades project46 led by res earcher Joy Buolamwini at the Massachusetts Institute of \\nTechnology (MIT). This study created an annotated dataset and then tested facial recognition \\nsystems from Microsoft, IBM and Face++. The results are categorical: if the error rate of these \\nthree syst ems is less than 1% for light -skinned men, it reaches more than 20% for dark -\\nskinned women. \\n \\nFigure 1 - Error rates observed on Microsoft, IBM and Face++ facial recognition systems \\nby gender47 \\n \\n \\nThese results are corroborated by an evaluation of 14 commer cial facial recognition models \\non RFW that showed disparities in performance between different ethnic groups, with a 12% \\ndifference in error rate between the best and worst performing groups. \\n \\nMedia coverage of suspected discriminatory bias \\nIn 2015, engineer Jacky Alciné publicized on Twitter an existing bias in Google Photo, which \\ndetected gorillas in a photograph showing two faces of people of color48.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 16, 'page_label': '17'}, page_content=\"detected gorillas in a photograph showing two faces of people of color48.  \\nApple's facial authentication system was also blamed in 2017 in an article in the Sun49 for \\nfailing to differentiate between the faces of people of Asian origin as well as it does between \\nthe faces of Caucasian people. \\n \\nStudies have also shown that despite very low error rates in sufficiently varied databases of \\nfaces, facial recognition systems are subject to significant security weaknesses. \\nPresentation attacks50 allow individuals to cover themselves (makeup, anti -pollution masks \\nlike during the 2019 demonstrations in Hong Kong, wigs, 3D silicone masks) to deceive the \\ndevices. \\n \\n \\n \\n \\n \\n46 See the website: http://gendershades.org/ and Joy Buolamwini and Timnit Gebru (2018), “Gender shades: \\nIntersectional accuracy disparities in commercial gender classification”, Conference on Fairness, Accountability \\nand Transparency, pp 77-91. \\n47 Source: http://gendershades.org/overview.html\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 16, 'page_label': '17'}, page_content='and Transparency, pp 77-91. \\n47 Source: http://gendershades.org/overview.html  \\n48 See: https://twitter.com/jackyalcine/status/615329515909156865  \\n49 “Chinese users claim iPhoneX face recognition can’t tell them apart”, The Sun, December, 2017: \\nhttps://www.thesun.co.uk/news/5182512/chinese-users-claim-iphonex-face-recognition-cant-tell-them-apart/  \\n50 Raghavendra Ramachandra and Christoph Busch, “Presentation attack detection methods for face recognition \\nsystems: a comprehensive survey”, ACM Computing Surveys (CSUR), 50(1):8, 2017.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 17, 'page_label': '18'}, page_content='18/61 \\nDodging attacks \\nSo-called \"adversarial\" attacks51 rely on subtle, calculated variations in image pixels, often \\nimperceptible to the naked eye, to change and distort the prediction of the algorithms. The \\nimage below is an example. \\n \\nThe photographs above52 illustrate an example of an adversarial attack. On the left: original \\nphotograph of the actress Eva Longoria. Center: disturbed image of the actress. Right: filter that \\nhas disturbed the original image to create the disturbed image. An algorithm that correctly \\nrecognized the actress in the photograph on the left failed to recognize her in the disturbed image in \\nthe center, though the disturbance is imperceptible to the naked eye. \\n \\nWhile some cyber -risks can be explained by the technical limitations of facial recognition, \\nothers stem from the predictive capacity of the algorithms. In addition, this predictive capacity \\ncan raise ethical concerns.  A study published in Nature Medicine53 proved that images of'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 17, 'page_label': '18'}, page_content='can raise ethical concerns.  A study published in Nature Medicine53 proved that images of \\nfaces contain sufficient information for a powerful system to predict demographic and \\nphenotypic information such as gender expression, age or private and particularly \\nsensitive genetic information (family ties, ethnic origins). In the same vein, a research team \\nat Stanford University54 has shown that an algorithm trained to classify the sexual orientation \\nof women and men simply on the basis of facial characteristics can predict the sexual \\norientation of individuals with an accuracy  of over 83%, whereas humans are only 61% \\naccurate. This study concludes by discussing the dangers of facial recognition on the privacy \\nand security of LGBTQ+ people. It is therefore necessary to reflect on safeguards for the \\nconfidentiality of this biolog ical data and to prevent possible leaks. As Raphaël de Cormis,'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 17, 'page_label': '18'}, page_content='confidentiality of this biolog ical data and to prevent possible leaks. As Raphaël de Cormis, \\nVice President of Innovation and Digital Transformation at Thales, points out, \" the larger the \\ndata, the more the size of the honeypot can attract attackers and put users at risk \"55. It is \\ntherefore necessary to avoid the centralization of data.   \\n \\n51 Akhil Goel, Anirudh Singh, Akshay Agarwal, Mayank Vatsa and Richa Singh (2018), “Unravelling robustness of \\ndeep learning based face recognition against adversarial attacks”, 8 pp ; Akhil Goel, Anirudh Singh, Akshay \\nAgarwal, Mayank Vatsa et Richa Singh (2018), “Smartbox: Benchmarking adversarial detection and mitigation \\nalgorithms for face recognition”, IEEE BTAS, 7 pp.  \\n52 These photographs are taken from Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, Michael K. Reiter (2016), \\n\"Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition\", Proc. ACM SIGSAC \\nConf. Comput. Commun. Secur., pp. 1528-1540.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 17, 'page_label': '18'}, page_content='Conf. Comput. Commun. Secur., pp. 1528-1540. \\n53 Yaron Gurovich, Yair Hanani, Omri Bar et al. (2019), “Identifying facial phenotypes of genetic disorders using \\ndeep learning”, Nature Medicine, 25:60 – 64. \\n54 Michal Kosinski and Yilun Wang (2018), “Deep neural networks are more accurate than humans at detecting \\nsexual orientation from facial images”, Journal of Personality and Social Psychology, Volume 114, Numéro 2, pp. \\n246-257. \\n55 Renaissance Numérique (2019), “Reconnaissance faciale : Interdiction, expérimentation, généralisation, \\nréglementation. Où en est-on ? Où allons-nous ?”, p. 34: \\nhttps://www.renaissancenumerique.org/publications/reconnaissance-faciale-interdiction-experimentation-\\ngeneralisation-reglementation-ou-en-est-on-ou-allons-nous'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 18, 'page_label': '19'}, page_content='19/61 \\nAccording to the NIST report, the revolution in deep learning explains the strong performance \\nimprovements over the period of 2013-2018 compared to between 2010-2013. Convolutional \\nartificial neural ne twork models benefit from a high robustness to invariance and work to \\nresolve limitations due to unsupervised presentation of faces in front of camera lenses. \\nHowever, these deep learning technologies are not immune from producing discriminatory \\nbiases. Th eir \"black -box\" aspect complicates their ability to be audited: it is impossible to \\npredict the exact behavior of a technology of this nature from the moment it is designed. As a \\nconsequence, facial recognition providers must ensure the transparency of the  \\nperformance of their models on different groups of people.  \\n \\n1.3.2. A perpetual technological race to correct their negative effects \\nThe deep learning revolution in computer vision explains recent advances in facial recognition'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 18, 'page_label': '19'}, page_content='The deep learning revolution in computer vision explains recent advances in facial recognition \\nand its integration into recreatio nal, commercial, industrial, forensic and security uses. Error \\nrates on some databases continue to decrease (on average the error rate is halved every \\nyear). However, the annotated datasets that are used to train and evaluate models are not \\nalways sufficie ntly diverse and can lead to biases that will result in discrimination in their \\napplications. The technological race therefore encourages actors to minimize bias, either by \\ndiversifying the training databases of the models (for example by inserting compute r-\\ngenerated images into them) or by improving the models themselves (for example by adapting \\ntheir recognition capacities from majority groups to minority groups). \\nDespite these advances, the technological race to correct errors in facial recognition \\ndevices is endless . On the one hand, by definition these technologies can never be'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 18, 'page_label': '19'}, page_content=\"devices is endless . On the one hand, by definition these technologies can never be \\n100% reliable. On the other hand, cyber-attacks and decoy attacks are progressing in \\nparallel with the progress made by the industry.  \\n \\nIncreasingly unsupervised annotated facial image datasets  \\n \\nA supervised learning model requires a significant amount of annotated data to learn and \\ngeneralize well. While initially the industry's deep facial recognition datasets were privately \\nowned, other databases have been made public so that the academic community could \\ncatch up with industry research. \\nThe figure below shows the evolution of the d atasets used in facial recognition, with the \\neffect of increasing scale and a generalization of images showing faces with less and less \\nsupervision and constraint: ages, poses and expressions vary, then external elements \\nobscure certain parts of the faces, either by cropping them or putting makeup on them.\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 18, 'page_label': '19'}, page_content='obscure certain parts of the faces, either by cropping them or putting makeup on them. \\nEvolution of facial recognition databases since 199456 \\n \\n \\n56 This diagram is taken from Mei Wang and Weihong Deng (2018), op. cit., p. 13.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 19, 'page_label': '20'}, page_content='20/61 \\n1.3.3. Probabilistic technologies prone to human deficiencies  \\nBeyond the technical shortcomings inherent in facial recognition technologies (errors, \\ndiscriminatory biases, cyber -risks), some flaws may result from human intervention in the \\nidentification processes.  \\nPublic safety is a common case in which the final decision rests with human beings, and where \\nthe decisions at stake are likely to seriously a ffect the fundamental freedoms of individuals. \\nWhen a video surveillance system at the entrance to a stadium or in the street identifies a \\nperson as a wanted criminal, this identification cannot automatically be considered valid. \\nBecause facial recognition  is a probabilistic technology, the risk of false positives is far too \\nhigh when it comes to the apprehension and arrest of an individual. Therefore, it is essential \\nthat security officers, and indeed users broadly, be trained in the operation of these'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 19, 'page_label': '20'}, page_content='that security officers, and indeed users broadly, be trained in the operation of these \\nprobabilistic technologies and in the interpretation of the results they present.  Every \\nuser must guard against \"automation bias\", or the idea of placing too much trust in the machine \\nand ignoring external information that could contradict the results of the algorithm57. In order \\nto minimize the risk of misjudgment when \\'manually\\' checking a computer -generated match, \\nit is also essential to ensure that the confidence level of the result is as high as possible. \\nFor example, in its \" Developer Guide \" for its Rekognition product, Amazon advises law \\nenforcement agencies to use a similarity threshold of 99% or above 58 to minimize the risk of \\nmisidentification. As indicated in the manual, a facial match established by a system such as \\nAmazon Rekognition cannot constitute irrefutable proof of a person\\'s identity, and must'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 19, 'page_label': '20'}, page_content='Amazon Rekognition cannot constitute irrefutable proof of a person\\'s identity, and must \\ninevitably be corroborated by additional evidence (verification of identity documents, \\nfingerprints, DNA, etc.).  \\nFurthermore, it is important to bear in mind that any decision taken in w hich facial \\nrecognition technology is involved is the result of a chain of events.  It is therefore \\nessential to ensure that decisions at each level of the chain can be explained, right \\ndown to the human decision. Indeed, as the European Commission points o ut in its White \\nPaper on Artificial Intelligence, \" opacity (\"black box effect\"), complexity, unpredictability and \\npartially autonomous behaviour \" are characteristic of many AI technologies 59. It is therefore \\nnot a question of explaining how these \"black boxes\" work, but rather of being able to identify \\nthe data that were used to make the system learn and explain the training approach of the'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 19, 'page_label': '20'}, page_content='the data that were used to make the system learn and explain the training approach of the \\nsystem beforehand, along with the elements that led the system to a decision afterwards. Prior \\nunderstanding should thus encourage the development of facial recognition technologies that \\nare ethical by design (in other words, whose source code integrates ethical dimensions) and \\nto identify possible biases inherent in these technologies from the design stage. Subsequently, \\nit is a question of monitoring the results presented by the algorithms over time 60. The \\n \\n57 For more on automation bias, see: https://en.wikipedia.org/wiki/Automation_bias  \\n58 Amazon Web Services (2020), Ibid., p.155: https://docs.aws.amazon.com/rekognition/latest/dg/rekognition-\\ndg.pdf \\n59 European Commission (2020), \"Artificial Intelligence: A European approach based on excellence and trust\", \\nCommunication, COM(2020) 65 final, p. 12: https://ec.europa.eu/info/sites/info/files/commission-white-paper-'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 19, 'page_label': '20'}, page_content='artificial-intelligence-feb2020_en.pdf \\n60 On this subject, see Renaissance Numérique (2017), “L’éthique dans l’emploi à l’ère de l’intelligence \\nartificielle”, 23 pp.: \\nhttps://www.renaissancenumerique.org/system/attach_files/files/000/000/137/original/Renaissance_Nume%CC%\\n81rique_IA___Emploi_Oct2017.pdf?1508946963'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 20, 'page_label': '21'}, page_content='21/61 \\nevolutionary nature of these technologies makes it necessary to monitor the results regularly \\nin order to be able to improve them61. \\nAt the same time, when an individual is deprived of his or her liberty as a result of a decision \\nmade with the help of a facial recognition device, this is not solely because of the machine. In \\napplications where this technology is used as a decision aid, it is also necessary to account \\nfor the human biases which persist regardless of the level of confidence in the equipment. In \\naddition, it should be the responsibility of the provider of the facial recognition technology to \\nexplain to its customer (for exa mple, law enforcement) the precise functioning of the device \\nand how the results of the technology should be taken into account in its decision. \\nConsideration should also always be given to less intrusive alternatives.  \\nDespite significant technological ad vances made possible by the use of deep'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 20, 'page_label': '21'}, page_content='Despite significant technological ad vances made possible by the use of deep \\nlearning in recent years, facial recognition technologies remain not only \\nimperfect but also highly sensitive devices. The processing of biometric data is \\nfar from being a trivial activity and the risk of violation of our fundamental rights \\nand freedoms merits particular attention. However, the number of relatively \\nsensitive uses and experiments is increasing, including in several member \\nstates of the European Union. It is therefore necessary to wonder whether the \\nlegal framework surrounding them really is sufficient to prevent applications of \\nfacial recognition technologies that could jeopardize our fundamental rights.   \\n \\n61 Ibid., pp. 20-21.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 21, 'page_label': '22'}, page_content='22/61 \\n2. An inconsistent and efficient application of the legal \\nframework \\n2.1. Facial recognition technologies are relatively well regulated \\nWhether in France, within the European Union or at the international level, the development \\nof facial recognition technologies does not take place in a total legal vacuum. This \\ndevelopment is framed by numerous st andards at various levels, ranging from fundamental \\nrights and freedoms to national legislations. In order to analyze the legal framework applicable \\nto facial recognition technologies, it is therefore necessary to consider the entire existing \\nnormative framework and to begin with the highest norms, in particular the fundamental rights \\nand principles that are the foundation of democracies, before looking at inferior norms. \\n \\nFigure 2 - The hierarchy of norms \\n \\n \\n \\n*subject to ratification by States and inclusion in their norm-setting system \\n \\n2.1.1. Fundamental rights are applicable to facial recognition \\ntechnologies'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 21, 'page_label': '22'}, page_content='2.1.1. Fundamental rights are applicable to facial recognition \\ntechnologies \\nFundamental rights form the basis of democracies. As such, they also represent the highest \\nstandards for facial recognition technologies. At the international level, many texts guarantee \\nfundamental rights that may be impacted by the use of these technologies. \\n \\n \\n \\n \\nConstitutionality \\nblock\\nInternational norms*\\nLaws and ordonnances\\nRegulatory norms\\nJurisprudence\\nAdministrative acts'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 22, 'page_label': '23'}, page_content='23/61 \\n2.1.1.1. Rights that are guaranteed widely \\nIn this regard, the 1948 Universal Declaration of Human Rights62 goes so far as to directly \\nlink fundamental rights to world peace: “ recognition of the inherent dignity and of the equal \\nand inalienable rights of all members of the human family is the foundation of freedom, justice \\nand peace in the world”. This Declaration is supplemented at the international level by specific \\ntexts that specify and refine certain rights and principles contained in the Declaration. These \\ninclude, among others, the International Covenant on Civil and Political Rights 63, the \\nInternational Covenant on Economic, Social and Cultural Rights64 and the Convention on the \\nRights of the Child65. The application of these international texts depends essentially on their \\nratification by States, the absence of reservations, and their adoption at the nat ional level \\n(through amending the Constitution, for example).'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 22, 'page_label': '23'}, page_content='(through amending the Constitution, for example).  \\nFurthermore, the principles contained in the Universal Declaration of Human Rights have \\nfound further legal scope through the adoption of binding regional texts. Such is the case with \\nthe European Convention for the Protection of Human Rights and Fundamental \\nFreedoms66 which was signed within the Council of Europe on November 4, 1950 and entered \\ninto force in 1953. The European Court of Human Rights, seated in Strasbourg since 1959, \\nmonitors its  application by member countries and the respect for the rights it guarantees. \\nWithin the European Union, the Treaty of Lisbon, which entered into force on December 1st, \\n2009, conferred on the Charter of Fundamental Rights of the European Union67 the same \\nlegal force as the Treaties of the European Union. It is consequently binding on member \\nstates, and any citizen can invoke it when his or her rights are not respected. It is based on \\nthe principle of democracy and the rule of law.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 22, 'page_label': '23'}, page_content='the principle of democracy and the rule of law. \\nIn France and within the EU – democracies that share the values of the Universal Declaration \\nof Human Rights, the European Convention on Human Rights, and have given a higher value \\nto the Charter of Fundamental Rights – the design, development and deployment of facial \\nrecognition technologies must be analyzed legally within this normative framework. \\n \\n2.1.1.2. The fundamental rights likely to be affected by the use of \\nfacial recognition technologies \\nFacial recognition technologies challenge many of the fundamental rights enshrined in the \\nabove-mentioned texts. \\n \\n62 United Nations (1948), Universal Declaration of Human Rights: \\nhttps://www.ohchr.org/EN/UDHR/Documents/UDHR_Translations/eng.pdf \\n63 United Nations (1966), International Covenant on Civil and Political Rights: \\nhttps://www.ohchr.org/fr/professionalinterest/pages/ccpr.aspx  \\n64 United Nations (1966), International Covenant on Economic, Social and Cultural Rights:'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 22, 'page_label': '23'}, page_content='64 United Nations (1966), International Covenant on Economic, Social and Cultural Rights: \\nhttps://www.ohchr.org/FR/ProfessionalInterest/Pages/CESCR.aspx  \\n65 United Nations (1989), Convention on the Rights of the Child: \\nhttps://www.ohchr.org/fr/professionalinterest/pages/crc.aspx  \\n66 Council of Europe (1950), European Convention for the Protection of Human Rights and Fundamental \\nFreedoms: https://www.echr.coe.int/Documents/Convention_ENG.pdf \\n67 European Union (2000), Charter of Fundamental Rights of the European Union: \\nhttps://www.europarl.europa.eu/charter/pdf/text_en.pdf'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 23, 'page_label': '24'}, page_content=\"24/61 \\nThe European Union Agency for Fundamental Rights (FRA) carried out a detailed review of \\nthe use of facial recognition technologies by public authorities in a note published in November \\n201968. In this report, the Agency underlines the following rights:  \\n● human dignity;  \\n● privacy; \\n● protection of personal data; \\n● non-discrimination; \\n● the rights of the child and the elderly; \\n● the rights of persons with disabilities; \\n● freedom of assembly and association; \\n● freedom of expression; \\n● the right to good administration; \\n● the right to a fair trial. \\nTable 1 - Examples of fundamental rights likely to be affected by the use of facial \\nrecognition technologies \\nFundamental rights or freedoms69 Impact of facial recognition technologies  \\nHuman dignity Facial recognition technologies, especially when \\nused in real -time, can be seen as surveillance \\ntechnologies that are intrusive enough on \\npeople's lives to affect their ability to lead a \\ndignified life.\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 23, 'page_label': '24'}, page_content=\"people's lives to affect their ability to lead a \\ndignified life. \\nNon-discrimination Discrimination may occur in the design \\n(conscious or unconscious) of the algorithm itself \\n(through the introduction of bias) or as a result of \\nthe implementation, by those who decide what \\naction to take based on the result of the \\nalgorithm. \\nFreedom of expression, association \\nand assembly  \\nThe use of facial recognition technologies \\nthrough video cameras installed in public space \\ncan deter people from expressing themselves \\nfreely, encourage them to change their behavior \\n \\n68 European Union Agency for Fundamental Rights (2019), “Facial recognition technology: fundamental rights \\nconsiderations in the context of law enforcement”, 36 pp.: https://fra.europa.eu/en/publication/2019/facial-\\nrecognition-technology-fundamental-rights-considerations-context-law \\n69 The distinction between fundamental rights and fundamental freedoms stems from the fact that freedom is\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 23, 'page_label': '24'}, page_content='inherent in the person as an individual, whereas a right is an obligation that the State owes to individuals.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 24, 'page_label': '25'}, page_content='25/61 \\nor revert to presenting them as part of a group of \\nindividuals. Some people may not want to gather \\nin public spaces for fear of facial recognition \\ntechnologies. This may also violate the freedom \\nto remain anonymous. \\nThe right to a fair trial  This right rests first and foremost on people’s \\nright to be informed. Thus, any lack of \\ntransparency could undermine this right 70. In \\naddition, public authorities must put in place \\nprocedures to enable the persons concerned to \\nbring challenges and complaints. For example, \\nindividuals should be able to object to their \\ninclusion in a ma tching database or claim \\ncompensation for damage due to \\nmisinterpretation of the results of facial \\nrecognition technologies. \\nThe right to good administration It refers to the concept of explicability and is \\nbased on a principle of transparency which \\nimplies that individuals may request to know the \\nreasons why a decision has been taken against \\nthem. With regard to facial recognition'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 24, 'page_label': '25'}, page_content='reasons why a decision has been taken against \\nthem. With regard to facial recognition \\ntechnologies, this would mean that the \\nadministration or the police would have to be able \\nto explain to a person the reasons why he or she \\nhas been arrested on the basis of the results of a \\nfacial recognition technology. \\nRight to education  A student who is denied access to a school in a \\nregion that has mandated access through facial \\nrecognition technologies, and does not offer any \\nother access alternatives, may invoke his or her \\nright to education. \\n \\nOf course, certain uses of facial recognition technologies may raise issues regarding other \\nfundamental rights. It is possible to imagine, for example, a ban on turning the hu man body \\nand its parts into a source of profit, in the event that the capture of faces by facial recognition \\ntechnologies is carried out for commercial purposes. Moreover, these rights are not exclusive:'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 24, 'page_label': '25'}, page_content='technologies is carried out for commercial purposes. Moreover, these rights are not exclusive: \\nthe use of facial recognition technology may call into question respect for multiple fundamental \\nrights. \\n \\n70 Informing people is an essential prerequisite. Without such information, recourse is not possible. See for \\nexample: CJEU (21 December 2016), cases C -203/15 & C-698/15, Tele2 Sverige AB v. Post -Och Telestyrelsen \\nand Secretary of State for the Home Department v. Tom Watson and Others; CJEU (19 January 2010), Case C -\\n555/07, Seda Kücükdeveci v. Swedex GmbH & Co KG.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 25, 'page_label': '26'}, page_content='26/61 \\nIn France, concerning the protection of personal data and respect for privacy, Article 1 of Law \\n78-17 of 6 January 1978 on data processing, databases and liberties already provided that \\n“informatics must be at the service of every citizen. [...] It must not infringe on human identity, \\nhuman rights, privacy or individual or public freedoms”71. \\nAs indicated by the European Union Agency for Fundamental Rights, the impact in terms of \\nfundamental rights varies considerably depending on the purpose, context and scope of the \\nuse of the facial recognition technology. Although some flaws arise from the lack of precision \\nof the technology itself, some impacts persist even in the absence of error. \\nMost of the a bove-mentioned fundamental rights are not only guaranteed by the European \\nCharter of Fundamental Rights, but also at international level. This means that the \\njustification for the compatibility of facial recognition technologies with fundamental'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 25, 'page_label': '26'}, page_content='justification for the compatibility of facial recognition technologies with fundamental \\nrights may also apply beyond the EU. \\n \\nInternationally recognized principles \\nThe principle of non-discrimination is internationally recognized: it is enshrined in Article \\n2 of the Universal Declaration of Human Rights. Several other rights derive from the principle \\nof non -discrimination in order to protect vulnerable persons who are susceptible to \\nparticularly severe discrimination. These groups are afforded additional protection to ensure \\nthat equality of dignity and rights is achieved. This applies to children, the elderly and people \\nwith disabilities. The principle of racial non-discrimination also derives from this principle. \\nThe rights to an effective remedy and a fair trial are also enshrined in articles 8 to 11 of \\nthe Universal Declaration of Human Rights. This means in particular that from the moment \\na person is arrested, the legitimacy of the arrest must be demonstrated.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 25, 'page_label': '26'}, page_content='a person is arrested, the legitimacy of the arrest must be demonstrated. \\nFreedom of expression is a crucial right and another recognized foundation of democracy. \\nIt is enshrined in article 19 of the Universal Declaration of Human Rights and article 19 of \\nthe International Covenant on Civil and Political Rights. \\nThe freedoms of assembly and association emanate from freedom of expression, but are \\nalso enshrined as such in the Universal Declaration of Human Rights (Article 20) and the \\nCovenant on Civil and Political Rights (Articles 21 and 22). \\n \\n2.1.1.3. How can facial recognition technologies be reconciled \\nwith respect for fundamental rights?  \\nThe question that then arises is how to reconcile these fundamental rights and principles with \\nfacial recognition technologies. The fact that facial recognition technologies have the potential \\nto undermine these rights calls for the highest degree of caution.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 25, 'page_label': '26'}, page_content='to undermine these rights calls for the highest degree of caution.   \\nThis is particularly the case when it comes to human dignity, which is an \"inviolable\" right as \\nstated in Article 1 of the European Charter of Fundamental Rights, meaning that it may not be \\ninfringed. Well before the adoption of the Charter of Fundamental Rights of the European \\n \\n71 Translated from the French original: “l\\'informatique doit être au service de chaque citoyen. […] Elle ne doit \\nporter atteinte ni à l\\'identité humaine, ni aux droits de l\\'homme, ni à la vie privée, ni aux libertés individuelles ou \\npubliques”.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 26, 'page_label': '27'}, page_content='27/61 \\nUnion, the French Conseil d\\'État  had, in its famous \" Commune de Morsang -sur-Orge\" \\njudgment of 27 October 1995, enshrined the principle of respect for human d ignity as a \\ncomponent of public order (the so -called \"dwarf throwing\" affair); in this case, this principle \\nprevails over the consent of the person him or herself. Similarly, the Constitutional Council has \\nalso considered that safeguarding human dignity is  a principle with constitutional value and \\nthe Court of Justice of the European Union (CJEU) has recognized it as a general principle of \\nlaw72. Therefore, if a facial recognition system violates human dignity then it must be banned, \\nand no derogation from this rule is possible.  \\nThough not possible with respect to human dignity (which cannot be derogated), it is possible \\nunder certain conditions to limit the exercise of other rights enshrined in the EU Charter of \\nFundamental Rights. Indeed, Article 52 of the Charter of Fundamental Rights of the European'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 26, 'page_label': '27'}, page_content='Fundamental Rights. Indeed, Article 52 of the Charter of Fundamental Rights of the European \\nUnion provides that \"any limitation on the exercise of the rights and freedoms recognised by \\nthis Charter must be provided for by law and respect the essence of those rights and freedoms. \\nSubject to the principle of proportionality, limitations may be made only if they are necessary \\nand genuinely meet objectives of general interest recognised by the Union or the need to \\nprotect the rights and freedoms of others.” \\nIt follows that any limitation on the rights en shrined in the Charter of Fundamental Rights of \\nthe European Union must:  \\n● be provided for by law: in other words, fall within the scope of an existing text with \\ncurrent legal force;  \\n● genuinely meet objectives of general interest recognized by the Union or of the need \\nto protect the rights and freedoms of others; \\n● respect the essence of the rights and freedoms, in other words, the inalienable core of \\nthe right concerned;'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 26, 'page_label': '27'}, page_content='the right concerned; \\n● respect the principle of proportionality; \\n● be necessary (principle of necessity). \\nIf the introduction of a facial recognition technology is likely to infringe a fundamental right and \\nfails to meet one of these conditions, its deployment may be considered contrary to the Charter \\nof Fundamental Rights of the European Union. It bears noting the CNIL\\'s opinion on the draft \\ndecree relating to the \"StopCovid\" application, in which it reiterates the importance of \\ncompliance with the above-mentioned conditions, in particular the motive of general interest \\nand the principle of proportionality73. \\n \\n72 Constitutional Council (27 July 1994), No. 94-343-344 DC; CJEU (14 October 2004), Omega, aff C-36/02. \\n73 Deliberation no. 2020-056 of 25 May 2020 giving its opinion on a draft decree relating to the mobile application \\nknown as \"StopCovid\", §5 : “La Commission rappelle néanmoins que les protections constitutionnelle et'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 26, 'page_label': '27'}, page_content='conventionnelle du droit au respect de la vie privée et à la protection des données à caractère personnel, assises \\nnotamment sur la Charte des droits fondamentaux de l’Union européenne et la Convention européenne de \\nsauvegarde des droits de l’homme et des libertés fondamentales, imposent que les atteintes portées à ces droits \\npar les autorités publiques soient non seulement justifiées par un motif d’intérêt général, comme cela est le cas \\nen l’espèce, mais soient également nécessaires et proportionnées à la réalisation de cet objectif.” (“The \\nCommission would nevertheless point out that the constitutional and conventional protection of the right to \\nrespect for private life and the protection of personal data, based in particular on the Charter of Fundamental \\nRights of the European Union and the European Convention for the Protection of Human Rights and \\nFundamental Freedoms, require that infringements of these rights by public authorities must not only be justified'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 26, 'page_label': '27'}, page_content='on grounds of general interest, as is the case here, but must also be necessary and proportionate to the \\nachievement of that objective.”)'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 27, 'page_label': '28'}, page_content='28/61 \\nThe European Convention for the Protection of Human Rights and Fundamental Freedoms, \\nwhile not as general in scope as the mechanism provided for in the Charter of Fundamental \\nRights of the European Union, also provides for a similar mechanism that  applies to \\ninterference by public authorities in the exercise of certain rights enshrined in that Convention, \\nnamely the right to respect for private and family life, freedom of thought, conscience and \\nreligion, freedom of expression, freedom of assembly,  association and movement.  \\nInterference by public authorities must therefore pursue certain aims defined by the convention \\nand these must be \"necessary measures in a democratic society\" 74 and proportionate to the \\naim pursued. \\nThe principle of proportional ity is an essential concept. It is defined as \"a balancing \\nmechanism between legal principles of equivalent rank, which are simultaneously'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 27, 'page_label': '28'}, page_content='mechanism between legal principles of equivalent rank, which are simultaneously \\napplicable but contradictory 75”. It is a question of weighing up and striking a balance \\nbetween each of the legal pri nciples in question - generally a power conferred on the State \\n(public order, law enforcement) and the fundamental rights of individuals - or between several \\nfundamental rights. Respect for the principle of proportionality requires that a measure \\nrestricting rights and freedoms must be: \\n● appropriate, in that it must enable the legitimate objective pursued to be attained;  \\n● necessary, in that it must not exceed what is required to achieve that objective;  \\n● and proportionate, in that it must not, by the burdens it creates, be disproportionate \\nto the result sought. \\nWhile the principle of proportionality was initially a mechanism used by judges to arbitrate \\nbetween competing legal principles, this “triple test” has acquired a general application'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 27, 'page_label': '28'}, page_content='between competing legal principles, this “triple test” has acquired a general application \\nat the European lev el.  Article 5 of the Treaty on European Union states that “ Under the \\nprinciple of proportionality, the content and form of Union action shall not exceed what is \\nnecessary to achieve the objectives of the Treaties. The institutions of the Union shall apply  \\nthe principle of proportionality as laid down in the Protocol on the application of the principles \\nof subsidiarity and proportionality.” The principle of proportionality is intended to limit and to \\nframe the actions of the European Union, which must confi ne itself to what is necessary to \\nachieve the objectives of the Treaties. This implies in particular that the European legislator \\nmust resort to this principle when adopting a text.  \\nOriginating in Germany, this “triple test” has gradually spread throughou t Europe76, including'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 27, 'page_label': '28'}, page_content='Originating in Germany, this “triple test” has gradually spread throughou t Europe76, including \\nthe United Kingdom. There is also emerging application of the “triple test” in the United \\nStates77. In France, the Constitutional Council uses the proportionality test when monitoring \\nlegislative provisions that restrict the exercise of a right or freedom in the name of safeguarding \\n \\n74 Council of Europe (1950), op. cit.  \\n75 G. Xynopoulos, “Proportionnalité”, in D. Alland and S. Rials (2003), Dictionnaire de la culture juridique , PUF, \\n2003, p. 1251. \\n76 CEDH (23 July 1968), aff. n° 1474/62, “Affai re relative à certains aspects du régime linguistique de \\nl’enseignement en Belgique c. Belgique\" pts. 5 et 10; CEDH (4 December 2008), aff. 30562/04 & 30566/04 S. and \\nMarper v. the United Kingdom, paras. 95 -104; CJCE (24 July 2003), aff. C -280/00 Altmark; CJUE (8 April 2014),'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 27, 'page_label': '28'}, page_content=\"aff. C-293/12 & C-594/12, Digital Rights Ireland Ltd v. Minister for Communications, Marine and Natural Resources \\nand Others and Kärntner Landesregierung and Others. \\n77 United States Court of Appeals for the 9th District (9 February 2017), State of Washington v. Donald J. Trump et \\nal, No. 17-35105. In this decision, the Court of Appeals weighed the decree's infringement of certain individual and \\nstate rights against the public interest in maintaining the decree.\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 28, 'page_label': '29'}, page_content=\"29/61 \\npublic order, or when it has to reconcile multiple fundamental rights with one another 78. The \\nFrench Data Protection Authority (Commission nationale de l'informatique et des libertés  - \\nCNIL) regularly uses proportionality and necessity checks to verify the lawfulness of data \\nprocessing. This is precisely what it did in its publication on facial recognition of November 15, \\n201979, as well as when it expressed its views on the application to locate individuals carrying \\nCovid-1980. \\nFull respect for fundamental rights is a precondition for any application of the law, \\nwhatever the technologies in question. It is therefore necessary to implement the “triple test” \\nbefore any deployment of facial recognition technologies. Moreover, the more intrusive the \\ntechnologies, the more strictly the test must be applied.  \\n2.1.2. National a nd regional regulations complete the framework \\noutlined by fundamental rights\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 28, 'page_label': '29'}, page_content='2.1.2. National a nd regional regulations complete the framework \\noutlined by fundamental rights  \\nBeyond fundamental rights, which are at the top of the hierarchy of norms, the deployment of \\nfacial recognition technologies must also respect various national regulations that may apply. \\nIn order to inform the debate at the national and European level, it is necessary, in addition to \\nthe French and EU regulatory framework, to also observe the developments taking place \\nabroad, particularly in the United States and China. This glo bal dimension is all the more \\nimportant since these two countries are trying to impose their standards in the global market \\nfor facial recognition technologies81.  \\nBefore examining these regulations, it is interesting to note that these technologies have long \\nbeen the subject of legal definitions: \\n● According to the \"Article 29 Working Party\" (now the European Data Protection'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 28, 'page_label': '29'}, page_content='● According to the \"Article 29 Working Party\" (now the European Data Protection \\nCommittee or \"EDPB\"), \"facial recognition is the automatic processing of digital images \\nwhich contain the faces of p ersons for the purpose of identification, \\nauthentication/verification or categorisation of those persons\"82; \\n● According to the French CNIL, \" facial recognition is a programming and probabilistic \\ntechnique that makes it possible to automatically recognize a person on the basis of \\nhis or her face, in order to authenticate or identify him or her\"83.  \\n \\n \\n \\n78 Conseil constitutionnel (23 July 2015), n° 2015 -713 DC, § 11; Conseil constitutionnel (22 December 2015), n° \\n2015-527 QPC, § 4; Conseil constitutionnel (10 February 2017), n° 2016-611 QPC. \\n79 “Reconnaissance faciale : pour un débat à la hauteur des enjeux”, CNIL, 15 November 2019: \\nhttps://www.cnil.fr/fr/reconnaissance-faciale-pour-un-debat-la-hauteur-des-enjeux'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 28, 'page_label': '29'}, page_content='https://www.cnil.fr/fr/reconnaissance-faciale-pour-un-debat-la-hauteur-des-enjeux  \\n80 Hearing before the Laws Committee of the National Assembly, introduc tory remarks by Marie -Laure Denis, \\nPresident of the CNIL, Wednesday 8 April 2020: “Si un dispositif de suivi des personnes était mis en place de \\nmanière obligatoire, alors il nécessiterait une disposition législative et devrait, en tout état de cause, démontrer sa \\nnécessité pour répondre à la crise sanitaire ainsi que sa proportionnalité en tenant compte des mêmes principes \\nde protection de la vie privée, et en étant réellement provisoire”. (“If an individual monitoring system were to be put \\nin place on a mandatory basis, then it would require a legislative provision and would, in any case, have to \\ndemonstrate its necessity to respond to the health crisis as well as its proportionality, while taking into account the \\nsame principles of privacy protection, and remaining truly provisional\".)'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 28, 'page_label': '29'}, page_content='same principles of privacy protection, and remaining truly provisional\".) \\n81  “How the US plans to crack down on Chinese facial recognition tech used to ‘strengthen authoritarian \\ngovernments’ ”, This Week in Asia, 18 June 2019.  \\n82 Opinion 02/2012 on facial recognition for online and mobile services, 22 March 2012. \\n83 \"Reconnaissance faciale pour un débat à la hauteur des enjeux\", CNIL, 15 November 2019. \\nIn the original French, “ la reconnaissance faciale est une technique informatique et probabiliste qui permet de \\nreconnaître automatiquement une personne sur la base de son visage, pour l’authentifier ou l’identifier”.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 29, 'page_label': '30'}, page_content='30/61 \\n2.1.2.1. The framework in France \\nIn France, facial recognition technologies are not regulated by a specific text. They are, \\nhowever, subject to the regulations applicable  to the processing of personal data and, to a \\ncertain extent, to the regulations applicable to the installation of video protection equipment. It \\nshould also be noted that, depending on the ultimate use, the implementation of facial \\nrecognition technologies may raise questions about rights other than the right to personal data \\nprotection or to privacy. One example of this could be in the area of labor law, when these \\ntechnologies are used to provide secure access to business premises. \\nThe regulation applicable to personal data  \\nPersonal data is any information relating to identified or identifiable natural persons 84. There \\nare special categories of personal data, known as \"sensitive\" data, which include biometric \\ndata85. This data is subject to a stricter legal regime than other data.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 29, 'page_label': '30'}, page_content='data85. This data is subject to a stricter legal regime than other data. \\nBecause facial recognition technologies involve biometric data, they are therefore subject to \\nthe rules applicable to the processing of personal data, namely:  \\n● Regulation (EU) 2016/679 of the European Parliament and of the Counc il of 27 April \\n2016 on the protection of natural persons with regard to the processing of personal \\ndata and on the free movement of such data (General Data Protection Regulation or \\n\"GDPR\"); \\n● Directive (EU) 2016/680 of the European Parliament and of the Coun cil of 27 April \\n2016 on the protection of natural persons with regard to the processing of personal \\ndata by competent authorities for the purposes of the prevention, investigation, \\ndetection or prosecution of criminal offences or the execution of criminal penalties, and \\non the free movement of such data, (“Law Enforcement Directive” or LED”); and'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 29, 'page_label': '30'}, page_content='on the free movement of such data, (“Law Enforcement Directive” or LED”); and \\n● Law n°78-17 of 6 January 1978 relating to data processing, files and individual liberties, \\nin its modified version (\"Loi Informatique et Libertés\").  \\nIn France, texts authorizing the use of facial recognition technologies for the processing of \\npersonal data were adopted a long time ago. No less than five expressly authorize the use of \\nfacial recognition technologies, namely, the Criminal Records Processing File (TAJ), the rapid \\nand secure crossing of external borders (PARAFE), the certified online authentication on \\nmobile phones (authentification en ligne certifiée sur mobile , or ALICEM) and two temporary \\nuses for experimental purposes (in several airfields and as part of a \"hackathon\").  Moreover, \\nas these technologies are generally considered \"suspect\" in French law, there are over twenty \\ntexts establishing processing operations involving digitized images of individuals that'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 29, 'page_label': '30'}, page_content='texts establishing processing operations involving digitized images of individuals that \\nexpressly exclude or even prohi bit the use of facial recognition technologies, acting as \\n\"safeguards\".  \\n \\n84 GDPR, article 4 §1: “personal data\\' means any information relating to an identified or identifiable natural person \\n(\\'data subject\\'); an identifiable natural person is one who can be id entified, directly or indirectly, in particular by \\nreference to an identifier such as a name, an identification number, location data, an online identifier or to one or \\nmore factors specific to the physical, physiological, genetic, mental, economic, cultur al or social identity of that \\nnatural person.” \\n85 GDPR, article 4 §14.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 30, 'page_label': '31'}, page_content='31/61 \\nThe GDPR - which applies not only in France, but throughout the member states of the \\nEuropean Union - requires compliance with a certain number of principles that apply to any \\nprocessing of personal data, including biometric data, and therefore necessarily to facial \\nrecognition technologies. These include: \\n● the principle of lawfulness: all data processing must be based on one of the \"legal \\nbases\" referred to in the GDPR in order to be implemented; \\n● the principle of fair and transparent processing:  the data subject must be informed of \\nthe existence of the processing operation and its purposes (this obligation is reinforced \\nfor minors through the use of appropriate and comprehensible terms);  \\n● the purpose limitation principle:  data must be collected for specified, explicit and \\nlegitimate purposes; \\n● the data minimization principle: data must be adequate, relevant and limited to what is \\nnecessary for the purposes;'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 30, 'page_label': '31'}, page_content='necessary for the purposes; \\n● the principle of accuracy: data must be accurate and up to date.  \\nBefore the entry into force of the GDPR86, biometric data was not regarded as \"sensitive data\", \\nthat is, as data that cannot, in principle, be processed. Biometric data were essentially covered \\nby the CNIL\\'s authorization  application system. In France, persons wishing to implement \\npersonal data processing involving facial recognition technologies thus had to obtain prior \\nauthorization from the CNIL. These applications for authorization have given rise to several \\ndeliberations by the authority 87. The processing of biometric data (and thus the use of \\nfacial recognition technologies) is, in principle, prohibited 88. There are, however, a \\nnumber of exceptions to this prohibition principle. With regard to biometric data, and therefore, \\nnecessarily, to facial recognition technologies, several exceptions are likely to apply:'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 30, 'page_label': '31'}, page_content='necessarily, to facial recognition technologies, several exceptions are likely to apply: \\n● when the person concerned has given his or her explicit consent;  \\n● where processing is necessary in order to protect vital interests; \\n● where processing is necessary on grounds of substantial public interest; \\n● where processing is necessary for scientific research purposes (in France, this is \\ncurrently limited to public research). \\nWhere the legal basis for processing is the explicit consent of the data subject, such c onsent \\nmust not only be free, specific, informed and unambiguous; it is only valid if the data subject \\nis able to decline or to withdraw his or her consent without suffering any prejudice. It is \\ntherefore necessary to provide an alternative solution for the data subject who might refuse to \\ngive consent or decide to withdraw it at a later stage. The GDPR specifies that where the'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 30, 'page_label': '31'}, page_content='give consent or decide to withdraw it at a later stage. The GDPR specifies that where the \\ncontroller is a public authority, it is unlikely that consent has been given freely, as there \\nis often a clear imbalance of power between the controller and the data subject89.  \\n \\n86 Under Directive 95/46/EC of 24 October 1995 on the protection of individuals with regard to the processing of \\npersonal data and on the free movement of such data. \\n87 To our knowledge, fourteen deliberations have authorized the use of facial recognition tec hnologies and five \\nhave refused it.  \\n88 GDPR, Article 9. \\n89 GDPR, Recitals 42 and 43; Article 29 Working Party - Consent Guidelines within the meaning of Regulation \\n2016/679.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 31, 'page_label': '32'}, page_content='32/61 \\nPrior consent and proportionality:  \\nthe example of \"virtual access control\" in high schools in the PACA region \\nOn 27 February 2020, the Administrative Court of Marseille handed down the first case law \\ndecision concerning facial recognition in France. The regional council of Provence -Alpes-\\nCôte d\\'Azur (PACA) had begun experimenting with a so -called \"virtual access control\" \\nsystem in two high schools, consisting of the installation of facial recognition g ateways at \\nthe entrances to these schools. The PACA Region sought to legally justify the processing \\nof biometric data by obtaining the prior consent of the high school students concerned.  The \\nAdministrative Court of Marseille granted the request for annul ment of the decision, noting \\nin particular that \"whereas the target public is in a relationship of authority with regard to the \\nheads of the public educational establishments concerned, the Region does not justify'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 31, 'page_label': '32'}, page_content=\"heads of the public educational establishments concerned, the Region does not justify \\nhaving provided sufficient guarantees in order to obtain the consent of high school students \\nor their legal representatives for the collection of their personal data in a free and enlightened \\nmanner”90.  \\nThe CNIL had also been seized by the Provence -Alpes-Côte d'Azur Region regarding a \\nrequest for advice on this experiment, which had previously been the subject of an impact \\nassessment relating to data protection, the results of which had been communicated to the \\nauthority. Following its decision of 17 October 2019, the CNIL noted that facial recogn ition \\ndevices are particularly intrusive and present major risks of infringement of privacy and \\nindividual freedoms, particularly when it comes to minors. In the presence of less intrusive \\nalternative means (for example, access control through the use of b adges), the authority\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 31, 'page_label': '32'}, page_content='alternative means (for example, access control through the use of b adges), the authority \\nconsidered that the envisaged device was contrary to the main principles of proportionality \\nand data minimization laid down in the GDPR. \\n \\nThe GDPR also provides for the need to carry out a data protection impact assessment (DPIA) \\non processing operations of personal data that are likely to pose a high risk to the rights and \\nfreedoms of the data subjects. The use of facial recognition technologies should require the \\nimplementation of a DPIA, either because they constitute an operation defined by the CNIL \\nfor which a DPIA is mandatory, or because they meet one or more of the criteria set out in the \\nguidelines of the \"Article 29 Group\"91. Among these criteria, the Group specifically addresses \\nfacial recognition technologies by evoking the  criterion of innovative use and the application \\nof new technological or organizational solutions 92. If it appears that the level of residual risk'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 31, 'page_label': '32'}, page_content='of new technological or organizational solutions 92. If it appears that the level of residual risk \\nremains high, the results of the DPIA must be communicated to the CNIL.  \\nLike the GDPR, the Law Enforcement Directive (LED) applies not only in France, but across \\nall European Union member states. If facial recognition technologies are used for security or \\n \\n90 In the French original, “alors que le public visé se trouve dans une relation d’autorité à l’égard des \\nresponsables des établissements publics d’enseignement concernés, la Région ne justifie pas avoir prévu des \\ngaranties suffisantes afin d’obtenir des lycéens ou de leurs représentants légaux qu’ils donnent leur \\nconsentement à la collecte de leurs données personnelles de manière libre et éclairée” \\n91 For example, systematic surveillance, collection of sensitive data or highly personal data, large -scale collection'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 31, 'page_label': '32'}, page_content=\"of personal data, cross -referencing of data, data concerning vulnerable pers ons (patients, the elderly, children, \\netc.) or innovative use (use of new technology). \\n92 Article 29 Data Protection Working Party (2017), 'Guidelines on data protection impact assessment (DPIA) and \\nhow to determine whether the processing is 'likely to crea te a high risk' for the purposes of Regulation (EU) \\n2016/679', p. 12: “Innovative use or application of new technological or organizational solutions: for example, \\ncombined use of fingerprint and facial recognition systems to improve physical access control, etc.”.\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 32, 'page_label': '33'}, page_content='33/61 \\nprevention purposes, they are not covered by the GDPR, but rather by the Law Enforcement \\nDirective. This dir ective is in a sense the \"twin\" of the GDPR and was adopted at the same \\ntime. It applies essentially to the processing of personal data for the purpose of the prevention, \\ninvestigation, detection, investigation, prosecution or enforcement of criminal offen ces, \\nincluding the protection and prevention of threats to public security. \\nUnder the Law Enforcement Directive, the processing of biometric data is authorized under \\nthe conditions laid down in Article 10:  “Processing of personal data revealing racial or e thnic \\norigin, political opinions, religious or philosophical beliefs, or trade union membership, and the \\nprocessing of genetic data, biometric data for the purpose of uniquely identifying a natural \\nperson, data concerning health or data concerning a natura l person\\'s sex life or sexual'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 32, 'page_label': '33'}, page_content='person, data concerning health or data concerning a natura l person\\'s sex life or sexual \\norientation shall be allowed only where strictly necessary, subject to appropriate safeguards \\nfor the rights and freedoms of the data subject, and only: \\n \\n(a) where authorised by Union or Member State law; \\n \\n(b) to protect the vital interests of the data subject or of another natural person; or \\n \\n(c) where such processing relates to data which are manifestly made public by the data \\nsubject.” \\n \\nTwo conditions are therefore required a minima by the Law Enforcement Directive for the \\nprocessing of biometric data: it is possible to process biometric data (1) only in cases of \\nabsolute necessity, subject to safeguards for the rights and freedoms of the data subjects and \\n(2) only where such processing is authorized by the law of the Union or t he law of a member \\nstate. \\nThe notion of \"absolute necessity\" raises questions, because if another course is'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 32, 'page_label': '33'}, page_content='state. \\nThe notion of \"absolute necessity\" raises questions, because if another course is \\npossible – which always seems to be the case93 – this could amount to a total exclusion \\nof the possibility of using biometric data and consequently facial recognition \\ntechnologies. It is difficult to answer this question at the moment, as it has - as far as we \\nknow - not been settled by case-law. The CNIL, when asked for its opinion, does not address \\nthis concept directly, but more often focuses on th e implementation of appropriate \\nsafeguards94.  \\nWith regard to individual\\'s consent, this cannot constitute a legal basis for the processing of \\ndata involving facial recognition technologies under the Law Enforcement Directive. The \\nimplementation of a system for security purposes requires at a minimum the adoption of a law \\nor a decree by the Conseil d\\'État. However, in the case of a simple experiment, i.e. without'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 32, 'page_label': '33'}, page_content='or a decree by the Conseil d\\'État. However, in the case of a simple experiment, i.e. without \\ngoing beyond testing, it falls within the scope of the GDPR and, ultimately, it will most often be \\nnecessary to obtain the consent of the volunteers.  \\n \\n \\n93 For example, with regard to the use of facial recognition by police forces, it is always possible to use human \\nlabor, such as shadowing, rather than monitoring a mass of individuals by means of cameras installed in public \\nspaces.  \\n94 See for example: Deliberation No. 2019-123 of 3 October 2019 giving its opinion on a draft decree creating an \\nautomated processing of personal data called \"mobile note-taking application\" (GendNotes).'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 33, 'page_label': '34'}, page_content=\"34/61 \\nExperimentation at the 2019 Nice Carnival: an incomplete report according to the \\nCNIL \\nA surveillance device based on facial recognition technologies was tested for several days \\nin the city of Nice during the March 2019 carnival. Nearly 1,000 peop le agreed to be \\nidentified within the crowd in real time by facial recognition technologies based on six \\ncameras positioned within the test perimeter. The City of Nice has encountered legal \\nchallenges that have hampered experimentation and has expressed its wish to see French \\nlegislation evolve with regard to experimentation with new technologies in real conditions in \\npublic spaces (and more specifically the Loi Informatique et Libertés). The CNIL asked for \\nadditional information, in particular on algorithm  error rates, image quality and the risks of \\ndiscrimination, and found the City of Nice's report to be incomplete.   \\n \\nThe Loi Informatique et Libertés (LIL)  is, naturally, aligned with the logic laid down in the\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 33, 'page_label': '34'}, page_content='The Loi Informatique et Libertés (LIL)  is, naturally, aligned with the logic laid down in the \\nEuropean texts. Its Article 6 refers to the exceptions provided for in the GDPR concerning the \\nprocessing of biometric data 95. In fact, the GDPR allows the processing of biometric data in \\ncertain specific cases, for example for scientific research purposes, where it is necessary \"on \\nthe basis of Union law or the law of a Member State\" 96. EU member states therefore have \\nsome flexibility: they can allow the processing of biometric data for scientific res earch \\npurposes provided that they adopt specific texts. At present, however, the French legislature \\nand the regulation authority have not adopted such texts. It is therefore not possible at present \\nto process biometric data for scientific research purposes in France, with the exception of for \\npublic research purposes, as such processing is made possible by the Loi Informatique et \\nLibertés97.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 33, 'page_label': '34'}, page_content=\"Libertés97. \\nIf a text is necessary, the processing carried out on behalf of the State involving facial \\nrecognition technologies should also give rise to a reasoned opinion by the CNIL and a decree \\nby the Conseil d'État98.  \\n \\n \\n95 Loi Informatique et Libertés, Article 6.II: “Les exceptions à l' interdiction mentionnée au I sont fixées dans les \\nconditions prévues par le 2 de l'article 9 du règlement (UE) 2016/679 du 27 avril 2016 et par la présente loi”. \\n96 GDPR, article 9 §2 (j). \\n97 LIL, article 44 : “L'article 6 ne s'applique pas si l'une des conditions prévues au 2 de l'article 9 du règlement (UE) \\n2016/679 du 27 avril 2016 est remplie, ainsi que pour […] 6° Les traitements nécessaires à la recherche publique \\nau sens de l'article L. 112 -1 du code de la recherche, sous réserve que des motif s d'intérêt public important les \\nrendent nécessaires, dans les conditions prévues par le g du 2 de l'article 9 du règlement (UE) 2016/679 du 27\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 33, 'page_label': '34'}, page_content='avril 2016, après avis motivé et publié de la Commission nationale de l\\'informatique et des libertés rendu selon les \\nmodalités prévues à l\\'article 34 de la présente loi.” (\"Article 6 shall not apply if one of the conditions laid down in \\nArticle 9(2) of Regulation (EU) 2016/679 of 27 April 2016 is fulfilled, as well as for [...] 6° Processing operations \\nnecessary for public research within the meaning of Article L. 112-1 of the Research Code, provided that grounds \\nof substantial public interest make them necessary, under the conditions laid down in g of 2 of Article 9 of Regulation \\n(EU) 2016/679 of 27 April 2016, afte r a reasoned and published opinion of the Commission nationale de \\nl\\'informatique et des libertés issued in accordance with the procedures laid down in Article 34 of this Act\".) \\n98 LIL, article 32 : “Sont autorisés par décret en Conseil d\\'État, pris après av is motivé et publié de la Commission'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 33, 'page_label': '34'}, page_content='nationale de l\\'informatique et des libertés, les traitements de données à caractère personnel mis en œuvre pour le \\ncompte de l\\'État, agissant dans l\\'exercice de ses prérogatives de puissance publique, qui portent sur des données \\ngénétiques ou sur des données biométriques nécessaires à l\\'authentification ou au contrôle de l\\'identité des \\npersonnes.” (\"The processing of personal data carried out on behalf of the State, acting in the exercise of its \\nprerogatives as a public authority, which relates to genetic data or biometric data necessary for the authentication \\nor verification of the identity of persons, shall be authorised by decree of the Conseil d\\'État, issued after a reasoned \\nopinion has been given and published by the National Commission for Information Technology and Civil Liberties\")'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 34, 'page_label': '35'}, page_content='35/61 \\nRegulations pertaining to videoprotection  \\nIn France, in addition to the regulations applicable to personal data (GDPR, Law Enforcement \\nDirective, Loi Informatique et Liberté), the use of facial recognition technologies is governed \\nby the regulations on video protection. These regulations result essentially from articles L.251-\\n1 and following of the Internal Security Code (CSI). It applies to the installation of c ollection \\ndevices on public thoroughfares and in places open to the public, excluding those installed in \\nprivate places and workplaces not open to the public (\"video surveillance\"). Under these \\nregulations, the installation of a video protection  device mus t meet certain objectives \\ndetermined by the legislator99, notably: \\n● the protection of public buildings and public facilities and their surroundings; \\n● the safeguarding of facilities useful to national defense; \\n● the regulation of transportation flows; \\n● the detection of traffic violations;'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 34, 'page_label': '35'}, page_content='● the regulation of transportation flows; \\n● the detection of traffic violations; \\n● the prevention of attacks on the security of persons and property in places particularly \\nexposed to risks of aggression, theft or drug trafficking, as well as the prevention of \\ncustoms fraud, in areas particularly exposed to these offenses; \\n● the prevention of acts of terrorism; \\n● the prevention of natural or technological risks; \\n● the rescue of persons and defense against fire; \\n● the security of facilities open to the public in amusement parks; \\n● compliance with the obligation to be covered by insurance guaranteeing civil liability \\nin order to operate a land motor vehicle. \\nDepending on the aims pursued, the installation of video protection systems also falls under \\nthe GDPR, the Law Enforcement Directive or the Loi Informatique et Libertés100.   \\nIn addition, the regulations on video protection also require operators to:'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 34, 'page_label': '35'}, page_content='In addition, the regulations on video protection also require operators to: \\n● inform persons likely to be filmed by means of posters or signs (the obligation to inform \\nalso arises from the GDPR and the Law Enforcement Directive);  \\n● limit the storage period of recordings, which may not be kept for more than one \\nmonth101 ;  \\n● ensure the security of the processed data (for example by restricting the viewing of \\nimages to authorized persons). \\nThe installation of a video protection devices in the public space must in principle be authorized \\nby the prefect with territorial jurisdiction, following the opinion of the Departmental Video \\nProtection Commission. If the systems installed are used for the processing of personal data, \\ntheir install ation must be authorized under the conditions laid down in the regulations \\napplicable to the processing of personal data 102. It therefore follows that the installation of'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 34, 'page_label': '35'}, page_content='applicable to the processing of personal data 102. It therefore follows that the installation of \\nvideo protection devices incorporating facial recognition technologies from the outset  are \\n \\n99 CSI, Article L. 251-2. \\n100 For more information see: \"Vidéoprotection: quelles sont les dispositions applicables\", CNIL, 13 December \\n2019: https://www.cnil.fr/fr/videoprotection-quelles-sont-les-dispositions-applicables \\n101 CSI, Article L.252-5. \\n102 CSI, Article L252-1.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 35, 'page_label': '36'}, page_content=\"36/61 \\nsubject to the provisions of the regulations on personal data processing. However, if video \\nprotection devices are installed and, as a result of this installation, facial recognition \\ntechnologies are used from the recordings, it will then be necessary  to comply with both \\nstandards.   \\nNGOs denounce a system  \\ncombining video protection and facial recognition in Marseille \\n \\nTwo NGOs, La Quadrature du Net and the Ligue des droits de l'Homme  (LDH), filed an \\nappeal with the administrative court of Marseille on 17 January 2020 to halt the deployment \\nof facial recognition technologies based on a network of around fifty video protection \\ncameras. The complainants criticized the City of Marseille f or having implemented this \\nsystem without a prior impact assessment or consultation of the French data protection \\nauthority and without establishing the absolute necessity of using such technology.\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 35, 'page_label': '36'}, page_content='authority and without establishing the absolute necessity of using such technology. \\nHowever, that action was dismissed on March 11, 2020 in the absence of evidence allowing \\nfor the establishment of the contested decision. \\n \\n2.1.2.2. Approaches abroad \\nLike France, no foreign country has so far chosen to adopt specific regulations on facial \\nrecognition technologies. Nonetheless, their use is increasing glo bally and within a variety of \\nregulatory frameworks. While it is difficult to establish a clear map of the legal initiatives being \\nundertaken abroad, some trends are discernible. Overall, the approach favoured by our \\nEuropean neighbours seems to be that of caution (regulation/experimentation). However, the \\nUnited Kingdom stands as an exception with its relatively \"open -minded\" adoption of facial \\nrecognition technologies, including for security purposes in public spaces. In China, it is rather'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 35, 'page_label': '36'}, page_content='recognition technologies, including for security purposes in public spaces. In China, it is rather \\nthe learning-by-doing approach that is taken. Finally, in the United States, the approach is \\ngradually moving towards regulation, or even prohibition in some states. \\nIn Europe \\nThe United Kingdom is the only country in Europe to use facial recognition technologies in \\npublic from \"real\" databases, that is, outside of tests (as was the case in the city of Nice). \\nAcross the Channel, facial recognition technologies have been deployed at several major \\npublic events, including concerts and rugby matches. This was also the case for the 2017 \\nUEFA Champions League final in Cardiff. This particular case also led to the first arrest using \\na facial recognition system, of an offender wanted for domestic violence. \\nOn 4 September 2019, a first decision was issued by the Queen\\'s Bench Div isional Court of'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 35, 'page_label': '36'}, page_content=\"On 4 September 2019, a first decision was issued by the Queen's Bench Div isional Court of \\nthe High Court of Justice sitting in Cardiff. The English courts considered that the use of facial \\nrecognition technologies was in line with their laws 103. An appeal has been launched against \\nthis decision.  \\n \\n \\n103 High Court of Justice, Queen's Bench Divisional Court, Cardiff, Case No: CO/4085/2018, R (Bridges) v \\nCCSWP and SSHD: https://www.judiciary.uk/wp-content/uploads/2019/09/bridges-swp-judgment-Final03-09-19-\\n1.pdf\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 36, 'page_label': '37'}, page_content='37/61 \\nThe ICO encourages the establishment of a binding code of practice  \\non the use of facial recognition by the police in public places \\nIn August 2019, a property developer installed and tested a system using facial recognition \\nat King\\'s Cross, a busy area of London. The system installed on the corner of one of the \\ncompany\\'s buildings was filming passers-by in the street without alerting them.  The police \\nhad apparently shared \" watchlists\" with the company to carry out operations to identify \\npersons on file. The Information Commi ssioner\\'s Office (ICO), the UK equivalent of the \\nCNIL, is currently investigating the company\\'s use of facial recognition.  As a result of this \\ncase, the ICO published a report on the use of facial recognition technologies by police in \\npublic places. Additionally, in a press release, the Information Commissioner called on the \\ngovernment to adopt a binding code of practice on the subject, while reminding the police'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 36, 'page_label': '37'}, page_content=\"government to adopt a binding code of practice on the subject, while reminding the police \\nthat they must slow down and justify any use made of these technologies. \\n \\nIn Germany, the legislator has made use of the authorization provided in Article 9(2)(j) of the \\nGDPR providing that sensitive data may be processed without consent for scientific research \\npurposes where it is necessary for that purpose and where the interests of the controll er far \\noutweigh the interests of the data subject. On the other hand, a bill aimed at updating the \\ncurrent text regulating the powers of the police has been purged of its explicit references to \\nfacial recognition104, as the Federal Data Protection Officer a nd the German Bar Association \\nhave expressed doubts about the bill's compatibility with the Constitution.  \\nIn the Netherlands, the legislator has also made use of the authorization provided for in Article\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 36, 'page_label': '37'}, page_content='In the Netherlands, the legislator has also made use of the authorization provided for in Article \\n9(2)(j) of the GDPR on the condition that the proce ssing is necessary for the purposes of \\nscientific research, that the research is in the public interest, that the request for explicit \\nconsent proves impossible or involves disproportionate effort, and that the execution provides \\nfor safeguards to ensure that the privacy of the data subject is not disproportionately affected. \\nOutside of Europe \\nIn the United States , the Federal Trade Commission (FTC) published best practices in \\nOctober of 2012 for companies wishing to develop and market facial recognition \\ntechnologies105. Many U.S. states are making use of facial recognition technologies, \\nsometimes for fraud control purposes (Texas106, Washington107 and Illinois108 among others), \\n \\n104 An earlier version of the bill envisaged authorizing the federal police to use facial recognition technologies'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 36, 'page_label': '37'}, page_content='based on images collected from 135 railway stations and 14 airports.  \\n105 “FTC Recommends Best Practices for Companies That Use Facial Recognition Technologies”, FTC, 22 \\nOctober 2012: https://www.ftc.gov/news-events/press-releases/2012/10/ftc-recommends-best-practices-\\ncompanies-use-facial-recognition \\n106 Texas Transportation Code, Title 7, Subtitle B, Chapter 521, Subchapter A: \\nhttps://statutes.capitol.texas.gov/Docs/TN/htm/TN.521.htm \\n107 Washington State Legislature, RCWs, Title 46, Chapter 46.20, Section 46.20.037: \\nhttps://app.leg.wa.gov/RCW/default.aspx?cite=46.20.037 \\n108 Illinois General Assembly, Illinois Compiled Statutes, Public Health (410 ILCS 705/): \\nhttp://www.ilga.gov/legislation/ilcs/ilcs4.asp?DocName=041007050HArt%2E+15&ActID=3992&ChapterID=35&Se\\nqStart=3000000&SeqEnd=6400000'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 37, 'page_label': '38'}, page_content='38/61 \\nsometimes for identity verification (Montana 109, Nevada 110, Connecticut 111and North \\nDakota112). The year 2019 and the beginning of 2020 have been particularly eventful in terms \\nof the use of facial recognition technologies in the United States:  \\n● In March 2019, the Commercial Facial Recognition Privacy Act of 2019 was introduced \\nin the Senate. The Act requires private companies to obtain consent from individuals \\nbefore using facial recognition technologies;  \\n● In May 2019, San Francisco became the first U.S. city to ban its use by police and city \\ndepartments. Other cities have followed suit, including Oakland and Berkeley;  \\n● On October 8, 2019, California passed the  Body Camera Accountability Act (or \"AB \\n1215\") prohibiting the use of facial recognition on body cameras worn by police officers; \\nin the same vein, the City of San Diego, whose police had been using facial recognition'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 37, 'page_label': '38'}, page_content='in the same vein, the City of San Diego, whose police had been using facial recognition \\ntechnologies since 2012, decided to ban their use for at least three years, as they had \\nnot led to any arrests or prosecutions; \\n● In October 2019, in the state of New York, a proposal was presented requiring \\ncompanies to inform users about the use of facial recognition technologies, the length \\nof time that data is stored and its transfer to third parties; \\n● On November 14, 2019, a bill was introduced in the Senate, which requires federal \\nofficers to obtain judicial approval before using facial recognition technology to monitor \\na suspected criminal113; \\n● On March 31, 2020, Washington State passed a law requiring government agencies \\nto obtain a warrant prior to any use of facial recognition technology, except in cases of \\nemergency114. According to this law, the device used must also be able to be \\nindependently tested to ensure that it does not present bias based on skin colour,'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 37, 'page_label': '38'}, page_content='independently tested to ensure that it does not present bias based on skin colour, \\ngender, age and other characteristics. The law also requires, prior to any deployment \\nby State or local authorities, the drafting of accountability reports115 and officer training. \\n \\n \\n \\n \\n \\n \\n109 Montana Code annotated 2019, Title 1 ‘General Laws And Definitions’, ‘Chapter 5 ‘Proof And \\nAcknowledgment Of Instruments Notaries Public’, Part 6. Notarial Acts, 1-5-602. ‘Definitions’: \\nhttps://leg.mt.gov/bills/mca/title_0010/chapter_0050/part_0060/section_0020/0010-0050-0060-0020.html \\n110 Nevada Revised Statutes, Chapter 133 ‘Wills”, NRS 133.085: https://www.leg.state.nv.us/NRS/NRS-\\n133.html#NRS133Sec085 \\n111 General Statutes of Connecticut, Volume 6, Chapter 319o, Department of Social Services, Sec. 17b-30. \\n‘Biometric identifier system’ : https://www.cga.ct.gov/current/pub/chap_319o.htm#sec_17b-30 \\n112 North Dakota Century Code, Chapter 44-04 ‘Duties, Records, and Meetings’: CHAPTER 44-04:'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 37, 'page_label': '38'}, page_content='112 North Dakota Century Code, Chapter 44-04 ‘Duties, Records, and Meetings’: CHAPTER 44-04: \\nhttps://www.legis.nd.gov/cencode/t44c04.pdf \\n113 Senate of the United States, 116th Congress, 1st Session, “Bill to limit the use of facial recognition technology \\nby Federal agencies, and for other purposes”: https://www.coons.senate.gov/imo/media/doc/ALB19A70.pdf \\n114 “Washington State Signs Facial Recognition Curbs Into Law; Critics Want Ban”, U.S. News, 31 March 2020: \\nhttps://www.usnews.com/news/us/articles/2020-03-31/washington-state-adopts-facial-recognition-rules-critics-\\nview-as-too-loose  \\n115 “Washington State’s regulation of facial recognition technology: first thoughts”, Global Partners Digital, 24 April \\n2020: https://www.gp-digital.org/washington-states-regulation-of-facial-recognition-technology-first-thoughts/'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 38, 'page_label': '39'}, page_content=\"39/61 \\nThe FBI and the U.S. Immigration and Customs Enforcement Agency  \\ninvolved in a controversy \\nIn July 2019, it was revealed that the FBI and the U.S. Immigration and Customs \\nEnforcement (ICE) had scanned the faces of millions of Americans without their consent \\nthrough driver's license databases and used these images in conjunction with facial \\nrecognition technologies. However, this use was never authorized by the U.S. Congress and \\nthe citizens concerned were never informed of the use of their personal data and \\nphotographs. Nearly 390,000 facial recognition searches have reportedly been conducted \\nby the FBI since 2011. \\n \\nIn China, for several years now, the governance model has been built on the basis of the \\nmassive collection and processing of citizens' personal data on social networks and through \\nsurveillance cameras. A social credit system has even been established in some regions and\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 38, 'page_label': '39'}, page_content='surveillance cameras. A social credit system has even been established in some regions and \\nallows the authorities to assign scores to citizens based on their behavior. If their score is too \\nlow, individuals are punished by being deprived of their most basic rights (access to credit, \\nmovement by train, access to school). In China, facial recognition data constitutes \"personal \\ninformation\" under Article 76 of the Cybersecurity Law 116. However, there is no unified \\nregulatory framework for facial recognition, but rather a multitude of sector-specific rules: \\n● on 21 January 2020, the Payments & Clearing Association of China published a Self-\\nRegulation Agreement for the offline facial recognition payment industry;  \\n● a law that came into force in December 2019 requires mobile telecommunications \\noperators to register the biometric facial data of any new user seeking to subscribe to \\ntheir services.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 38, 'page_label': '39'}, page_content='their services.  \\nIn November 2019, a first complaint was filed in the country against a company using facial \\nrecognition technologies. A law professor, who owned a pass to access a natural reserve, \\nalleged a contractual viol ation against the park that changed the method of identification at \\nthe entrance. \\nGenerally speaking, China takes a learning-by-doing approach to the legal framework of facial \\nrecognition technologies. There, their use is not limited beforehand by the legi slator, but on \\nthe contrary is restricted by the technical possibilities of manufacturers. However, as part of \\nthe strengthening of the regulatory regime for the protection of personal information, the state \\nauthorities in charge of supervising the Chinese  market published a new version of the \\nstandards for the protection of personal information on March 6, 2020117. These new standards \\nrequire data controllers to inform data subjects of the rules for the collection, use and storage'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 38, 'page_label': '39'}, page_content='116 “Translation: Cybersecurity Law of the People’s Republic of China (Effective 1 June 2017)”, New America, 29 \\nJune 2018: https://www.newamerica.org/cybersecurity-initiative/digichina/blog/translation-cybersecurity-law-\\npeoples-republic-china/  \\n117 “China tightens protection of personal information - what you need to know about the 2020 Chinese Personal \\nInformation National Standard”, Lexology, 23 March 2020:  \\nhttps://www.lexology.com/library/detail.aspx?g=5a63a595-4116-4567-bea2-f6b3380540cb'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 39, 'page_label': '40'}, page_content='40/61 \\nof data derived from facial recognition technologies and to obtain their consent for the \\nprocessing of their data118.  \\nThe analysis of th e legal framework applicable to facial recognition technologies shows that \\nthe development of facial recognition technologies is relatively well regulated in the EU. The \\nGDPR, the Law Enforcement Directive, as well as national legislations (for example the  Loi \\nInformatique et Libertés  in France) have been established to protect our personal data, \\nincluding biometric data. At the top of the hierarchy of norms, fundamental rights also exist as \\nsafeguards to protect us from the deployment of facial recognition  technologies that could \\nundermine the principles of democracy and the rule of law. In order to ensure that this \\nframework fulfils its mission, however, it is necessary to look beyond the theoretical analysis \\nand consider its application. A legal framework  is only useful to citizens if it is effectively and'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 39, 'page_label': '40'}, page_content='(easily) applicable.  \\n2.2. The legal framework suffers from deep weaknesses in its implementation  \\n2.2.1. A varied application across member states \\nThe GDPR and the Law Enforcement Directive have not fully led to th e harmonization of the \\nlegal framework applicable to the processing of personal data within the European Union, in \\nparticular as regards the processing of biometric data. Certain exceptions to the principle of \\nthe prohibition of the processing of sensitive  data, including biometric data, require the \\nadoption of a national or European text for their implementation. For the time being, only a few \\nStates have decided to take the step, in particular with regard to the exception for scientific \\nresearch purposes. There may therefore be significant differences in the application of \\nthe framework from one member state to another , particularly in the field of research. As \\nmentioned above, in France, the processing of biometric data in the context of research is only'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 39, 'page_label': '40'}, page_content='mentioned above, in France, the processing of biometric data in the context of research is only \\npossible under certain conditions and only for public research purposes. At present, there is \\nno derogation for private research as such.  \\nFurthermore, there are no plans at present to adopt a European text which would certainly \\nmake it possible to level o ut the disparities between member states and harmonize the \\ntransposing rules, for example with regard to the adoption of an experimental methodological \\nframework. A draft white paper on artificial intelligence by the European Commission leaked \\nin January 2020, which contained a total ban for several years on the use of facial recognition \\ntechnologies in European public spaces in order to allow time to assess the impacts of these \\ntechnologies and regulate them. The version of the White Paper finally publishe d on 19 \\nFebruary 2020 does not mention this ban and, on the contrary, paves the way for a reflection'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 39, 'page_label': '40'}, page_content='February 2020 does not mention this ban and, on the contrary, paves the way for a reflection \\nthat should be engaged at the European level119.  \\nMoreover, there are even differences of interpretation within the same state. This is what \\nhas occurred in the United Kingdom.  \\n \\n118 “China introduces stricter facial recognition standards”, South China Morning Post, 10 March 2020: \\nhttps://www.scmp.com/tech/article/3074443/china-introduces-stricter-facial-recognition-standards  \\n119 European Commission (2020), \"Artificial intelligence: a European approach based on excellence and trust\", p. \\n22: \"In order to address possible societal concerns relating to the use of AI for such purposes in public \\nplaces, and to avoid fragmentation in the internal market, the Commission will launch a broad \\nEuropean debate on the specific circumstances, if any, which might justify such use, and on common \\nsafeguards\".'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 40, 'page_label': '41'}, page_content=\"41/61 \\nIn France, experiments are carried out by private and public actors, most often with the advice \\nof the CNIL. However, the results of these experiments are not shared between the various \\nplayers and the service providers are fighting a  battle at a distance in an attempt to impose \\ntheir technology. There is currently no reliable experimentation methodology that \\nrespects citizens' rights and freedoms.  Similarly, improving the performance of facial \\nrecognition technologies requires access to increasingly large image databases. For \\nfundamental research purposes, it may therefore be appropriate to allow European and \\nFrench suppliers to access databases under conditions that respect the rights and freedoms \\nof individuals in order to preserve their competitiveness. \\nFinally, not all supervisory authorities in the European Union have the same means at \\ntheir disposal. Overall, their effectiveness is further limited by the small budgets allocated to\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 40, 'page_label': '41'}, page_content=\"their disposal. Overall, their effectiveness is further limited by the small budgets allocated to \\nthem by member states. In this respect, a recent r eport shows that the lack of technical \\nexpertise is a major obstacle to the implementation of the GDPR in Europe 120. The report \\npoints out that out of the 28 national authorities responsible for the application of the GDPR, \\nonly 5 count more than 10 technical specialists. Indeed, data protection authorities (DPAs) are \\noften not in a position to defend legal actions against multinational companies, which mobilize \\nconsiderable financial resources to challenge the authorities' injunctions in court. As a result,  \\nDPAs are not in a position to investigate the largest digital players. It would therefore also be \\nuseful to reflect on improving the means at their disposal, in particular to enable them to audit \\nthe conditions related to the deployment of facial recognition technologies. It is urgent to give\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 40, 'page_label': '41'}, page_content='the conditions related to the deployment of facial recognition technologies. It is urgent to give \\nthese authorities the means to check whether or not the use of facial recognition technologies \\nis carried out in compliance with the regulation. \\n2.2.2. Enforcement difficulties which lead to inefficiencies \\n2.2.2.1. Difficulties in applying fundamental rights \\nWhile fundamental rights and the application of the “triple test” should be mandatory, it must \\nbe noted that the “triple test” is used more by the courts in the event of litigation than by the \\nlegislature. Moreover, there is no mechanism to make it binding a priori. \\nTimid application of the “triple test” by legislators and public authorities \\nWhile the “triple test” should be imposed on legislators and public authorities, it must be \\nacknowledged that it is used sparingly and rarely in an explicit manner. It would be useful, \\nhowever, to have a systematic demonstration of it, in particular in the context of the screening'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 40, 'page_label': '41'}, page_content='however, to have a systematic demonstration of it, in particular in the context of the screening \\nassessments of draft standards or during the process of adopting administrative decisions. \\nIn the case of the experiments with facial recognition technologies that have been conducted \\nin France, it is highly likely that the use of the “triple test” would have been useful, if only to \\nvalidate their deployment, adapt it or prohibit it.  \\nCurrently, the protection of  personal data and privacy is the main concern in relation to the \\nuse of facial recognition technologies, even though their deployment is likely to infringe upon \\nother fundamental rights. Widespread use of the “triple test” would allow for a critical \\nassessment in terms of the compatibility of uses with all fundamental rights and freedoms.  \\n \\n120 “Europe’s governments are failing the GDPR - Brave’s 2020 report on the enforcement capacity of data'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 40, 'page_label': '41'}, page_content='protection authorities”, Brave: https://brave.com/wp-content/uploads/2020/04/Brave-2020-DPA-Report.pdf'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 41, 'page_label': '42'}, page_content='42/61 \\nThe ex post application of the “triple test” through jurisprudence \\nWith regard to case law, it generally intervenes ex post and, more often than not, in order to \\npunish a use or behavior. Judges make use of the principle of proportionality and have been \\napplying the “triple test” for several decades. However, in order for them to rule on and apply \\nthe “triple test”, an appeal must first be brought to them. The problems en countered in the \\nupholding of fundamental rights and the application of the “triple test” are therefore highly \\ndependent on challenges brought by the persons concerned and the referral of cases to the \\ncompetent courts.  \\nAs far as fundamental rights are con cerned, the main applicable texts are international and \\nEuropean. Appeals to an international body or to the European Court of Human Rights - even \\nthough national judges may also rule on this issue - are particularly complex and lengthy 121.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 41, 'page_label': '42'}, page_content='though national judges may also rule on this issue - are particularly complex and lengthy 121. \\nIn order to reach the Court of Justice of the European Union (CJEU), an action must first be \\nbrought at the national level and a preliminary question must be referred, unless an action for \\nfailure to fulfil obligations has been brought by the European Commission against a  member \\nstate. In any event, these procedures are particularly time -consuming, which creates a gap \\nwith respect to the deployment of the technologies in question, which are evolving very rapidly. \\nThe judgment will thus be given when the facial recognition technology has been or is about \\nto be deployed: \\n● if the technology is already deployed, there will potentially be an infringement of \\nfundamental rights and thus also a prejudice to the persons concerned. By definition, \\nit will no longer be possible to go back in time for those whose rights and freedoms'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 41, 'page_label': '42'}, page_content='it will no longer be possible to go back in time for those whose rights and freedoms \\nhave been violated. This raises the question of compensation for the damage suffered \\nand the consequences of possible sanctions against States; \\n● if the technology is about to be deployed, it is possible to resort to the “interim release” \\nprocedure, provided that three required conditions are met: urgency, infringement of a \\nfundamental freedom, and demonstration that the infringement of that freedom is \\nserious and manifestly unlawful122. \\nHowever, this requires t he persons concerned to appeal to the competent courts to ensure \\nthat their fundamental rights are respected, which implies constant and regular monitoring of \\nthe texts and projects potentially deployed by the public authorities. There are, of course, rights \\ndefending associations that are committed to this mission, but they will not necessarily have'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 41, 'page_label': '42'}, page_content=\"defending associations that are committed to this mission, but they will not necessarily have \\nthe means to act, particularly if facial recognition technologies multiply. In France, La \\nQuadrature du Net and the Ligue des droits de l'Homme regularly lodge  appeals against the \\ndeployment of facial recognition technologies. They recently succeeded in having the \\nMarseille administrative court overturn the decision by which the regional council of Provence-\\nAlpes-Côte d'Azur had approved the implementation of an  access control system based on \\nfacial comparison and trajectory tracking in the region's high schools.  \\nIn judicial matters, in the case of an appeal against a private company deploying facial \\nrecognition technology, the procedures are also long and compl ex. Even when summary \\nproceedings are possible, it usually takes several months before a decision is obtained.\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 41, 'page_label': '42'}, page_content='proceedings are possible, it usually takes several months before a decision is obtained. \\n \\n121 See the diagram \"Le cheminement d’une enquête” (“The course of an investigation”) produced by the European \\nCourt of Human Rights: https://www.echr.coe.int/Documents/Case_processing_FRA.pdf  \\n122 Code of Administrative Justice, Article L521-2.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 42, 'page_label': '43'}, page_content='43/61 \\nEventually, it is legitimate to ask whether it is desirable to rely solely on individuals and rights \\nassociations to ensure that our fundamental rights are respected when facial recognition \\ntechnologies are deployed, given the constraints inherent in the administrative and judicial \\nprocedures. \\nThe absence of a priori constraints  \\nTo date, there is no a priori control mechanism or obligation to carry out an impact assessment \\nprior to the deployment of facial recognition technologies on the basis of respect for \\nfundamental rights, apart from those imposed by the rules on personal data processing. While \\nthere is a willingness to launch several experiments and to supervise them, there has not yet \\nbeen any question of implementing this prior analysis with regard to fundamental rights. Yet, \\nmany voices call for an in-depth reflection on the subject of facial recognition technologies, as'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 42, 'page_label': '43'}, page_content='many voices call for an in-depth reflection on the subject of facial recognition technologies, as \\nthey have the potential to impact the very foundation of democracies. This is the case at the \\nEuropean level through the EU Fundamental Rights Agency (FRA) and the European Data \\nProtection Supervisor, and also at the national level through the CNIL and through initiatives \\nled by members of parliament. \\nWith regard to the rules on the processing of personal data, it should be noted that the system \\nof prior authorization has almost completely disappeared and that those involved in the \\nprocessing of personal data are now responsi ble. Each controller must therefore carry out a \\nrisk assessment and, in particular, carry out their own data protection impact assessment \\n(DPIA). Nonetheless, the question arises of the systematic submission of DPIAs to the \\nsupervisory authorities, for example to the CNIL in France.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 42, 'page_label': '43'}, page_content='supervisory authorities, for example to the CNIL in France. \\nThe European Commission, in its White Paper on Artificial Intelligence, states that \"it is vital \\nthat European AI is grounded in our values and fundamental rights such as human dignity and \\nprivacy protection\" and contemplates va rious certification and labeling mechanisms allowing \\nfor a priori review123.  \\n2.2.2.2. The question of liability \\nIn addition to the difficulties in enforcing fundamental rights, the issue of liability is another \\nweakness inherent in the legal framework around facial recognition technologies. This thorny \\nproblem arises differently not only from one country to another, but also according to the \\nsystem of reference under consideration (protection of personal data, privacy, etc.). \\nUnder the rules on the processing of personal data, it is first of all the controller - i.e. the person \\nwho defines the means and purpose of the processing - who is responsible vis -à-vis'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 42, 'page_label': '43'}, page_content='who defines the means and purpose of the processing - who is responsible vis -à-vis \\nindividuals, although the data processors may also be held liable since the entry into force of \\nthe GDPR. \\nUnder tort law, the implementation of liability (contractual or tortious) is subject to the fulfilment \\nof three cumulative conditions, namely fault, harm and a causal link between the two: \\n● Contractual liability refers to the obligation to remedy any damage  resulting from a \\ndefect in the performance of a contract (non -performance, poor performance or late \\nperformance). For example, if facial recognition technologies cause direct damage to \\n \\n123 European Commission (2020), \"Artificial Intelligence: A European approach based on excellence and trust\", p. \\n2.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 43, 'page_label': '44'}, page_content='44/61 \\nthe customer-user, the supplier of these technologies could be liable to remedy the \\ndamage;  \\n● Tortious liability refers to the obligation to compensate for harm caused to others \\noutside of any contractual relationship. The question of the liability of suppliers of facial \\nrecognition technologies vis -à-vis third parties arises  in terms substantially similar to \\nthat of artificial intelligence. In French law, the \" responsabilité du fait des choses \" is \\nessentially based on the control of the object in question at the time of the occurrence \\nof the harmful event, which is not necess arily adapted to artificial intelligence. The \\nsame is true with regard to product safety regulations - which impose liability on the \\nproducer of the product placed on the market - in particular when artificial intelligence \\nis incorporated after the product has been placed on the market by a party other than \\nthe producer.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 43, 'page_label': '44'}, page_content='the producer. \\nIn any case, the issue of liability for facial recognition technologies must be addressed for all \\nactors along the chain of liability, whether they are the designers of the algorithm, the solution \\nproviders, the private and public actors deploying these technologies, or the users. \\nThe responsibility of providers is most often questioned, but the responsibility of users \\n(operators) is perhaps not questioned enough.  Facial recognition technologies provide a \\npercentage chance that template A will match template B and, based on this result, an \\nindividual then decides whether or not intervention is necessary. It is therefore important that \\nthis individual have sufficient knowledge to make a decision, but also and above all that he or \\nshe approaches the person concerned bearing in mind that this is only a potential match and \\nnot necessarily a proven one (see Section 1.3.3.).'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 43, 'page_label': '44'}, page_content='not necessarily a proven one (see Section 1.3.3.).   \\nFinally, the issue of criminal liability must also be considered, which is likely to arise in \\nparticular for client -users, for example, in relation to the criminal sanctions attached to non -\\ncompliance regarding the regulations on personal data processing124.  \\nUltimately, it must be admitted that, although the legal framework surrounding \\nthe deployment of facial recognition technologies is well provided, it suffers \\nfrom serious weaknesses in its implementation. In particular, it is complex to \\nensure the conformity of these technologies with our fundamental right s in the \\nabsence of a priori scrutiny. As for the analyses carried out ex post by courts, \\nthese require that the matter be referred to the judge, and that a considerable \\ninvestment be made by the applicant, particularly in terms of time and skills. Not \\neveryone has the means required for this. When it comes to the protection of'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 43, 'page_label': '44'}, page_content='everyone has the means required for this. When it comes to the protection of \\nour personal data, and therefore of our biometric data, the lack of harmonization \\nat the European level and the lack of resources allocated to supervisory \\nauthorities presents an iss ue. If our aim is to ensure that facial recognition \\ntechnologies are deployed in accordance with European values, that is to say in \\ncompliance with the principles of the rule of law and democracy, then we cannot \\nbe satisfied with this current situation. At  a time when more and more facial \\nrecognition technologies are being deployed, there is an urgent need for the \\n \\n124 Criminal Code, Articles 226-16 and following.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 44, 'page_label': '45'}, page_content='45/61 \\nEuropean Union to address these issues and for member states to agree on a \\nrobust system to guarantee our rights.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 45, 'page_label': '46'}, page_content='46/61 \\n3. Towards a European standardization system guaranteeing \\nfundamental rights and freedoms  \\nThe processing of contactless biometric data gives facial recognition devices a highly sensitive \\ncharacter that requires increased vigilance on the part of society. Leaving the door open to \\nmass surveillance is not an option and the boundaries between the various applications of \\nthese technologies are often porous. How, therefore, can we protect ourselves from the threat \\nto our fundamental rights and freedoms? Several pathways have emerged in the public debate \\nover the last few months, ranging from a total ban on facial recognition technologies to \\nimproving the existing framework through the development of complementary regulations.  \\nAs regards their regulation, facial recognition technologies are currently well regulated legally \\nwithin the European Union. However, what the legal analysis of this framework reveals is a'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 45, 'page_label': '46'}, page_content='within the European Union. However, what the legal analysis of this framework reveals is a \\nprofound lack of efficiency in its application, in particular with regard to respect for fundamental \\nrights. There is a cruc ial area for improvement here, the aim of which must be to compel all \\nactors in the ecosystem of facial recognition technologies to better apply the existing \\nframework as early as in the experimentation and marketing phases. The evolving nature of \\nthese technologies must also be taken into account, in order to guarantee the robustness of a \\nprotective framework over time and prevent it from quickly becoming obsolete. \\nMoreover, there are strong geopolitical issues at stake in the deployment of facial recognition \\ntechnologies, ranging from competition in the international market, to the defense of European \\nsovereignty. This debate offers a unique opportunity for Europe to impose its own rules in'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 45, 'page_label': '46'}, page_content=\"sovereignty. This debate offers a unique opportunity for Europe to impose its own rules in \\norder not only to increase its competitivity compared with the Uni ted States and China, but \\nalso to build a strong European path, one that reflects our values.  \\nThe European standardization approach therefore stands out as the best way forward. \\nDeveloping our own standards would give us a chance to put respect for fundam ental rights \\nat the heart of the deployment of facial recognition technologies. Such an initiative at the \\nEuropean level would also encourage the harmonization and efficient application of the \\nexisting legal framework, while guaranteeing the technological independence of the European \\nUnion. Indeed, in the domain of facial recognition, it is high time for the EU to free itself from \\nthe US's grip on the international standardization market. \\n3.1. The NIST’s dominance over international standards \\n3.1.1. The reasons for this predominance: an internationally recognized\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 45, 'page_label': '46'}, page_content='3.1.1. The reasons for this predominance: an internationally recognized \\nauthority without a European equivalent \\nWith regard to biometrics, the National Institute for Standards and Technology (NIST) became \\nthe international reference centre for standardization in the 2000s. Together with industry, this \\nagency of the US Department of Commerce regularly conducts evaluations of algorithms and \\ndevelops norms and standards that are then exported internationally. Because of this close \\ncollaboration with industry and given the bridges it has built with the academic world, the NIST \\nis recognized as the most competent body in this domain. In fact, it is frequently solicited by \\nthe American government to carry out missions to evaluate the performance of algorithms, \\nincluding in the area of facial recognition. Thus, great legitimacy is attributed to algorithms that \\nare well placed in the rankings established by the NIST, and compliance with the norms and'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 45, 'page_label': '46'}, page_content='are well placed in the rankings established by the NIST, and compliance with the norms and \\nstandards developed by this American agency has become an absolute priority for many'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 46, 'page_label': '47'}, page_content=\"47/61 \\nproducers (not only American, but also European, Russian and Chinese 125) of facial \\nrecognition devices. The results of the famous Facial Recognition Vendor Test (FRVT) are \\nregularly cited by technology providers as a measure of their credibility and by policy maker s \\nas a guarantee of their quality to justify usage126.  \\nWith the diffusion in Europe of technologies based on algorithms evaluated by the NIST, the \\nUS agency's testing criteria have gradually become the reference at the EU level. As a result, \\nthe evaluation criteria established by the NIST are now often put forward in European calls for \\ntender127. Moreover, the European companies, including French companies, which have \\nestablished themselves as leaders in the international market for facial recognition \\ntechnologies are those which have accepted the American standards as their measuring stick. \\nIf these firms have been able to access the international market and gradually gain market\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 46, 'page_label': '47'}, page_content=\"If these firms have been able to access the international market and gradually gain market \\nshares, it is by complying with the NIST’s standards. \\nThis predominance is also made possible by the absence of a European equivalent to \\nthe NIST. The European Committee for Standardization (CEN), which brings together the \\nEuropean national standardization bodies (for example AFNOR for France 128), has less \\ninfluence in the development  of supranational standards. This difference in impact between \\nthe NIST and the CEN is linked to several factors. On the one hand, the CEN is not a \\ngovernmental agency. It relies on contributions from its members and, to a lesser extent, from \\nthe European Commission, for its functioning. It therefore has a relatively limited budget. On \\nthe other hand, when it comes to evaluation exercises, the US agency has access to the U.S. \\ngovernment's vast biometric databases (provided in particular by the FBI, the Stat e\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 46, 'page_label': '47'}, page_content='government\\'s vast biometric databases (provided in particular by the FBI, the Stat e \\nDepartment and the Department of Homeland Security). Over time, the NIST has built up \\ntesting bases containing millions of biometric data. In Europe, where it is difficult to collect this \\nkind of data, such scale effects are impossible for the CEN. Last, the NIST is also working with \\ninternational standards committees to develop common standards, which further strengthens \\nits grip on the international standardization market. The NIST\\'s financial resources and high \\nlevel of expertise enable it to mobilize large delegations within these bodies, and to win the \\nsupport of smaller delegations with less expertise and more limited budgets. In particular, the \\nU.S. agency collaborates with the ISO -IEC Joint Technical Committee 1 and, more \\nspecifically, with its spe cialized subcommittee 37 dedicated to biometrics (see the \"ISO -IEC \\nSC-37\" box).'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 46, 'page_label': '47'}, page_content='SC-37\" box).  \\n \\n \\n \\n \\n \\n125 “Technology: how the US, EU and China compete to set industry standards”, Financial Times, 24 July 2019: \\nhttps://www.ft.com/content/0c91b884-92bb-11e9-aea1-2b1d33ac3271  \\n126 “How the US plans to crack down on Chinese facial recognition tech used to ‘strengthen authoritarian \\ngovernments’ ”, This Week in Asia, 18 June 2019: https://www.scmp.com/week-\\nasia/geopolitics/article/3014868/how-us-plans-crack-down-chinese-facial-recognition-tech-used  \\n127 During the hearings conducted for this report, one industry member pointed out, for example, that questions \\nsuch as: \"Is your system referenced in the NIST’s FRVT?\" are found in European tenders.  \\n128 For a complete list of CEN members, see: \\nhttps://standards.cen.eu/dyn/www/f?p=204:5:0::::FSP_ORG_ID,FSP_LANG_ID:,34&cs=1177845D46C9904580C\\nCC631EC8FE906F'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 47, 'page_label': '48'}, page_content='48/61 \\nThe SC-37 of ISO/IEC129 \\nIn addition to the CEN, some European national standards bodies also meet in the \\nframework of Joint Technical Committee 1 of the ISO-IEC international standardization body \\ndedicated to information technology, and more specifically in the SC -37, the subcommittee \\ndedicated to biometrics. This subcommittee is organized around six working groups, \\nincluding one on \" cross jurisdictional and societal aspects of biometrics \"130. Each country \\nrepresented within the SC-37 delegates a team through its sole ISO member (ANSI for the \\nUnited States, AFNOR for France, Deutsches Institut für Normung for Germany, the British \\nStandards Institution for the United Kingdom, etc.) 131. These teams participate in the \\ntechnical discussions of the subcommittee through written contributions. The statutes of the \\nISO/IEC committee provide for equal representation among countries (that is, without'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 47, 'page_label': '48'}, page_content='ISO/IEC committee provide for equal representation among countries (that is, without \\ndeciding vote casting in the decision-making process and therefore with representativeness \\nregardless of the size of the delegation). However, the size of the delegations is not \\nirrelevant in the decision -making mechanism. Indeed, the details of technical discussions \\noften lead to arbitration, so that within the SC -37, the practice is to make decisions on the \\nbasis of the majority of the votes of the  delegations present at the meeting. In the absence \\nof a strong point of view (due to the narrow scope of competence), small delegations are \\ninclined to adopt a mimetic approach and to base their votes on personal confidence in \\ncertain delegates or delegations, or even on the surrounding political context. \\nThe financial aspect is not neutral either in the operation of these international committees \\nwhich, in order to develop a standard, rely on three to four annual meetings, sometimes'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 47, 'page_label': '48'}, page_content='which, in order to develop a standard, rely on three to four annual meetings, sometimes \\nspread over several ye ars. Only large companies investing in biometrics, institutions of a \\ncertain size (the NIST, the Fraunhofer Institute) and governments significantly involved, \\nmobilize delegates in their representation at the SC-37. The U.S. delegation is always very \\nlarge. In addition to the representatives of various federal departments, it includes most of \\nthe American biometric manufacturers, who see this as an opportunity to promote their \\nknow-how in the construction of biometric standards. \\nSince its inception in 2002,  the SC -37 has developed no less than 130 international \\nstandards for biometrics. While most of these standards are not in use (for example, the \\nbiometric sensor API 132 standard has never been as successful as hoped), some of them \\nare effective and widely applied worldwide. This is the case of the ISO/IEC 19794-2, 19794-'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 47, 'page_label': '48'}, page_content='are effective and widely applied worldwide. This is the case of the ISO/IEC 19794-2, 19794-\\n4, 19794-5 and 19794-6 standards on biometric data exchange formats, which are almost \\nsystematically included in calls for tenders involving biometrics. Standard 19794 -5, for \\nexample, defines the criteria to be met for the photos used on our ID cards133.  \\nThis is therefore a crucial issue for the European Union, which must mobilize the means to \\ninvest fully in these bodies. In order to lend greater force to this action, the European strategy \\nmust be collective, rather than the result of isolated players from different member states.  \\n \\n \\n129 International standardization Organisation (ISO)/International Electronical Commission (IEC). \\n130  For more information on SC-37, see: https://www.iso.org/committee/313770.html \\n131 For the complete list of SC-37 members, see: https://www.iso.org/committee/313770.html?view=participation  \\n132 Application programming interfaces.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 47, 'page_label': '48'}, page_content='132 Application programming interfaces. \\n133 See: https://www.diplomatie.gouv.fr/IMG/pdf/depliant_norme_photo-2.pdf'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 48, 'page_label': '49'}, page_content='49/61 \\n3.1.2. This predominance must be questioned  \\nBehind this predominance lie two challenges for Europe: to maintain a certain technological \\nindependence from the United States, and to adopt values that are not necessarily ours. In \\nthe global digital ecosystem, the predominance of the United States and C hina, and \\nEurope\\'s relative technological lag, raises the question of the control of data. This issue \\nis particularly acute in the context of facial recognition technologies, which are based on the \\nprocessing of biometric data, which are among the most sensitive data.  \\nFurthermore, the criteria established and used by the NIST, notably in the Face Recognition \\nVendor Test (FRVT), widely considered the standard measure for determining the reliability of \\nfacial recognition software 134, are exclusively technical . As explained in the report \"Face \\nRecognition Vendor Test (FRVT) Part 3: Demographic Effects\" 135 published in December'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 48, 'page_label': '49'}, page_content='Recognition Vendor Test (FRVT) Part 3: Demographic Effects\" 135 published in December \\n2019, the performance of systems subject to the FRVT is mainly analyzed according to one \\ncriterion: accuracy136. This accuracy is reported by the American agency through an error rate, \\nor the number of Type I or \"false positive\" errors (when an individual is incorrectly associated \\nwith another person) and Type II or \"false negative\" errors (when an individual is not \\nassociated with him or herself) committed by the algorithms in question. Algorithm execution \\ntime and \"demographic differentials\" (variations in precision based on demographic group 137) \\nare also taken into account. At the end of the test, the algorithms are ranked by the NIST, from \\nthe most to the least performant138, based on these criteria.  \\nHowever, when it comes to facial recognition technologies, the reliability of a system \\ncannot simply be measured by its technical performance. If we consider the protection of'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 48, 'page_label': '49'}, page_content='cannot simply be measured by its technical performance. If we consider the protection of \\npersonal data and  respect for fundamental rights as essential attributes, then reliable facial \\nrecognition technology is not only a high -performance technology. It is a technology we can \\ntrust because it will not lead to the usurpation of our identity, the uncontrolled sha ring of our \\nbiometric data, the intrusion into our privacy without prior consent, or the massive remote \\nsurveillance of our actions. Despite recent initiatives by the U.S. Senate to prohibit companies \\nthat do not respect human rights from submitting their algorithms to the FRVT139, there is still \\na long way to go. In any case, taking these criteria into account means establishing intrinsically \\nEuropean specifications that correspond to our vision of the digital society: a society that is \\ninclusive and respectful of fundamental rights and freedoms.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 48, 'page_label': '49'}, page_content='inclusive and respectful of fundamental rights and freedoms.  \\nAs was the case in the field of personal data protection, this situation offers an \\nunprecedented opportunity for Europe to break free from American dominance  by \\nproposing and promoting its own standards, in order to effectively protect the rights of its \\ncitizens.  \\n \\n134 “How the US plans to crack down on Chinese facial recognition tech used to ‘strengthen authoritarian \\ngovernments’ ”, This Week in Asia, 18 June 2019: https://www.scmp.com/week-\\nasia/geopolitics/article/3014868/how-us-plans-crack-down-chinese-facial-recognition-tech-used  \\n135 National Institute of Standardization and Technology (2019), “Face Recognition Vendor Test (FRVT) Part 3: \\nDemographic Effects”, U.S. Department of Commerce, 82 pp. \\n136 Ibid., p. 20. \\n137 The criteria used to assess these \"demographic differentials\" are the age, sex and place of birth (referring to \\nethnic origin) of individuals.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 48, 'page_label': '49'}, page_content='ethnic origin) of individuals.  \\n138 For an example of a NIST ranking, see: https://pages.nist.gov/frvt/html/frvt11.html  \\n139 “Senators introduce bill to regulate facial recognition technology”, The Hill, 14 March 2019: \\nhttps://thehill.com/policy/technology/434166-bipartisan-senators-introduce-bill-to-regulate-facial-recognition'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 49, 'page_label': '50'}, page_content='50/61 \\n3.2. Making European standards a lever for protecting citizens \\nAs stated in its preamble, the purpose of the Charter of Fundamental Rights of the Europea n \\nUnion is \"to strengthen the protection of fundamental rights in the light of changes in society, \\nsocial progress and scientific and technological developments\". If the European Union wants \\nto ensure that its values and principles are effectively protected and taken into account in the \\ndeployment of facial recognition technologies, it must invest more heavily in the global \\nstandardization race. In order to assert itself, the European Union can count on its soft power \\n(see the \"The Brussels effect\" box), as  has been the case with the adoption of the GDPR. \\nHowever, this framework suffers from variations in its application between countries. This is \\nwhy bringing together the initiatives of the different member states around this standardization'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 49, 'page_label': '50'}, page_content='why bringing together the initiatives of the different member states around this standardization \\nmust be a prior ity. This is all the more important since, as highlighted by the European \\nCommission in its White Paper on Artificial Intelligence, some EU members are already \\nembarking on unilateral initiatives to regulate AI applications140. \\n \\nThe ‘Brussels effect’ \\nIn the global race for standardization, the European Union stands out from the two other \\nmajor leaders in the digital market, which are the United States and China. While China has \\nadopted an aggressive strategy to push the spread of its standards worldwide, and  the \\nTrump administration has engaged in a genuine economic war with China, the European \\nUnion is relying on its soft power and more particularly on the ‘Brussels effect’ to impose its \\nvalues141. This term, coined by a journalist at the Financial Times, ref ers to the fact that'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 49, 'page_label': '50'}, page_content=\"values141. This term, coined by a journalist at the Financial Times, ref ers to the fact that \\ncertain rules laid down by the EU (particularly in the automotive, chemical and food \\nindustries) have gradually been adopted worldwide. The GDPR is the most recent example \\nin the digital domain: many countries around the world are impl ementing laws on the \\nregulation of personal data that are strongly inspired by this EU regulation. This is also the \\ncase of California, which frequently takes the lead in regulation in the United States 142. It \\nremains to be seen whether the EU will be able to use this 'Brussels effect' to make its mark \\nin the global market for the standardization of facial recognition technologies.  \\n \\n3.2.1. Accounting for both technical and legal aspects \\nWhereas the American standards currently prevailing on the market are based exclusively on \\ncharacteristics relating to the technical performance of algorithms, Europe must distinguish\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 49, 'page_label': '50'}, page_content='characteristics relating to the technical performance of algorithms, Europe must distinguish \\nitself by introducing a legal dimension into its standards . Taking these  aspects into \\naccount is essential if we are to ensure that the development of facial recognition technologies \\nis respectful of European values. Moreover, the current standards, which are based on simple \\nrankings, can be perfected even from an algorithmic performance perspective. In the process \\n \\n140 European Commission (2020), \"Artificial Intelligence: A European approach based on excellence and trust\", p. \\n10: \"The German Data Ethics Commission has called for a five-level risk-based system of regulation that would \\ngo from no regulation for the most innocuous AI systems to a complete ban for the most dangerous ones. \\nDenmark has just launched the prototype of a Data Ethics Seal. Malta has introduced a voluntary certification'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 49, 'page_label': '50'}, page_content='system for AI. If the EU fails to provide an EU-wide approach, there is a real risk of fragmentation in the internal \\nmarket, which would undermine the objectives of trust, legal certainty and market uptake\". \\n141 “Technology: how the US, EU and China compete to set industry standards”, Financial Times, 24 July 2019:  \\nhttps://www.ft.com/content/0c91b884-92bb-11e9-aea1-2b1d33ac3271  \\n142 Ibid.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 50, 'page_label': '51'}, page_content='51/61 \\nof developing European standards for facial recognition technologies, it is therefore essential \\nnot to neglect these aspects. \\nWhatever the ultimate use of facial recognition technologies, it is important to ensure that the \\nalgorithms are fair, that is that they perform the tasks for which they were designed as \\neffectively as possible.  \\nIn this regard, although the NIST takes into account a certain number of criteria in its FRVT \\n(error rates, execution time, demographi c differentials) in order to evaluate the performance \\nof algorithms in relation to each other and the performance of a given algorithm over time, the \\nU.S. agency does not issue technical certifications. The NIST evaluations are not intended to \\nmean \"this s ystem conforms to our standards, while this one does not\". Instead, algorithms \\nare ranked from most to least efficient. So how can we determine which systems have an \\n\"acceptable\" level of performance for large -scale deployment? Should the top 100 be'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 50, 'page_label': '51'}, page_content='\"acceptable\" level of performance for large -scale deployment? Should the top 100 be \\nconsidered? The top 500?  The European standards must therefore take the technical \\ncriteria used by the NIST as a basis, but also introduce evolving thresholds.  For each \\ncriterion analysed, whether it be the error rate, execution time or demographic differentials , \\ndefining a threshold below which the system is deemed to be non -compliant would result in \\nmore precise standards and would allow to set up a real certification mechanism. Algorithms \\nclassified below the threshold (for example, those whose results in term s of managing \\ndiscriminatory biases are deemed too low) would not receive certification. In order to take into \\naccount technological developments over time, such thresholds need to be adaptable.   \\nWhile these technical aspects should form the first pillar of European standards for facial'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 50, 'page_label': '51'}, page_content='While these technical aspects should form the first pillar of European standards for facial \\nrecognition technologies, the second pillar should deal with legal aspects. For the time being, \\nthese are completely absent from the standards established by the NIST.  \\nIn April 2019, the High-Level Expert Group on Artifici al Intelligence, set up by the European \\nCommission, published its \"Ethical Guidelines for Trustworthy AI\" 143. In the document, the \\nexperts identify seven essential requirements for AI, namely: (1) human action and human \\ncontrol; (2) technical robustness and  security; (3) privacy and data governance; (4) \\ntransparency; (5) diversity, non -discrimination and fairness; (6) societal and environmental \\nwell-being; and (7) accountability144. More recently, in its White Paper on Artificial Intelligence, \\nthe European Commission has taken up a number of these requirements and clarified them'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 50, 'page_label': '51'}, page_content='the European Commission has taken up a number of these requirements and clarified them \\nfor so-called \"high -risk\" AI applications. Among the Commission\\'s recommendations is the \\nneed to establish specific requirements for remote biometric identification, including \\ncompliance with the Charter of Fundamental Rights145.  \\nAlthough what exactly constitutes a \"high -risk\" artificial intelligence application remains to be \\ndefined, the European Commission refers in its White Paper to the example of the deployment \\nof facial recognition technologies in public places 146. However, in an approach based on \\nstandardization, there is no need to distinguish between uses. All use cases, regardless of \\n \\n143 High Level Group of Independent Experts on Artificial Intelligence set up by the European Commission in \\nJune 2018 (2020), \"Ethical Guidelines for Trustworthy AI\": https://ec.europa.eu/futurium/en/ai-alliance-\\nconsultation/guidelines#Top \\n144 Ibid., p. 3.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 50, 'page_label': '51'}, page_content='consultation/guidelines#Top \\n144 Ibid., p. 3.  \\n145 European Commission (2020), \"Artificial Intelligence: A European approach based on excellence and trust\", \\nCommunication, COM(2020) 65 final, p. 21.: https://ec.europa.eu/info/sites/info/files/commission-white-paper-\\nartificial-intelligence-feb2020_en.pdf  \\n146 Ibid., p. 25.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 51, 'page_label': '52'}, page_content='52/61 \\ntheir degree of sensitivity, should respect the above-mentioned criteria. In other wor ds, any \\ndeployment (including experimentations) of a facial recognition system must pass a \\ntest with regard to fundamental rights. As such, each deployment must first: \\n● be provided for by law; \\n● genuinely meet objectives of general interest recognized by the European Union, or \\nthe need to protect the rights and freedoms of others; \\n● respect the essence of rights and freedoms, that is, the inalienable core of the right \\nconcerned; \\n● be necessary (principle of necessity); \\n● respect the principle of proportionality (which requires passing the “triple test”)147. \\nIn fine , only technologies that bring together all these dimensions would be considered \\nrespectful of our fundamental rights and could be implemented.        \\n \\nTable 2 - Essential criteria for Eu ropean standards applicable to facial recognition \\ntechnologies148'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 51, 'page_label': '52'}, page_content='technologies148 \\nTechnical Criteria For all uses: criteria established by the NIST (error rate, \\nspeed of execution, demographic differentials) enriched \\nwith evolving performance thresholds \\nLegal criteria For all uses: respect for the EU Charter of Fundamental \\nRights \\n \\nDepending on the practice: compliance with the GDPR \\nor the Law Enforcement Directive and other norms \\nspecific to the member states (for example, in France, \\nthe Loi Informatique et Libertés) \\n \\n3.2.2. Ensuring the adoption of European standards by enforcing \\ncompliance in public procurement contracts \\nIn any event, the definition of standards to accompany the deployment of facial recognition \\ntechnologies on the European continent cannot be an end in itsel f. These standards must \\neffectively fulfill their mission: to guarantee harmonization and efficiency in the application of \\nthe existing legal framework. In the practical implementation of the standardization system,'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 51, 'page_label': '52'}, page_content='the existing legal framework. In the practical implementation of the standardization system, \\nthought must therefore be given to how t o encourage compliance with and dissemination of \\nstandards. As standards are voluntary, this must be part of a performative approach. \\nThe first step in this process would be to impose compliance with standards in the context \\nof European, national and local  public procurements, including for experimentations . \\nIn concrete terms, this means awarding public contracts only to organizations that comply with \\n \\n147 According to the “triple test”, a measure restricting fundamental rights must be appropriate, necessary and \\nproportionate. \\n148 It should be noted that we do not mention among the essential criteria certain major principles inherent to all \\ntechnologies based on artificial intelligence or to digital technology in general. However, it goes without saying'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 51, 'page_label': '52'}, page_content='that, as far as possible, the facial recognition technologies used on the continent must also integrate the principle \\nof \"green technology\" into their operation, in accordance with the European Green Deal presented by the \\nEuropean Commission in December 2019.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 52, 'page_label': '53'}, page_content='53/61 \\nthe standards in question for all facial recognition technologies, regardless of the degree of \\nrisk. This o bligation would make it possible to guard against initiatives that are currently \\nemerging in territories without sufficient supervision and without any attempt to find \\nalternatives. To this end, calls for tenders should contain criteria for assessing the p roposed \\nsolutions against European standards. Failure to comply with the standards would be \\ndetrimental to the operators present on the market (and those wishing to enter it), and the \\nlatter would be encouraged to comply with it either by taking them into account during the \\nsystem development phase or by bringing existing systems into conformity.  \\nThis \"levelling up\" should have a performative effect and enable European standards to \\nbecome the benchmark for the deployment of facial recognition technologies within the EU,'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 52, 'page_label': '53'}, page_content='become the benchmark for the deployment of facial recognition technologies within the EU, \\nwhether their manufacturers are based there or not, for both public and private  contracts. \\nFurthermore, in addition to their dissemination, the imposition of standards in the context \\nof public procurement would provide an effective framework for public surveillance.  \\nThe final stage in the adoption of European standards for facial rec ognition would be their \\ninternational dissemination through the famous ‘Brussels effect’.  \\nHowever, all of this requires the ability to monitor compliance with the standards at the EU \\nlevel, which means that European bodies must be able both to set and audit these standards. \\n3.3. A European governance dedicated to the standardization of facial \\nrecognition technologies \\nWhile the current framework for facial recognition technologies is characterized by a disparate \\napplication at the EU level,  the development of a common reference framework for all'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 52, 'page_label': '53'}, page_content='application at the EU level,  the development of a common reference framework for all \\nmember states inevitably requires that all relevant actors pool their knowledge through \\na multi-stakeholder body responsible for these standards. \\n3.3.1. Gathering expertise within a multi-stakeholder body  \\nAlthough it should be revisited, the ecosystem of actors likely to be involved in the control of \\nEuropean standards relating to the deployment of facial recognition technologies does not \\nneed to be completely built from scratch. For the time being, this ec osystem is essentially \\nmade up of a network of national data protection authorities and national standardization \\nagencies that we find at the European level within various authorities. The body responsible \\nfor European standards in the field of facial reco gnition should rely on these organizations to \\ndraw up a common frame of reference for all member states149.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 52, 'page_label': '53'}, page_content='draw up a common frame of reference for all member states149. \\nIn particular, such a body could rely on the European Committee for Standardization (CEN), \\nwhich brings together the national standardization organizations of the member states and \\nwhose primary mission is the production of European safety and quality standards. This body \\nhas been working on facial recognition technologies for several years, but this work merits an \\nupdate. In its 2016 annual report, th e Committee announced that it had approved a work \\nprogram aimed at reaching \"an agreement on the upcoming drafting of a European Standard \\non ‘Privacy protection by design and by default’ and on sector -specific guidelines for video \\n \\n149 This principle, according to which priority should be given to existing bodies, was also reiterated on 12 May in \\nthe framework of the European Commission\\'s JURI Committee by the shadow rapporteurs of the PPE, Renew,'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 52, 'page_label': '53'}, page_content='ECR and ID Groups. The latter were opposed to the idea of creating a \"European agency for AI\" defended by \\nMEP Iban Garcia del Blanco (S&D) and proposed instead to rely on existing authorities. See: \\nhttps://multimedia.europarl.europa.eu/en/juri-committee-meeting_20200512-0900-COMMITTEE-JURI_vd'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 53, 'page_label': '54'}, page_content='54/61 \\nsurveillance (CCTV) and biometric measures for access control including face recognition\"150. \\nWhile the area of privacy protection by design and by default has since been taken over by \\nthe European Data Protection Board (EDPB) 151, the area of biometrics seems to have been \\nsomewhat neglected152.  \\nIn addition to the national standardization organizations, it is crucial to involve representatives \\nof the EDPB153, i.e. national data protection authorities (the CNIL in France and its European \\ncounterparts)154, in this body. The protection of biometric data - highly sensitive data - must \\nindeed be at the heart of the European standardization system.  \\nAs the development of European standards must also account for the respect for fundamental \\nrights, it is essential that the representatives of the  CEN and the EDPB work closely with the \\nexperts of the EU Fundamental Rights Agency within this body. The Agency provides'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 53, 'page_label': '54'}, page_content='experts of the EU Fundamental Rights Agency within this body. The Agency provides \\nindependent expert advice and analysis on fundamental rights to EU institutions and member \\nstates. It is the body best placed to contr ibute to the mainstreaming of the “triple test”. In \\naddition, the EU Agency for Fundamental Rights has a particularly close working relationship \\nwith the national authorities responsible for defending rights, who must be involved in the \\ndevelopment of Euro pean standards for facial recognition technologies. In France, the \\nDéfenseur des droits notably protects the rights of users of public services and the rights of \\nthe child, and actively fights against discrimination.  \\nThis pooling of knowledge and skills w ith a view to drawing up European standards and \\nmaking them understandable both for the industry and supervisory authorities could require \\nthe creation of working groups, each dedicated to a specific topic (e.g. technical aspects, data'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 53, 'page_label': '54'}, page_content='the creation of working groups, each dedicated to a specific topic (e.g. technical aspects, data \\nprotection, fundamental rights, cross-cutting rights, transparency, etc.). \\nFor this system to be truly comprehensive and democratic, the standardization body\\'s \\ndiscussions should also include consultation with civil society (think tanks 155, consumer \\nassociations, advocacy groups), the research community, businesses and public authorities, \\non the implementation of standards and their further development (see Figure 3 \"The structure \\nof the European standardization body for facial recognition technologies\"). In order to avoid \\nredundancy, this work should also be carried out hand in hand with the relevant Directorates-\\nGeneral of the European Commission156. \\n \\n \\n \\n \\n \\n \\n \\n \\n150 European Committee for standardization (2017), Annual Report 2016, p. 9: \\nhttps://www.cen.eu/news/brochures/brochures/Annual_Report_2016_Tome_1_accesibility.pdf'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 53, 'page_label': '54'}, page_content='https://www.cen.eu/news/brochures/brochures/Annual_Report_2016_Tome_1_accesibility.pdf \\n151 European Data Protection Board (2019), “2018 Annual Report: Cooperation & Transparency”, p. 25: \\nhttps://edpb.europa.eu/sites/edpb/files/files/file1/edpb_annual_report_2018_-_digital_final_1507_en.pdf  \\n152 There is no mention of this in the 2017 and 2018 NEC activity reports. \\n153 Not to be confused with the European Data Protection Supervisor (EDPS).  \\n154 For a complete list of EDPB members, see: https://edpb.europa.eu/about-edpb/board/members_en \\n155 For example, the Biometric Institute, which is doing a lot of work on the subject. \\n156 For example, DG Justice is currently working on the development of a standard for fingerprint recognition.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 54, 'page_label': '55'}, page_content='55/61 \\nFigure 3 - The structure of the European standardization body for facial recognition \\ntechnologies \\n \\nNot only does monitoring adherence to standards at the EU level require a body dedicated to \\ntheir elaboration, but it also requires that these standards be auditable. \\n \\n3.3.2. Putting auditability at the heart of the standardization system \\nAuditability is the foundation of any standardization system. If our standards are not auditable, \\nthen we have no way of monitoring their compliance. Establishing these standards should \\nmake it possible to draw up a certification reference framework157 common to all the member \\nstates of the EU. This certification reference frame for European facial recognition \\ntechnologies must include the list of requirements to be verified, translating in a clear and \\naffordable manner the standards established by consensus within the standardization body.  \\nOnce the reference framework has been established, that is once the legal principles (in'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 54, 'page_label': '55'}, page_content='Once the reference framework has been established, that is once the legal principles (in \\nparticular the “triple test”) and the technical aspects have been translated into practical \\nrequirements, it becomes possible for a European inspection body to audit a device with a \\n \\n157  That is to say, a \"technical document defining the characteristics that an industrial product or service must \\nhave and the arrangements for checking its conformity with those characteristics\". See Ministry of Economy, \\nFinance and Industry (2004): \"La certification en 7 questions des produits industriels et des services\", p.4.:  \\nhttps://evaluation.cstb.fr/doc/certification/certification-en-7-questions.pdf'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 55, 'page_label': '56'}, page_content='56/61 \\nview to its certification. This is where the idea of imposing standards in public procurement \\ntakes on its full meaning. If a manufacturer of facial recognition technology hopes that their \\ndevice will be selected in a public tender, then they will have every interest in having it certified \\nby a competent independent body. Without such certification, their application will not be \\nsuccessful158. This mechanism allows both the company and the authority using a facial \\nrecognition technology to prove that it meets the established standards, and to assure citizens \\nthat the device to which they are subject is trustworthy (not only from a technical point of view, \\nbut also from an ethical point of view).  \\nIt should be noted here that cert ification is granted for a limited period of time, during which \\nthe certifying body carries out monitoring. Furthermore, since facial recognition technologies'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 55, 'page_label': '56'}, page_content='the certifying body carries out monitoring. Furthermore, since facial recognition technologies \\nare devices that are constantly evolving with innovation, perhaps a notification mechanism \\nshould be considered to alert the certifying body about changes over time in a technology that \\nit has certified. This would involve the manufacturer of the technology notifying the body of \\nany significant changes to the device in question, with a view to reassessing the certification. \\nThis monitoring should focus on significant changes to the functionality of the product that may \\nsignificantly affect its performance in testing or the nature of the safety information to be \\nprovided. Updates such as security patches or simple enhancements should not trigger a new \\nrisk assessment after a technology has been placed on the market.  \\nHowever, this compliance check requires the ability to compare the algorithms on which facial'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 55, 'page_label': '56'}, page_content='However, this compliance check requires the ability to compare the algorithms on which facial \\nrecognition technologies are based with large , centralized image databases, which is \\nparticularly difficult in the EU at present. Although regulation allows this under certain \\nconditions, several principles of the GDPR hinder the creation of large centralized databases. \\nTo this end, it is crucial for the European Union to develop a doctrine to encourage innovation \\nin artificial intelligence, while at the same time preserving the core principles of the GDPR. \\nBeyond the implementation of this certification mechanism based on third -party auditing \\n(independent certifying offices/bodies), having a reference system at the European level would \\nalso allow self-auditing of companies developing and/or using facial recognition technologies. \\nThe latter must be able to appropriate the reference system in order to carry out a priori impact'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 55, 'page_label': '56'}, page_content='The latter must be able to appropriate the reference system in order to carry out a priori impact \\nassessments. The possibility of this self-assessment in relation to the standard is all the more \\nnecessary since the almost complete disappearance of the system of prior authorization with \\nthe entry into force of the GDPR. While it is not currently the case, we could imagine requiring \\nthe transmission of the results of these impact assessments to the national supervisory \\nauthorities (to the CNIL, for example), so that they can issue opinions. However, self -\\nauditability is not intended to replace third party certification. It is a voluntary approach on the \\npart of the manufacturer enabling them to take into account the requirements of the European \\nstandards from the design stage. Since the certification process has a cost, it is essen tial for \\na manufacturer to make sure that their certification application has the best possible chance \\nof being accepted.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 55, 'page_label': '56'}, page_content='of being accepted.     \\nFinally, in addition to third-party certifiers and technology providers, the authorities responsible \\nfor monitoring compliance wit h the standards must also be able to appropriate the standard. \\n \\n158 It should be noted that care should be taken to ensure that the standardization system does not become a \\nbarrier to new entrants. All companies, from very small businesses to multinationals, must be able to develop \\ntechnologies that comply with the standards. Hence the need to consult also the smaller players in the \\nimplementation and further development of the standards (see diagram \"The structure of the European \\nstandardization body for facial recognition technologies\").'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 56, 'page_label': '57'}, page_content='57/61 \\nThis is indispensable not only for the purpose of generalizing the use of the “triple test” (legal \\naspect), but also to increase the effectiveness of the supervision of technical aspects. \\nConsequently, regulators must step up their efforts. Carrying out impact assessments on \\ncomplex technologies is an extremely time -consuming task which requires extensive \\nresources, not only budgetary but also (and just as importantly) human resources (highly \\nqualified staff). As things stand at present, these resources are far from assured 159. There is \\nan urgent need for member states to show real political will and provide their supervisory \\nauthorities with the resources they need. In addition to financial resou rces, this also requires \\na significant training effort.  \\nWhile the establishment of this European standardization system emerges as the option most \\nlikely to guarantee a deployment of facial recognition technologies that respects European'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 56, 'page_label': '57'}, page_content='likely to guarantee a deployment of facial recognition technologies that respects European \\nvalues, achieving  this project will not be easy. Increased cooperation between national \\nauthorities within a European body, as well as investment (financial and human resources) by \\nmember states appear to be the sine qua non  conditions for the success of such an \\nundertaking. Failure in this mission would contribute to the erosion of European digital \\nsovereignty and to the potential undermining of the democratic guarantees of the rule of law. \\nIt is thus not an option.   \\n \\n159 Brave, op. cit.'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 57, 'page_label': '58'}, page_content=\"58/61 \\nConclusion - The EU's opportunity to place humans at the heart of \\nthe system \\n  \\nIn light of the current predominance of US standardization, and as the deployment of facial \\nrecognition devices accelerates internationally, it is crucial that the European Union build a \\nsystem that guarantees its values. It is estimated that by 2024, the market for facial recognition \\ntechnologies will generate revenues of $7 billion (more than double the $3.2 billion recorded \\nfor 2019)160.  \\nBeyond the guarantee of citizens' rights, there is also, in the implementation of European  \\nstandards applicable to facial recognition technologies, an important issue of digital \\nsovereignty.  \\nHowever, the European strategy cannot be based solely on the implementation of auditable \\nEuropean standards.  In addition to establishing a standardization system that \\nguarantees a trusted technology, the essential issue is to give control to humans.\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 57, 'page_label': '58'}, page_content='guarantees a trusted technology, the essential issue is to give control to humans.  \\nWhether they are public or private players, users of facial recognition technologies play a \\nmajor role in the deployment of these technologies across territorie s. As such, the highly \\nsensitive and intrusive nature of these technologies must always prompt the question of an \\nalternative. It is also necessary to ensure, including in the private sector, that individuals are \\nenabled to interact with these technologies  in the best possible way. In this respect, it is the \\nresponsibility of users to explicitly inform citizens about the deployment of a facial recognition \\ndevice, so that they can make a conscious decision as to whether or not to subject their face \\nto biomet ric processing. An awareness campaign at the European level should also be \\ndeployed, with the aim of informing citizens and enabling them to exercise their rights. It is'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 57, 'page_label': '58'}, page_content='deployed, with the aim of informing citizens and enabling them to exercise their rights. It is \\nessential that each individual who is subjected (voluntarily or involuntarily) to a fa cial \\nrecognition device understand their rights and remedies, where their data are sent, for what \\npurposes, by whom they are processed, for how long, what risks they run, etc.  \\nOn February 20 of this year, the European Commission launched a public consulta tion on its \\nWhite Paper on Artificial Intelligence, announcing (among other things) its intention to launch \\na wide-ranging debate on facial recognition technologies. Renaissance Numérique hopes that \\nthe concrete proposals put forward in this paper will con tribute to an informed public debate \\nand to ensuring the deployment of facial recognition technologies in line with the values that \\nform the cornerstone of the European Union.  \\n  \\n \\n160 “Facial Recognition Market by Component (Software Tools (2D Recognition, 3D Recognition, and Facial'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 57, 'page_label': '58'}, page_content='Analytics) and Services), Application Area (Emotion Recognition, Access Control, and Law Enforcement), \\nVertical, and Region - Global Forecast to 2024”, Markets & Markets, June 2019: \\nhttps://www.marketsandmarkets.com/PressReleases/facial-recognition.asp'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 58, 'page_label': '59'}, page_content=\"59/61 \\nAcknowledgements \\n  \\nWe would like to thank the various actors who took part in  the consultations for their \\ncontribution, namely: \\n  \\nDidier Baichère, Deputy (“Député”) for the Yvelines Region \\nVincent Bouatou, Director of Innovation, Idemia \\nAntoine Courmont, Research Officer, CNIL \\nThéodore Christakis, Professor of International Law, University of Grenoble Alpes \\nMartin Drago, Lawyer, La Quadrature du Net \\nRaphaël de Cormis, Vice President of Innovation and Digital Transformation, Thalès \\nMarie Duboys Fresney, Legal Counsel, CNIL \\nArthur Messaud, Legal and Policy Analyst, La Quadrature du Net \\nHenri Verdier, French Ambassador for Digital Affairs \\n  \\nWe would also like to thank the office of the Défenseur des Droits and the Ligue des Droits de \\nl'Homme for our constructive discussions on these issues. \\n  \\nFinally, we would like to extend our warmest thanks to the speakers and participants who took\"),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 58, 'page_label': '59'}, page_content='Finally, we would like to extend our warmest thanks to the speakers and participants who took \\npart in the symposium on 19 December 2019 at the French Assemblée nationale , and \\nespecially to Jean -Michel Mis, Deputy ( “Député”) for the Loire Region, with whom we co -\\norganised this event. \\n  \\n  \\nFurther Resources \\n“Reconnaissance faciale : Ce que nous en disent les Français”, Renaissance Numérique \\n(December 2019) \\n  \\n“Reconnaissance faciale : Interdiction, expérimentation, généralisation, réglementation. Où en \\nest-on ? Où allons-nous ?”, summary of the symposium organised on 19 December, 2020 at \\nthe French Assemblée nationale, Renaissance Numérique (February 2020)'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 59, 'page_label': '60'}, page_content='60/61 \\nDirector of the publication \\n  \\nHenri Isaac, President, Renaissance Numérique \\nCamille Vaziaga, Head of Public Affairs, Microsoft France \\n  \\nCoordination \\n  \\nJennyfer Chrétien, Executive Director, Renaissance Numérique \\nJessica Galissaire, Studies Manager, Renaissance Numérique \\n  \\nAuthors \\n  \\nValérie Fernandez, Professor and Chairholder of the Responsible Digital Identity Chair, \\nTelecom Paris \\nJessica Galissaire, Studies Manager, Renaissance Numérique \\nLéo Laugier, PhD Student in Computer Science, Institut Polytechnique de Paris \\nGuillaume Morat, Senior Associate, Pinsent Masons \\nMarine Pouyat, Independent Consultant Expert in Data Protection, marine-talents.com  \\nAnnabelle Richard, Associate Lawyer, Technology, Media and Telecommunications Division, \\nPinsent Masons \\n  \\nThe working group \\n  \\nSarah Boiteux, Senior Public Affairs Analyst, Google France \\nHector de Rivoire, Head of Public Affairs, Microsoft France \\nEtienne Drouard, Associate Lawyer, Hogan Lovells'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 59, 'page_label': '60'}, page_content='Etienne Drouard, Associate Lawyer, Hogan Lovells \\nValérie Fernandez, Professor and Chairholder of the Responsible Digital Identity Chair, \\nTelecom Paris \\nLéo Laugier, PhD Student in Computer Science, Institut Polytechnique de Paris \\nGuillaume Morat, Senior Associate, Pinsent Masons \\nDelphine Pouponneau, Director of Diversity and Inclusion, Orange \\nMarine Pouyat, Independent Consultant Expert in Data Protection, marine-talents.com  \\nPhilippe Régnard, Director of Public Affairs for the Digital Branch, La Poste \\nAnnabelle Richard, Associate Lawyer, Technology, Media and Telecommunications Division, \\nPinsent Masons \\nThierry Taboy, Vice President Corporate Social Responsibility, Orange \\nAmal Taleb, Director of Public Affairs, SAP France \\nValérie Tiacoh, Communications Director for Corporate Social Responsibility, Orange'),\n",
       " Document(metadata={'source': 'data/eye_eurp.pdf', 'page': 60, 'page_label': '61'}, page_content='61/61 \\nAbout Renaissance Numérique \\n \\nRenaissance Numérique is France’s main independent think tank \\nfocusing on the challenges of the digital transformation of society. \\nBringing together universities, associations, corporations, start-ups \\nand schools, it aims to develop workable proposals to help public \\nstakeholders, citizens and businesses build an inclusive e-society. \\n \\n \\n  \\nRenaissance Numérique \\n22 bis rue des Taillandiers - 75011 Paris \\nwww.renaissancenumerique.org \\n  \\nJune 2020 \\nCC BY-SA 3.0')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=vector_store.similarity_search(\"what is deep learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='7baf2ab3-34cb-48e4-bd4e-0e43ee867f67', metadata={'page': 8.0, 'page_label': '9', 'source': 'data/eye_eurp.pdf'}, page_content=\"9/61 \\non deep convolutional neural network architectures 21 to push LFW accuracy to 99.8% in just \\nthree years (in other words, the error rate has been divided by 15 in comparison to Deepface). \\nThese results, emerging from both academic and industrial actors, are often published in \\nconference proceedings or peer-reviewed scientific journals. The source codes of the trained \\nalgorithms and models are also very often open (open access in open source), which tends to \\nencourage the deployment of these technologies.  \\n \\nDeep Learning \\nDeep learning is based on the training of so -called deep artificial neural network models22, \\nwhose breakthroughs in computer vision (notably in 2 012 when the AlexNet system 23 won \\nthe ImageNet competition24) earned its developers the Turing Prize (equivalent to the Nobel \\nPrize in Computer Science) in 2018. Deep learning is based on the biological process that \\nleads a young child's brain to:\"),\n",
       " Document(id='926f46c2-8af0-4723-bf5d-a14693fde665', metadata={'page': 8.0, 'page_label': '9', 'source': 'data/eye_eurp.pdf'}, page_content='leads a young child\\'s brain to: \\n● learn to recognize familiar faces by observing faces in a variety of contexts; \\n● Quickly extract and memorize apparent physical characteristics relevant to different \\nlevels of abstraction (haircut, eye color, scarring, expression of emotion, wearing an \\naccessory, etc.); \\n● associate them with people or groups of people; \\n● \"generalize\" the recognition of a face, i.e. recognize a face even in new contexts \\n(with a new expression, colored lighting, change of position/orientation, new haircut, \\nglasses, etc.).  \\n \\n \\n \\n \\n21 An artificial convolutional neural network is a type of artificial neural network in which neurons are connected \\nso as to calculate mathematical operation of convolution in order to reproduce the biological process observed in \\nthe visual cortex of animals. \\n22 Yann LeCun, Yoshua Bengio and Geoffrey Hinton (2015), “Deep learning”, Nature 521, pp. 436-444.'),\n",
       " Document(id='d9f116df-a54a-4c8b-a393-bbc25200a251', metadata={'page': 8.0, 'page_label': '9', 'source': 'data/eye_eurp.pdf'}, page_content='22 Yann LeCun, Yoshua Bengio and Geoffrey Hinton (2015), “Deep learning”, Nature 521, pp. 436-444. \\n23 Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton (2012), “Imagenet classification with deep \\nconvolutional neural networks”, Advances in neural information processing systems, pp. 1097-1105. \\n24 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li and Li Fei-Fei (2009), “ImageNet: A Large-Scale \\nHierarchical Image Database”, 2009 conference on Computer Vision and Pattern Recognition.'),\n",
       " Document(id='7e9d47ad-09b5-402d-a824-87d426423520', metadata={'page': 7.0, 'page_label': '8', 'source': 'data/eye_eurp.pdf'}, page_content='intelligence. Today, these techniques are studied through the approach of machine learning, \\na field at the intersection of artificial intelligence and data science. Most often, this learning is \\nsupervised: an algorithm trains a statistical model to recognize faces in images from a massive \\nvolume of annotated data (big data). \\nRecent research into facial recognition draws on relatively mature deep learning techniques \\n(see the box on \"Deep learning\").  Previously, pre -deep learning techniques had taken more \\nthan twenty years to increase accuracy from 60% to 90% on the benchmark Labeled faces in \\nthe wild (LFW)14, a reference tool used for conducting work on facial recognition. The \"deep\" \\nfacial recognition techniques now in use, which apply multiple cascading layers of image \\nprocessing in order to extract and transform physical features, have revolutionized the field \\nsince Facebook designed the Deepfa ce15 facial recognition system in 2014 . Deepface')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three steps of biometric facial recognition are:\n",
      "\n",
      "1. Enrollment phase: Capturing representative data of the diversity of contexts in which the intended subjects will appear.\n",
      "2. Storage phase: Storing the captured data for future use in verification or identification.\n",
      "3. Comparison phase: Comparing a given facial image to a known identity or database of known faces to verify or identify the person.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"what are the three steps of biometric facial recognition?\"})\n",
    "print(response[\"answer\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what are the three steps of biometric facial recognition?',\n",
       " 'context': [Document(id='966b640c-fc00-4723-9431-d311288cdfec', metadata={'page': 9.0, 'page_label': '10', 'source': 'data/eye_eurp.pdf'}, page_content='question \"who is this person?\". It can be applied as part of a system of monitoring or \\nstreamlining itineraries in the physical world (for example through customer tracking) or online. \\nFace detection, which recognizes the presence of a face in an image and can potentially \\nsegment or track it if the system input is a sequence of images (for example a video), is often \\nthe first step of a verification or identification system. Its purpose is  to align and standardize \\nthe faces contained in the images. \\n \\nThe three steps of biometric facial recognition \\n \\n1) Enrolment phase \\nThe first step is to capture data that is sufficiently representative of the diversity of the \\ncontexts in which the intended subjects will appear during the use of the technology. This \\ncorresponds to an image captured with little control, for example an image of moving people. \\nEngineers can also rely on public databases. Learning models are trained from these \\ndatabases. \\n2) Storage phase'),\n",
       "  Document(id='4d0792b7-b7ba-4f7f-9493-f2f3843c66d4', metadata={'page': 9.0, 'page_label': '10', 'source': 'data/eye_eurp.pdf'}, page_content='1.2. The field of facial recognition encompasses a diversity of uses \\nThe analysis of facial recognition must be considered in the plurality of its applications. Facial \\nrecognition tasks can be divided into two categories: verification (or authentication) and facial \\nidentification (or recognition).  \\n \\n1.2.1. Varied uses with different levels of risk \\nFace verification (or authentication) compares a given facial image to a known identity and \\nanswers the question \"does this person appear in the image?” Verification, as an unlocking \\nsystem (for example on smartphones), is a form of biometrics, similarly to fingerprint or iris \\nrecognition. Face identification  (or recognition) associates a given facial image with an \\nidentity (or group of people) from a database of known faces. Identification answers the \\nquestion \"who is this person?\". It can be applied as part of a system of monitoring or'),\n",
       "  Document(id='8ba3b6ad-01a4-46ef-acb9-3fce04a37183', metadata={'page': 12.0, 'page_label': '13', 'source': 'data/eye_eurp.pdf'}, page_content='three actions (smile, turn their head and blink, in random order). So-called \"static\" facial \\nrecognition is also carried out, using a photograph extracted from the video and \\ncompared with the photogra ph stored in the microchip. As with the two previous \\nexamples, this application raises questions in relation to the protection of personal \\ndata, to which the state is trying to respond. The Ministry of the Interior has made it \\nknown that users\\' personal da ta are only stored on their smartphones and are only \\nused by ALICEM during the registration of the device 35.  The Ministry also stipulates \\nthat this data will not be used for any purposes other than electronic authentication and \\naccess to online services by ALICEM and that it will not be shared with third parties36. \\nSeized for an opinion on the draft decree setting up the processing of biometric data \\nas part of the development of the application, the CNIL – by deliberation on October'),\n",
       "  Document(id='7e9d47ad-09b5-402d-a824-87d426423520', metadata={'page': 7.0, 'page_label': '8', 'source': 'data/eye_eurp.pdf'}, page_content='intelligence. Today, these techniques are studied through the approach of machine learning, \\na field at the intersection of artificial intelligence and data science. Most often, this learning is \\nsupervised: an algorithm trains a statistical model to recognize faces in images from a massive \\nvolume of annotated data (big data). \\nRecent research into facial recognition draws on relatively mature deep learning techniques \\n(see the box on \"Deep learning\").  Previously, pre -deep learning techniques had taken more \\nthan twenty years to increase accuracy from 60% to 90% on the benchmark Labeled faces in \\nthe wild (LFW)14, a reference tool used for conducting work on facial recognition. The \"deep\" \\nfacial recognition techniques now in use, which apply multiple cascading layers of image \\nprocessing in order to extract and transform physical features, have revolutionized the field \\nsince Facebook designed the Deepfa ce15 facial recognition system in 2014 . Deepface')],\n",
       " 'answer': 'The three steps of biometric facial recognition are:\\n\\n1. Enrollment phase: Capturing representative data of the diversity of contexts in which the intended subjects will appear.\\n2. Storage phase: Storing the captured data for future use in verification or identification.\\n3. Comparison phase: Comparing a given facial image to a known identity or database of known faces to verify or identify the person.'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The principle of proportionality is a mechanism used to arbitrate between competing legal principles that are simultaneously applicable but contradictory. It requires that any measure restricting rights and freedoms must be appropriate, necessary, and proportionate. This principle has been used by judges to weigh and balance legal principles, such as a power conferred on the State and the fundamental rights of individuals, or between several fundamental rights.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"what is the  principle of proportionality\"})\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'retrieve': {'context': [Document(id='73a63e1e-c2e4-42de-8e9d-60c1214cf8c7', metadata={'page': 27.0, 'page_label': '28', 'source': 'data/eye_eurp.pdf'}, page_content='mechanism between legal principles of equivalent rank, which are simultaneously \\napplicable but contradictory 75”. It is a question of weighing up and striking a balance \\nbetween each of the legal pri nciples in question - generally a power conferred on the State \\n(public order, law enforcement) and the fundamental rights of individuals - or between several \\nfundamental rights. Respect for the principle of proportionality requires that a measure \\nrestricting rights and freedoms must be: \\n● appropriate, in that it must enable the legitimate objective pursued to be attained;  \\n● necessary, in that it must not exceed what is required to achieve that objective;  \\n● and proportionate, in that it must not, by the burdens it creates, be disproportionate \\nto the result sought. \\nWhile the principle of proportionality was initially a mechanism used by judges to arbitrate \\nbetween competing legal principles, this “triple test” has acquired a general application'), Document(id='09f5a27c-0a92-435a-9a53-d7e236bd7d86', metadata={'page': 27.0, 'page_label': '28', 'source': 'data/eye_eurp.pdf'}, page_content='between competing legal principles, this “triple test” has acquired a general application \\nat the European lev el.  Article 5 of the Treaty on European Union states that “ Under the \\nprinciple of proportionality, the content and form of Union action shall not exceed what is \\nnecessary to achieve the objectives of the Treaties. The institutions of the Union shall apply  \\nthe principle of proportionality as laid down in the Protocol on the application of the principles \\nof subsidiarity and proportionality.” The principle of proportionality is intended to limit and to \\nframe the actions of the European Union, which must confi ne itself to what is necessary to \\nachieve the objectives of the Treaties. This implies in particular that the European legislator \\nmust resort to this principle when adopting a text.  \\nOriginating in Germany, this “triple test” has gradually spread throughou t Europe76, including'), Document(id='07557c8d-5f86-45e4-899c-8651fe3d645a', metadata={'page': 27.0, 'page_label': '28', 'source': 'data/eye_eurp.pdf'}, page_content='Originating in Germany, this “triple test” has gradually spread throughou t Europe76, including \\nthe United Kingdom. There is also emerging application of the “triple test” in the United \\nStates77. In France, the Constitutional Council uses the proportionality test when monitoring \\nlegislative provisions that restrict the exercise of a right or freedom in the name of safeguarding \\n \\n74 Council of Europe (1950), op. cit.  \\n75 G. Xynopoulos, “Proportionnalité”, in D. Alland and S. Rials (2003), Dictionnaire de la culture juridique , PUF, \\n2003, p. 1251. \\n76 CEDH (23 July 1968), aff. n° 1474/62, “Affai re relative à certains aspects du régime linguistique de \\nl’enseignement en Belgique c. Belgique\" pts. 5 et 10; CEDH (4 December 2008), aff. 30562/04 & 30566/04 S. and \\nMarper v. the United Kingdom, paras. 95 -104; CJCE (24 July 2003), aff. C -280/00 Altmark; CJUE (8 April 2014),'), Document(id='47da5396-8f2e-41f3-ade7-8b810510cc9b', metadata={'page': 26.0, 'page_label': '27', 'source': 'data/eye_eurp.pdf'}, page_content='the right concerned; \\n● respect the principle of proportionality; \\n● be necessary (principle of necessity). \\nIf the introduction of a facial recognition technology is likely to infringe a fundamental right and \\nfails to meet one of these conditions, its deployment may be considered contrary to the Charter \\nof Fundamental Rights of the European Union. It bears noting the CNIL\\'s opinion on the draft \\ndecree relating to the \"StopCovid\" application, in which it reiterates the importance of \\ncompliance with the above-mentioned conditions, in particular the motive of general interest \\nand the principle of proportionality73. \\n \\n72 Constitutional Council (27 July 1994), No. 94-343-344 DC; CJEU (14 October 2004), Omega, aff C-36/02. \\n73 Deliberation no. 2020-056 of 25 May 2020 giving its opinion on a draft decree relating to the mobile application \\nknown as \"StopCovid\", §5 : “La Commission rappelle néanmoins que les protections constitutionnelle et')]}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'generate': {'answer': 'The principle of proportionality is a mechanism initially used by judges to arbitrate between competing legal principles. It has three components: the measure must be appropriate, necessary, and proportionate. It has acquired general application at the European level, intended to limit and frame the actions of the European Union.'}}\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"what is the  principle of proportionality?\"}, stream_mode=\"updates\"\n",
    "):\n",
    "    print(f\"{step}\\n\\n----------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|The| principle| of| proportional|ity| is| a| mechanism| initially| used| by| judges| to| arbit|rate| between| competing| legal| principles|.| It| has| a| general| application| at| the| European| level| and| is| intended| to| limit| and| frame| the| actions| of| the| European| Union|,| which| must| conf|ine| itself| to| what| is| necessary| to| achieve| the| objectives| of| the| Treat|ies|.| The| principle| of| proportional|ity| requires| that| a| measure| restricting| rights| and| freedoms| must| be| appropriate|,| necessary|,| and| proportion|ate|.||"
     ]
    }
   ],
   "source": [
    "for message, metadata in graph.stream(\n",
    "    {\"question\": \"what is the  principle of proportionality?\"}, stream_mode=\"messages\"\n",
    "):\n",
    "    print(message.content, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
