{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97c79c38-38a3-40f3-ba2e-250649347d63",
   "metadata": {},
   "source": [
    "# Multimodal Parsing using GPT4o-mini\n",
    "\n",
    "\n",
    "This cookbook shows you how to use LlamaParse to parse any document with the multimodal capabilities of GPT4o-mini.\n",
    "\n",
    "LlamaParse allows you to plug in external, multimodal model vendors for parsing - we handle the error correction, validation, and scalability/reliability for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e60ecf-519c-41fc-911b-765adaf8bad4",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Download the data - the blog post from Meta on Llama3.1, in PDF form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91a9e532-1454-40e0-bbf0-fd442c350121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d9fb0aa-74cd-476f-8161-efd9e04248bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-06 18:49:23--  https://www.dropbox.com/scl/fi/8iu23epvv3473im5rq19g/llama3.1_blog.pdf?rlkey=5u417tbdox4aip33fdubvni56&st=dzozd11e&dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6031:18::a27d:5112\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc79992fcff9ede59fc27b1e6352.dl.dropboxusercontent.com/cd/0/inline/Cb-FUxDnwQHu5ZhOHQe0kushb4ZgLxftc-a7eKcyGRPuPqGimIDdgAY9m9CsFdU-9nrmgajYazl2UKPODsYEpIZS0fc26rfJvzaEXU83lfinGHk4wQj6QeHXKHGRrPPj3Ia2VlOpnLZYk1sJmxxjJCFG/file?dl=1# [following]\n",
      "--2024-10-06 18:49:24--  https://uc79992fcff9ede59fc27b1e6352.dl.dropboxusercontent.com/cd/0/inline/Cb-FUxDnwQHu5ZhOHQe0kushb4ZgLxftc-a7eKcyGRPuPqGimIDdgAY9m9CsFdU-9nrmgajYazl2UKPODsYEpIZS0fc26rfJvzaEXU83lfinGHk4wQj6QeHXKHGRrPPj3Ia2VlOpnLZYk1sJmxxjJCFG/file?dl=1\n",
      "Resolving uc79992fcff9ede59fc27b1e6352.dl.dropboxusercontent.com (uc79992fcff9ede59fc27b1e6352.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6031:15::a27d:510f\n",
      "Connecting to uc79992fcff9ede59fc27b1e6352.dl.dropboxusercontent.com (uc79992fcff9ede59fc27b1e6352.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cd/0/inline2/Cb9WOngSi1WZ2ogYFKxL-w6B8eLzV6MuPA9DHkXKsnQl8gbcwRnOuGDeG4tqe9KZENQvkx01JsWyTk-1QKiOAbAjkoPJHDwLm9hvcDnb22bz0XOHcBVgxWJ6pmC4q_-rj6OC8DGghAyC1BgCdMto0JPcA_9HNV-HK8YrqIlXHoiZtf2Bb2L4ngjrFczxs4bvmqVgW8JKI43BIvkNVs5EhFfRqgPrWfmvBFqPMRKrY9KXmDmNvCIZrZaZl-FREg5H6M0LrXBw0p5qm3GHNzNZ8cSNxSd2DkCjYIWYYlaOW-7DpbjEsTc3hfh7sgbzV01yh-MveqLS2dshyv-hSaoHUFFWgRgi1XTpe8xbWgtogA2YZYO-BGY09EaswCE_IRputv8/file?dl=1 [following]\n",
      "--2024-10-06 18:49:25--  https://uc79992fcff9ede59fc27b1e6352.dl.dropboxusercontent.com/cd/0/inline2/Cb9WOngSi1WZ2ogYFKxL-w6B8eLzV6MuPA9DHkXKsnQl8gbcwRnOuGDeG4tqe9KZENQvkx01JsWyTk-1QKiOAbAjkoPJHDwLm9hvcDnb22bz0XOHcBVgxWJ6pmC4q_-rj6OC8DGghAyC1BgCdMto0JPcA_9HNV-HK8YrqIlXHoiZtf2Bb2L4ngjrFczxs4bvmqVgW8JKI43BIvkNVs5EhFfRqgPrWfmvBFqPMRKrY9KXmDmNvCIZrZaZl-FREg5H6M0LrXBw0p5qm3GHNzNZ8cSNxSd2DkCjYIWYYlaOW-7DpbjEsTc3hfh7sgbzV01yh-MveqLS2dshyv-hSaoHUFFWgRgi1XTpe8xbWgtogA2YZYO-BGY09EaswCE_IRputv8/file?dl=1\n",
      "Reusing existing connection to uc79992fcff9ede59fc27b1e6352.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14191422 (14M) [application/binary]\n",
      "Saving to: ‘data/report.pdf’\n",
      "\n",
      "data/report.pdf     100%[===================>]  13.53M  5.96MB/s    in 2.3s    \n",
      "\n",
      "2024-10-06 18:49:28 (5.96 MB/s) - ‘data/report.pdf’ saved [14191422/14191422]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.dropbox.com/scl/fi/8iu23epvv3473im5rq19g/llama3.1_blog.pdf?rlkey=5u417tbdox4aip33fdubvni56&st=dzozd11e&dl=1\" -O \"data/report.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70d420d-1778-4b0d-81e2-db09276e90cf",
   "metadata": {},
   "source": [
    "![llama_blog_img](llama3.1-p5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e29a9d7-5bd9-4fb8-8ec1-4c128a748662",
   "metadata": {},
   "source": [
    "## Initialize LlamaParse\n",
    "\n",
    "Initialize LlamaParse in multimodal mode, and specify the vendor.\n",
    "\n",
    "**NOTE**: optionally you can specify the OpenAI API key. If you do so you will be charged our base LlamaParse price of 0.3c per page. If you don't then you will be charged 1.5c per page, as we will make the calls to gpt4o-mini for you and give you price predictability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc921729-3446-42ca-8e1b-a6fd26195ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "\n",
    "def get_text_nodes(json_list: List[dict]):\n",
    "    text_nodes = []\n",
    "    for idx, page in enumerate(json_list):\n",
    "        text_node = TextNode(text=page[\"md\"], metadata={\"page\": page[\"page\"]})\n",
    "        text_nodes.append(text_node)\n",
    "    return text_nodes\n",
    "\n",
    "\n",
    "def save_jsonl(data_list, filename):\n",
    "    \"\"\"Save a list of dictionaries as JSON Lines.\"\"\"\n",
    "    with open(filename, \"w\") as file:\n",
    "        for item in data_list:\n",
    "            json.dump(item, file)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "\n",
    "def load_jsonl(filename):\n",
    "    \"\"\"Load a list of dictionaries from JSON Lines.\"\"\"\n",
    "    data_list = []\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file:\n",
    "            data_list.append(json.loads(line))\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2e9d9cf-8189-4fcb-b34f-cde6cc0b59c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id bf28a200-24a8-4e73-b834-2dc60cc33ac4\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    \n",
    " \n",
    "    invalidate_cache=True,\n",
    ")\n",
    "json_objs = parser.get_json_result(\"./data/report.pdf\")\n",
    "json_list = json_objs[0][\"pages\"]\n",
    "docs = get_text_nodes(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96a81df0-1026-4e30-a930-f677dc31e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save\n",
    "save_jsonl([d.dict() for d in docs], \"docs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee2e6920-8893-4b39-ae12-94d13c651406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load\n",
    "from llama_index.core import Document\n",
    "\n",
    "docs_dicts = load_jsonl(\"docs.jsonl\")\n",
    "docs = [Document.parse_obj(d) for d in docs_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2dd56b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='daf829c8-2a1b-4abb-8686-f80a1fe9e8d6', embedding=None, metadata={'page': 1}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\n# Large Language Model\\n\\n# Introducing Llama 3.1: Our most capable models to date\\n\\nJuly 23, 2024 • 15 minute read\\n\\n405B\\nMeet Llama 3.1\\n70B\\n8 B\\n# Takeaways:\\n\\n- Meta is committed to openly accessible AI. Read Mark Zuckerberg’s letter detailing why open source is good for developers, good for Meta, and good for the world.\\n- Bringing open intelligence to all, our latest models expand context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first frontier-level open source AI model.\\n- Llama 3.1 405B is in a class of its own, with unmatched flexibility, control, and state-of-the-art capabilities that rival the best closed source models. Our new model will enable the community to unlock new workflows, such as synthetic data generation and model distillation.\\n\\nhttps://ai.meta.com/blog/meta-llama-3-1/', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c968a6ac-0d1e-4729-9b37-1deb7becd400', embedding=None, metadata={'page': 2}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\nWe’re continuing to build out Llama to be a system by providing more components that work with the model, including a reference system. We want to empower developers with the tools to create their own custom agents and new types of agentic behaviors. We’re bolstering this with new security and safety tools, including Llama Guard 3 and Prompt Guard, to help build responsibly. We’re also releasing a request for comment on the Llama Stack API, a standard interface we hope will make it easier for third-party projects to leverage Llama models.\\n\\nThe ecosystem is primed and ready to go with over 25 partners, including AWS, NVIDIA, Databricks, Groq, Dell, Azure, Google Cloud, and Snowflake offering services on day one.\\n\\nTry Llama 3.1 405B in the US on WhatsApp and at meta.ai by asking a challenging math or coding question.\\n\\nUntil today, open source large language models have mostly trailed behind their closed counterparts when it comes to capabilities and performance. Now, we’re ushering in a new era with open source leading the way. We’re publicly releasing Meta Llama 3.1 405B, which we believe is the world’s largest and most capable openly available foundation model. With more than 300 million total downloads of all Llama versions to date, we’re just getting started.\\n\\n# RECOMMENDED READS\\n\\n- Expanding the Llama ecosystem responsibly\\n- The Llama ecosystem: Past, present, and future\\n\\n# Introducing Llama 3.1\\n\\nLlama 3.1 405B is the first openly available model that rivals the top AI models when it comes to state-of-the-art capabilities in general knowledge, steerability, math, tool use, and multilingual translation. With the release of the 405B model, we’re poised to supercharge innovation—with unprecedented opportunities for growth and exploration. We believe the latest generation of Llama will ignite new applications and modeling paradigms, including synthetic data generation to enable the improvement and training of smaller models, as well as model distillation—a capability that has never been achieved at this scale in open source.\\n\\nhttps://ai.meta.com/blog/meta-llama-3-1/', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0d2a52d2-68b1-4c37-a5e5-2bb953dd3349', embedding=None, metadata={'page': 3}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\nAs part of this latest release, we’re introducing upgraded versions of the 8B and 70B models. These are multilingual and have a significantly longer context length of 128K, state-of-the-art tool use, and overall stronger reasoning capabilities. This enables our latest models to support advanced use cases, such as long-form text summarization, multilingual conversational agents, and coding assistants. We’ve also made changes to our license, allowing developers to use the outputs from Llama models—including the 405B—to improve other models. True to our commitment to open source, starting today, we’re making these models available to the community for download on llama.meta.com and Hugging Face and available for immediate development on our broad ecosystem of partner platforms.\\n\\n# Model evaluations\\n\\nFor this release, we evaluated performance on over 150 benchmark datasets that span a wide range of languages. In addition, we performed extensive human evaluations that compare Llama 3.1 with competing models in real-world scenarios. Our experimental evaluation suggests that our flagship model is competitive with leading foundation models across a range of tasks, including GPT-4, GPT-4o, and Claude 3.5 Sonnet. Additionally, our smaller models are competitive with closed and open models that have a similar number of parameters.\\n\\nhttps://ai.meta.com/blog/meta-llama-3-1/', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='28743110-c7bd-4636-b79e-d09167463cb7', embedding=None, metadata={'page': 4}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\n|Category|Llama 3.1|Nemotron 4|GPT-4|GPT-4|Claude 3.5|\\n|---|---|---|---|---|---|\\n|Benchmark|405B|340B Instruct|401251|Omni|Sonnet|\\n|Gcnam|78.7|78.7|78.7|78.7|78.7| | | | |\\n|MMLU (shot CoT)|88.6|(non-CoT)|85.4|88.7|88.3|\\n|MMLUPRO (S-shot; CoT)|73.3|62.7|64.8|74.0|77.0|\\n|IFEval|88.6|85.1|84.3|85.6|88.0|\\n|HumanEval (0-shot)|89.0|73.2|86.6|90.2|92.0|\\n|MBPP EvalPlus (basic) (0-shot)|88.6|72.8|83.6|87.8|90.5|\\n|Math|92.3| | | | | | | |\\n|GSMBK (A-shot, CoT)|96.8| |94.2|96.1| |\\n|MATH (0-shot, CoT)|73.8|41.1|64.5|76.6|71.1|\\n|ARC Challenge (0-shot)|96.9|94.6|96.4|96.7|96.7|\\n|GPQA (0-shot, CoT)|51.1| |41.4|53.6|59.4|\\n|BFCL|88.5|86.5|88.3|80.5|90.2|\\n|Nexus|58.7| |50.3|56.1|45.7|\\n|ZeroSCROLLSIQuALITY|95.2|95.2| |90.5|90.5|\\n|InfiniteBench/En MC|83.4| |72.1|82.5| |\\n|NIHIMulti-needle|98.1| |100.0|100.0|90.8|\\n|Multilingual|Multilingual MGSMShot|91.6|85.9|90.5|91.6|\\n\\nhttps://ai.meta.com/blog/meta-llama-3-1/', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a226bbea-5d5c-44a3-a328-380e4de3356c', embedding=None, metadata={'page': 5}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\n|Category|Llama 3.1|Gemma|Mistral|Llama 3.1|Mixtral|GPT 3.5| |\\n|---|---|---|---|---|---|---|---|\\n|Benchmark|8B|9B IT|7B Instruct|70B|8x22B Instruct|Turbo| |\\n|Gcnam|72.3|72.3|72.3|72.3|72.3|72.3| | | | | | |\\n|MMLU (shot CoT)|73.0|(5 shot non Cot)|60.5|86.0|79.9|69.8| |\\n|MMLUPRO (S-shot; CoT)|48.3| |36.9|66.4|56.3|49.2| |\\n|IFEval|80.4|73.6|57.6|87.5|72.7|59.9| |\\n|Code| | | | | | | |\\n|HumanEval (0-shot)|72.6|54.3|40.2|80.5|75.6|68.0| |\\n|MBPP EvalPlus|72.8|71.7|49.5|86.0|78.6|82.0| |\\n|(basc) (0-shot)|Math|76.7|53.2|95.1|88.2|81.6| |\\n|GSMBK (S-shot, CoT)|84.5| | | | | | |\\n|MATH (0-shot, CoT)|51.9|44.3|13.0|68.0|54.1|43.1| |\\n|Reasoning?|ARC Challenge (S-shot)|83.4|87.6|74.2|94.8|88.7|83.7|\\n|GPQA (S-shot CoT)|32.8| |28.8|46.7|33.3|30.8| |\\n|Tool uSC|85.9|85.9|85.9|85.9|85.9|85.9| | | | | | |\\n|BFCL|76.1| |60.4|84.8| | | |\\n|Nexus|38.5|30.0|24.7|56.7|48.5|37.2| |\\n|Long context|ZeroSCROLLSIQuALITY|81.0| | |90.5| | |\\n|InfiniteBench/En MC|65.1| | |78.2| | | |\\n|NIHIMulti-needle|98.8| | |97.5| | | |\\n|Multilingual|Multilingual MGSMShot|58.9|53.2|29.9|86.9|71.1|51.4|\\n\\n# Llama 3.1 405B Human Evaluation\\n\\n| |Win|Tie|Loss|\\n|---|---|---|---|\\n|Llama 3.1 405B|23.39|52.2%|24.5%|\\n|vs GPT-4-0125-Preview|19.1%|51.7%|29.2%|\\n|vs Claude 3.5 Sonnet|24.9%|50.8%|24.2%|\\n\\n# Win Rate\\n\\n0% 20% 40% 60% 80% 100% % win rate\\n\\nhttps://ai.meta.com/blog/meta-llama-3-1/', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='24f5ae78-207f-4429-9c99-084cebfcc098', embedding=None, metadata={'page': 6}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\n# Model Architecture\\n\\nAs our largest model yet, training Llama 3.1 405B on over 15 trillion tokens was a major challenge. To enable training runs at this scale and achieve the results we have in a reasonable amount of time, we significantly optimized our full training stack and pushed our model training to over 16 thousand H100 GPUs, making the 405B the first Llama model trained at this scale.\\n\\n|INPUT|Token|Self - Attention|Feedforward Network|Self - Attention|Feedforward Network|OUTPUT|\\n|---|---|---|---|---|---|---|\\n|Text tokens|embeddings| | | | |Text token|\\n\\n# AUTOREGRESSIVE DECODING\\n\\nTo address this, we made design choices that focus on keeping the model development process scalable and straightforward.\\n\\nWe opted for a standard decoder-only transformer model architecture with minor adaptations rather than a mixture-of-experts model to maximize training stability. We adopted an iterative post-training procedure, where each round uses supervised fine-tuning and direct preference optimization. This enabled us to create the highest quality synthetic data for each round and improve each capability’s performance.\\n\\nCompared to previous versions of Llama, we improved both the quantity and quality of the data we use for pre- and post-training. These improvements include the development of more careful pre-processing and curation pipelines for pre-training data, the development of more rigorous quality assurance, and filtering approaches for post-training data.\\n\\nAs expected per scaling laws for language models, our new flagship model outperforms smaller models trained using the same procedure. We also used the 405B parameter model to improve the post-training quality of our smaller models.\\n\\nTo support large-scale production inference for a model at the scale of the 405B, we quantized our models from 16-bit (BF16) to 8-bit (FP8) numerics, effectively lowering the compute requirements needed and allowing the model to run within a single server node.\\n\\n# Instruction and chat fine-tuning\\n\\nWith Llama 3.1 405B, we strove to improve the helpfulness, quality, and detailed instruction-following capability of the model in response to user instructions while\\n\\nhttps://ai.meta.com/blog/meta-llama-3-1/', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cd78f9fc-8aa5-4ed9-a191-f66653c4c19f', embedding=None, metadata={'page': 7}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\nensuring high levels of safety. Our biggest challenges were supporting more capabilities, the 128K context window, and increased model sizes.\\n\\nIn post-training, we produce final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involves Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO). We use synthetic data generation to produce the vast majority of our SFT examples, iterating multiple times to produce higher and higher quality synthetic data across all capabilities. Additionally, we invest in multiple data processing techniques to filter this synthetic data to the highest quality. This enables us to scale the amount of fine-tuning data across capabilities.\\n\\nWe carefully balance the data to produce a model with high quality across all capabilities. For example, we maintain the quality of our model on short-context benchmarks, even when extending to 128K context. Similarly, our model continues to provide maximally helpful answers, even as we add safety mitigations.\\n\\n# The Llama system\\n\\nLlama models were always intended to work as part of an overall system that can orchestrate several components, including calling external tools. Our vision is to go beyond the foundation models to give developers access to a broader system that gives them the flexibility to design and create custom offerings that align with their vision. This thinking started last year when we first introduced the incorporation of components outside of the core LLM.\\n\\nAs part of our ongoing efforts to develop AI responsibly beyond the model layer and helping others to do the same, we’re releasing a full reference system that includes several sample applications and includes new components such as Llama Guard 3, a multilingual safety model and Prompt Guard, a prompt injection filter. These sample applications are open source and can be built on by the community.\\n\\nThe implementation of components in this Llama System vision is still fragmented. That’s why we’ve started working with industry, startups, and the broader community to help better define the interfaces of these components. To support this, we’re releasing a request for comment on GitHub for what we’re calling “Llama Stack.” Llama Stack is a set of standardized and opinionated interfaces for how to build canonical toolchain components (fine-tuning, synthetic data generation) and agentic applications. Our hope is for these to become adopted across the ecosystem, which should help with easier interoperability.\\n\\nWe welcome feedback and ways to improve the proposal. We’re excited to grow the ecosystem around Llama and lower barriers for developers and platform providers.\\n\\nhttps://ai.meta.com/blog/meta-llama-3-1/', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='df0fda5f-a584-44b1-9117-ab5f8cc05ec6', embedding=None, metadata={'page': 8}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\n7/25/24, 1:44 PM\\n\\nBon dnsc MonclsIneuudingtha 8 bllcn; 70bulion, Jnd 405 minpont #antring rclatcd ai htlonn\\n\\nHoact 0:00 / 1:10\\n\\n# Openness drives innovation\\n\\nUnlike closed models, Llama model weights are available to download. Developers can fully customize the models for their needs and applications, train on new datasets, and conduct additional fine-tuning. This enables the broader developer community and the world to more fully realize the power of generative AI. Developers can fully customize for their applications and run in any environment, including on prem, in the cloud, or even locally on a laptop—all without sharing data with Meta.\\n\\nhttps://ai.meta.com/blog/meta-llama-3-1/', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='07e8f4b0-ae3d-4576-8b8f-72c4100c8d15', embedding=None, metadata={'page': 9}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\nWhile many may argue that closed models are more cost effective, Llama models offer some of the lowest cost per token in the industry, according to testing by Artificial Analysis. And as Mark Zuckerberg noted, open source will ensure that more people around the world have access to the benefits and opportunities of AI, that power isn’t concentrated in the hands of a small few, and that the technology can be deployed more evenly and safely across society. That’s why we continue to take steps on the path for open access AI to become the industry standard.\\n\\nWe’ve seen the community build amazing things with past Llama models including an AI study buddy built with Llama and deployed in WhatsApp and Messenger, an LLM tailored to the medical field designed to help guide clinical decision-making, and a healthcare non-profit startup in Brazil that makes it easier for the healthcare system to organize and communicate patients’ information about their hospitalization, all in a data secure way. We can’t wait to see what they build with our latest models thanks to the power of open source.\\n\\n# Building with Llama 3.1 405B\\n\\nFor the average developer, using a model at the scale of the 405B is challenging. While it’s an incredibly powerful model, we recognize that it requires significant compute resources and expertise to work with. We’ve spoken with the community, and we realize there’s so much more to generative AI development than just prompting models. We want to enable everyone to get the most out of the 405B, including:\\n\\n- Real-time and batch inference\\n- Supervised fine-tuning\\n- Evaluation of your model for your specific application\\n- Continual pre-training\\n- Retrieval-Augmented Generation (RAG)\\n- Function calling\\n- Synthetic data generation\\n\\nThis is where the Llama ecosystem can help. On day one, developers can take advantage of all the advanced capabilities of the 405B model and start building immediately. Developers can also explore advanced workflows like easy-to-use synthetic data generation, follow turnkey directions for model distillation, and enable seamless RAG with solutions from partners, including AWS, NVIDIA, and Databricks. Additionally, Groq has optimized low-latency inference for cloud deployments, with Dell achieving similar optimizations for on-prem systems.\\n\\nhttps://ai.meta.com/blog/meta-llama-3-1/', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9dd35578-0e13-45c6-9407-d1b6aa93380f', embedding=None, metadata={'page': 10}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\n|Feature|aws|databricksDeLL|9roq|IBM|Google Clud|Microsoft scale|snowflake|NVIDia|\\n|---|---|---|---|---|---|---|---|---|\\n|Real-time inference| | | | | | | | |\\n|Batch inference| | | | | | | | |\\n|Fine tuning| | | | | | | | |\\n|Model evaluation| | | | | | | | |\\n|Knowledge base| | | | | | | | |\\n|Continual pre-training| | | | | | | | |\\n|Safety guardrails| | | | | | | | |\\n|Synthetic data generation| | | | | | | | |\\n|Distillation recipe| | | | | | | | |\\n\\nWe’ve worked with key community projects like vLLM, TensorRT, and PyTorch to build in support from day one to ensure the community is ready for production deployment.\\n\\nWe hope that our release of the 405B will also spur innovation across the broader community to make inference and fine-tuning of models of this scale easier and enable the next wave of research in model distillation.\\n\\n# Try the Llama 3.1 collection of models today\\n\\nWe can’t wait to see what the community does with this work. There’s so much potential for building helpful new experiences using the multilinguality and increased context length. With the Llama Stack and new safety tools, we look forward to continuing to build together with the open source community responsibly. Before releasing a model, we work to identify, evaluate, and mitigate potential risks through several measures, including pre-deployment risk discovery exercises through red teaming, and safety fine-tuning. For example, we conduct extensive red teaming with both external and internal experts to stress test the models and find unexpected ways they may be used. (Read more about how we’re scaling our Llama 3.1 collection of models responsibly in this blog post.)\\n\\nWhile this is our biggest model yet, we believe there’s still plenty of new ground to explore in the future, including more device-friendly sizes, additional modalities, and more investment at the agent platform layer. As always, we look forward to seeing all the amazing products and experiences the community will build with these models.\\n\\nhttps://ai.meta.com/blog/meta-llama-3-1/', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='34cf8156-b50b-42b4-b115-0fdd2ab6010d', embedding=None, metadata={'page': 11}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\nThis work was supported by our partners across the AI community. We’d like to thank and acknowledge (in alphabetical order): Accenture, Amazon Web Services, AMD, Anyscale, CloudFlare, Databricks, Dell, Deloitte, Fireworks.ai, Google Cloud, Groq, Hugging Face, IBM WatsonX, Infosys, Intel, Kaggle, Microsoft Azure, NVIDIA, OctoAI, Oracle Cloud, PwC, Replicate, Sarvam AI, Scale.AI, SNCF, Snowflake, Together AI, and vLLM project developed in Sky Computing Lab at UC Berkeley.\\n\\n# Get started with Llama 3.1\\n\\n- Read the Llama 3.1 paper\\n- Visit the Llama GitHub repo\\n- Download Llama 3.1 on Hugging Face\\n\\n# Share:\\n\\nOur latest updates delivered to your inbox. Subscribe to our newsletter to keep up with Meta AI news, events, research breakthroughs, and more.\\n\\nJoin us in the pursuit of what’s possible with AI.\\n\\nhttps://ai.meta.com/blog/meta-llama-3-1/', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='211bcc3e-7ced-4ac8-8b26-89c3f5905fee', embedding=None, metadata={'page': 12}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\n7/25/24, 1:44 PM\\n\\n# See all open positions\\n\\n# Related Posts\\n\\n# OPEN SOURCE\\n\\n# Llama: A platform for responsible and open AI development\\n\\n# GOMeta\\n\\n# AI at Meta\\n\\n# Open Source\\n\\n# Expanding our open source large language models responsibly\\n\\nJuly 23, 2024\\n\\nRead post\\n\\n# Cost Int lahon AFFechng Slab SA Roo areragemining\\n\\nIncrease Costs CBMEau\\n\\n# SOURCE\\n\\n# Foonda Mate O0Gay WhatsAFF\\n\\n# Large Language Model\\n\\n# A social ‘study buddy’ gets a conversational lift from Meta Llama\\n\\nJune 6, 2024\\n\\nRead post\\n\\nhttps://ai.meta.com/blog/meta-llama-3-1/', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bb1a1523-4e9b-4b20-95b2-d14b7e2eb802', embedding=None, metadata={'page': 13}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\n# Code Llama\\n\\n# Built with Llama\\n\\n# Large Language Model\\n\\n# How SAIF CHECK is using Meta Llama 3 to validate and build trust in AI models\\n\\n# June 20, 2024\\n\\nRead post\\n\\n# Search AI content\\n\\n# Our approach\\n\\n# Research\\n\\n# Product experiences\\n\\n# Latest news\\n\\n# Foundational models', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c1792ee6-7da7-4f75-b7bb-80593655145b', embedding=None, metadata={'page': 14}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Introducing Llama 3.1: Our most capable models to date\\n\\nPrivacy Policy\\n\\nMeta © 2024\\n\\nTerms\\n\\nCookies\\n\\nhttps://ai.meta.com/blog/meta-llama-3-1/', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c51b0-7878-48d7-9bc3-02b516500128",
   "metadata": {},
   "source": [
    "### Setup GPT-4o-mini baseline\n",
    "\n",
    "For comparison, we will also parse the document using GPT-4o-mini (3c per page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fc3f258-50ae-4988-b904-c105463a498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 546bab8a-2806-4907-8e08-1cdb37091cdd\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser_gpt4o = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    use_vendor_multimodal_model=True,\n",
    "    vendor_multimodal_model=\"openai-gpt4o-mini\",\n",
    "    # invalidate_cache=True\n",
    ")\n",
    "json_objs_gpt4o = parser_gpt4o.get_json_result(\"./data/report.pdf\")\n",
    "\n",
    "json_list_gpt4o = json_objs_gpt4o[0][\"pages\"]\n",
    "docs_gpt4o = get_text_nodes(json_list_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a47f04e-12e1-4c80-a71d-ef7721f96401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save\n",
    "save_jsonl([d.dict() for d in docs_gpt4o], \"docs_gpt4o-mini.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c38b5ca3-fa87-434b-b477-bf6a4962eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load\n",
    "from llama_index.core import Document\n",
    "\n",
    "docs_gpt4o_mini_d = load_jsonl(\"docs_gpt4o-mini.jsonl\")\n",
    "docs_gpt4o_mini = [Document.parse_obj(d) for d in docs_gpt4o_mini_d]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c20f7a-2901-4dd0-b635-a4b33c5664c1",
   "metadata": {},
   "source": [
    "## View Results\n",
    "\n",
    "Let's visualize the results between GPT-4o-mini and GPT-4o along with the original document page.\n",
    "\n",
    "We see that \n",
    "\n",
    "**NOTE**: If you're using llama2-p33, just use `docs[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "778698aa-da7e-4081-b3b5-0372f228536f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page: 5\n",
      "\n",
      "# Introducing Llama 3.1: Our most capable models to date\n",
      "\n",
      "|Category|Llama 3.1|Gemma|Mistral|Llama 3.1|Mixtral|GPT 3.5| |\n",
      "|---|---|---|---|---|---|---|---|\n",
      "|Benchmark|8B|9B IT|7B Instruct|70B|8x22B Instruct|Turbo| |\n",
      "|Gcnam|72.3|72.3|72.3|72.3|72.3|72.3| | | | | | |\n",
      "|MMLU (shot CoT)|73.0|(5 shot non Cot)|60.5|86.0|79.9|69.8| |\n",
      "|MMLUPRO (S-shot; CoT)|48.3| |36.9|66.4|56.3|49.2| |\n",
      "|IFEval|80.4|73.6|57.6|87.5|72.7|59.9| |\n",
      "|Code| | | | | | | |\n",
      "|HumanEval (0-shot)|72.6|54.3|40.2|80.5|75.6|68.0| |\n",
      "|MBPP EvalPlus|72.8|71.7|49.5|86.0|78.6|82.0| |\n",
      "|(basc) (0-shot)|Math|76.7|53.2|95.1|88.2|81.6| |\n",
      "|GSMBK (S-shot, CoT)|84.5| | | | | | |\n",
      "|MATH (0-shot, CoT)|51.9|44.3|13.0|68.0|54.1|43.1| |\n",
      "|Reasoning?|ARC Challenge (S-shot)|83.4|87.6|74.2|94.8|88.7|83.7|\n",
      "|GPQA (S-shot CoT)|32.8| |28.8|46.7|33.3|30.8| |\n",
      "|Tool uSC|85.9|85.9|85.9|85.9|85.9|85.9| | | | | | |\n",
      "|BFCL|76.1| |60.4|84.8| | | |\n",
      "|Nexus|38.5|30.0|24.7|56.7|48.5|37.2| |\n",
      "|Long context|ZeroSCROLLSIQuALITY|81.0| | |90.5| | |\n",
      "|InfiniteBench/En MC|65.1| | |78.2| | | |\n",
      "|NIHIMulti-needle|98.8| | |97.5| | | |\n",
      "|Multilingual|Multilingual MGSMShot|58.9|53.2|29.9|86.9|71.1|51.4|\n",
      "\n",
      "# Llama 3.1 405B Human Evaluation\n",
      "\n",
      "| |Win|Tie|Loss|\n",
      "|---|---|---|---|\n",
      "|Llama 3.1 405B|23.39|52.2%|24.5%|\n",
      "|vs GPT-4-0125-Preview|19.1%|51.7%|29.2%|\n",
      "|vs Claude 3.5 Sonnet|24.9%|50.8%|24.2%|\n",
      "\n",
      "# Win Rate\n",
      "\n",
      "0% 20% 40% 60% 80% 100% % win rate\n",
      "\n",
      "https://ai.meta.com/blog/meta-llama-3-1/\n"
     ]
    }
   ],
   "source": [
    "# using GPT4o-mini\n",
    "print(docs[4].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511a30f-3efc-4142-9668-7dc056a24d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page: 5\n",
      "\n",
      "# Introducing Llama 3.1: Our most capable models to date\n",
      "\n",
      "## Meta\n",
      "\n",
      "| Category | Benchmark | Llama 3.1 8B | Gemma 2 9B IT | Mistral 7B Instruct | Llama 3.1 70B | Mixtral 8x22B Instruct | GPT 3.5 Turbo |\n",
      "|----------|-----------|--------------|---------------|---------------------|---------------|-----------------------|---------------|\n",
      "| General  | MMLU (0-shot, CoT) | 73.0 | 72.3 (0-shot, non-CoT) | 60.5 | 86.0 | 79.9 | 69.8 |\n",
      "|          | MMLU PRO (5-shot, CoT) | 48.3 | 71.7 | 36.9 | 66.4 | 56.3 | 49.2 |\n",
      "|          | ITEval | 80.4 | 73.6 | 57.6 | 87.5 | 72.7 | 69.9 |\n",
      "| Code     | HumanEval (0-shot) | 72.6 | 54.3 | 40.2 | 80.5 | 75.6 | 68.0 |\n",
      "|          | MBPP EvalPlus (5-shot) (0-shot) | 72.8 | 71.7 | 49.5 | 86.0 | 78.6 | 82.0 |\n",
      "| Math     | GSM8K | 84.5 | 76.7 | 53.2 | 95.1 | 88.2 | 81.6 |\n",
      "|          | MATH (0-shot, CoT) | 51.9 | 44.3 | 13.0 | 68.0 | 54.1 | 43.1 |\n",
      "| Reasoning | ARC Challenge (0-shot) | 83.4 | 87.6 | 74.2 | 94.8 | 88.7 | 83.7 |\n",
      "|          | GOPA (0-shot) | 32.8 | 40.8 | 28.0 | 46.7 | - | - |\n",
      "| Tool use | BFCL | 76.1 | 60.3 | 60.4 | 94.8 | - | 85.9 |\n",
      "|          | Noxus | 38.5 | 30.0 | 24.7 | 56.7 | 48.5 | 37.2 |\n",
      "| Long context | ZeroSCROLLS/QuaLITY | 81.0 | - | - | 90.5 | - | - |\n",
      "|          | InfiniteBench/En.MC | 65.1 | - | - | 78.2 | - | - |\n",
      "|          | NIH/Multi-needle | 98.8 | - | - | 97.5 | - | - |\n",
      "| Multilingual | Multilingual MGSM (0-shot) | 68.9 | 53.2 | 29.9 | 86.9 | 71.1 | 51.4 |\n",
      "\n",
      "## Llama 3.1 405B Human Evaluation\n",
      "\n",
      "| Model Comparison | Win | Tie | Loss |\n",
      "|------------------|-----|-----|------|\n",
      "| Llama 3.1 405B vs GPT-4-0125-Preview | 23.3% | 52.2% | 24.5% |\n",
      "| Llama 3.1 405B vs GPT-4o | 19.1% | 51.7% | 29.2% |\n",
      "| Llama 3.1 405B vs Claude 3.5 Sonnet | 24.9% | 50.8% | 24.2% |\n",
      "\n",
      "https://ai.meta.com/blog/meta-llama-3-1/\n"
     ]
    }
   ],
   "source": [
    "# using GPT-4o\n",
    "print(docs_gpt4o[4].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705f7729-fa0f-4ca0-8562-c42afeaa8532",
   "metadata": {},
   "source": [
    "## Setup RAG Pipeline\n",
    "\n",
    "Let's setup a RAG pipeline over this data.\n",
    "\n",
    "(we also use gpt4o-mini for the actual text synthesis step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a53ee5d-cc63-421b-8896-588c83edfcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\",api_base=\"https://models.inference.ai.azure.com\",api_key=\"\")\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\",api_base=\"https://models.inference.ai.azure.com\",api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60972d7a-7948-4ad7-89df-57004acee917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import SummaryIndex\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "index = VectorStoreIndex(docs)\n",
    "query_engine = index.as_query_engine(similarity_top_k=5)\n",
    "\n",
    "index_gpt4o = VectorStoreIndex(docs_gpt4o)\n",
    "query_engine_gpt4o = index_gpt4o.as_query_engine(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7df7bcb-1df4-4a01-88fc-2d596b1cc74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does Llama3.1 compare against gpt-4o and Claude 3.5 Sonnet in human evals?\"\n",
    "\n",
    "response = query_engine.query(query)\n",
    "response_gpt4o = query_engine_gpt4o.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7070a31-3bb8-4134-8338-20bc2fd6f3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In human evaluations, Llama 3.1 405B achieved a win rate of 23.39%, with 52.2% ties and 24.5% losses when compared to GPT-4-0125-Preview, which had a win rate of 19.1%, 51.7% ties, and 29.2% losses. Against Claude 3.5 Sonnet, Llama 3.1 had a win rate of 24.9%, with 50.8% ties and 24.2% losses. Overall, Llama 3.1 performed competitively against both models in these evaluations.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7bee8167-f021-4c87-8d28-9f40a4f7b69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Introducing Llama 3.1: Our most capable models to date\n",
      "\n",
      "|Category|Llama 3.1|Nemotron 4|GPT-4|GPT-4|Claude 3.5|\n",
      "|---|---|---|---|---|---|\n",
      "|Benchmark|405B|340B Instruct|401251|Omni|Sonnet|\n",
      "|Gcnam|78.7|78.7|78.7|78.7|78.7| | | | |\n",
      "|MMLU (shot CoT)|88.6|(non-CoT)|85.4|88.7|88.3|\n",
      "|MMLUPRO (S-shot; CoT)|73.3|62.7|64.8|74.0|77.0|\n",
      "|IFEval|88.6|85.1|84.3|85.6|88.0|\n",
      "|HumanEval (0-shot)|89.0|73.2|86.6|90.2|92.0|\n",
      "|MBPP EvalPlus (basic) (0-shot)|88.6|72.8|83.6|87.8|90.5|\n",
      "|Math|92.3| | | | | | | |\n",
      "|GSMBK (A-shot, CoT)|96.8| |94.2|96.1| |\n",
      "|MATH (0-shot, CoT)|73.8|41.1|64.5|76.6|71.1|\n",
      "|ARC Challenge (0-shot)|96.9|94.6|96.4|96.7|96.7|\n",
      "|GPQA (0-shot, CoT)|51.1| |41.4|53.6|59.4|\n",
      "|BFCL|88.5|86.5|88.3|80.5|90.2|\n",
      "|Nexus|58.7| |50.3|56.1|45.7|\n",
      "|ZeroSCROLLSIQuALITY|95.2|95.2| |90.5|90.5|\n",
      "|InfiniteBench/En MC|83.4| |72.1|82.5| |\n",
      "|NIHIMulti-needle|98.1| |100.0|100.0|90.8|\n",
      "|Multilingual|Multilingual MGSMShot|91.6|85.9|90.5|91.6|\n",
      "\n",
      "https://ai.meta.com/blog/meta-llama-3-1/\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[1].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f9fef7f-510b-46a5-8716-f5616f542035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In human evaluations, Llama 3.1 405B shows competitive performance against GPT-4o and Claude 3.5 Sonnet. Specifically, Llama 3.1 won 19.1% of the comparisons against GPT-4o, tied in 51.7%, and lost 29.2%. Against Claude 3.5 Sonnet, it won 24.9%, tied 50.8%, and lost 24.2%. This indicates that while Llama 3.1 performs well, it has a mix of wins, ties, and losses compared to these models.\n"
     ]
    }
   ],
   "source": [
    "print(response_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d40f9dd4-2dd4-4fa5-b636-1f901dc1601b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Meta\n",
      "\n",
      "## Category Benchmark\n",
      "\n",
      "| Category                  | Llama 3.1 8B | Gemma 2 9B IT | Mistral 7B Instruct | Llama 3.1 70B | Mixtral 8x22B Instruct | GPT 3.5 Turbo |\n",
      "|---------------------------|--------------|---------------|---------------------|---------------|------------------------|---------------|\n",
      "| **General**               |              |               |                     |               |                        |               |\n",
      "| MMLU (0-shot, CoT)        | 73.0         | 72.3          | 60.5                | 86.0          | 79.9                   | 69.8          |\n",
      "| MMLU PRO (5-shot, CoT)    | 48.3         | 53.6          | 37.6                | 66.4          | 56.3                   | 49.2          |\n",
      "| IIEval                    | 80.4         | 73.6          | 57.6                | 87.5          | 72.7                   | 69.9          |\n",
      "| **Code**                  |              |               |                     |               |                        |               |\n",
      "| HumanEval (0-shot)        | 72.6         | 54.3          | 40.2                | 80.5          | 75.6                   | 82.6          |\n",
      "| MBPP EvalPlus (5-shot)    | 72.8         | 71.7          | 49.7                | 81.7          | 80.0                   | 82.0          |\n",
      "| **Math**                  |              |               |                     |               |                        |               |\n",
      "| GSM8K (8-shot, CoT)       | 84.5         | 76.7          | 53.2                | 95.1          | 88.2                   | 81.6          |\n",
      "| MATH (4-shot, CoT)        | 51.9         | 44.3          | 13.0                | 68.0          | -                      | -             |\n",
      "| **Reasoning**             |              |               |                     |               |                        |               |\n",
      "| ARC Challenge (0-shot)    | 83.4         | 87.6          | 74.2                | 94.8          | -                      | -             |\n",
      "| GOP (0-shot)              | 32.8         | 39.4          | 28.8                | 46.7          | 33.3                   | 35.9          |\n",
      "| **Tool use**              |              |               |                     |               |                        |               |\n",
      "| BFLC                      | 76.1         | 80.3          | -                   | 88.5          | -                      | 85.9          |\n",
      "| Nexus                     | 38.5         | 30.0          | 24.7                | 56.7          | 48.5                   | 37.2          |\n",
      "| **Long context**          |              |               |                     |               |                        |               |\n",
      "| ZeroSCROLLS/QuALITY       | 81.0         | -             | -                   | 90.5          | -                      | -             |\n",
      "| InfiniteBench/En.MC       | 65.1         | -             | -                   | 78.2          | -                      | -             |\n",
      "| NIH/Multi-needle          | 98.8         | -             | -                   | 97.5          | -                      | -             |\n",
      "| **Multilingual**          |              |               |                     |               |                        |               |\n",
      "| Multilingual MGSM (8-shot)| 68.9         | 53.2          | 29.9                | 86.9          | 71.1                   | 51.4          |\n",
      "\n",
      "## Llama 3.1 405B Human Evaluation\n",
      "\n",
      "| Comparison                        | Win  | Tie  | Loss |\n",
      "|-----------------------------------|------|------|------|\n",
      "| Llama 3.1 405B vs GPT-4-0125-Preview | 23.3% | 52.2% | 24.5% |\n",
      "| Llama 3.1 405B vs GPT-4o          | 19.1% | 51.7% | 29.2% |\n",
      "| Llama 3.1 405B vs Claude 3.5 Sonnet | 24.9% | 50.8% | 24.2% |\n",
      "\n",
      "[Source](https://ai.meta.com/blog/meta-llama-3-1/)\n"
     ]
    }
   ],
   "source": [
    "print(response_gpt4o.source_nodes[1].get_content())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
