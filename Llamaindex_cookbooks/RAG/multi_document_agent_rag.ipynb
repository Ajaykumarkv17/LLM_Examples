{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-core\n",
    "%pip install llama-index-agent-openai\n",
    "%pip install llama-index-readers-file\n",
    "%pip install llama-index-postprocessor-cohere-rerank\n",
    "%pip install llama-index-llms-openai\n",
    "%pip install llama-index-embeddings-openai\n",
    "%pip install unstructured[html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import UnstructuredReader\n",
    "\n",
    "reader = UnstructuredReader(api_key=\"\",url=\"https://api.unstructured.io/general/v0/general\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "all_files_gen = Path(\"./docs.llamaindex.ai/\").rglob(\"*\")\n",
    "all_files = [f.resolve() for f in all_files_gen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/AjaykumarKV/Documents/LLM_Cookbooks/LLM_Examples/Llamaindex_cookbooks/RAG/docs.llamaindex.ai/AdvancedRetrievalStrategiesLlamaIndex.html'),\n",
       " WindowsPath('C:/Users/AjaykumarKV/Documents/LLM_Cookbooks/LLM_Examples/Llamaindex_cookbooks/RAG/docs.llamaindex.ai/BuildingPerformantRAGApplicationsforProductionLlamaIndex.html'),\n",
       " WindowsPath('C:/Users/AjaykumarKV/Documents/LLM_Cookbooks/LLM_Examples/Llamaindex_cookbooks/RAG/docs.llamaindex.ai/index.html'),\n",
       " WindowsPath('C:/Users/AjaykumarKV/Documents/LLM_Cookbooks/LLM_Examples/Llamaindex_cookbooks/RAG/docs.llamaindex.ai/NodeParserUsagePatternLlamaIndex.html'),\n",
       " WindowsPath('C:/Users/AjaykumarKV/Documents/LLM_Cookbooks/LLM_Examples/Llamaindex_cookbooks/RAG/docs.llamaindex.ai/QueryTransformationsLlamaIndex.html'),\n",
       " WindowsPath('C:/Users/AjaykumarKV/Documents/LLM_Cookbooks/LLM_Examples/Llamaindex_cookbooks/RAG/docs.llamaindex.ai/UsagepatternLlamaIndex.html')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_html_files = [f for f in all_files if f.suffix.lower() == \".html\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_html_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx 0/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.unstructured.io/general/v0/general \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AjaykumarKV\\Documents\\LLM_Cookbooks\\LLM_Examples\\Llamaindex_cookbooks\\RAG\\docs.llamaindex.ai\\AdvancedRetrievalStrategiesLlamaIndex.html\n",
      "Idx 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.unstructured.io/general/v0/general \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AjaykumarKV\\Documents\\LLM_Cookbooks\\LLM_Examples\\Llamaindex_cookbooks\\RAG\\docs.llamaindex.ai\\BuildingPerformantRAGApplicationsforProductionLlamaIndex.html\n",
      "Idx 2/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.unstructured.io/general/v0/general \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AjaykumarKV\\Documents\\LLM_Cookbooks\\LLM_Examples\\Llamaindex_cookbooks\\RAG\\docs.llamaindex.ai\\index.html\n",
      "Idx 3/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.unstructured.io/general/v0/general \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AjaykumarKV\\Documents\\LLM_Cookbooks\\LLM_Examples\\Llamaindex_cookbooks\\RAG\\docs.llamaindex.ai\\NodeParserUsagePatternLlamaIndex.html\n",
      "Idx 4/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.unstructured.io/general/v0/general \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AjaykumarKV\\Documents\\LLM_Cookbooks\\LLM_Examples\\Llamaindex_cookbooks\\RAG\\docs.llamaindex.ai\\QueryTransformationsLlamaIndex.html\n",
      "Idx 5/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.unstructured.io/general/v0/general \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AjaykumarKV\\Documents\\LLM_Cookbooks\\LLM_Examples\\Llamaindex_cookbooks\\RAG\\docs.llamaindex.ai\\UsagepatternLlamaIndex.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "\n",
    "doc_limit = 100\n",
    "\n",
    "docs = []\n",
    "for idx, f in enumerate(all_html_files):\n",
    "    if idx > doc_limit:\n",
    "        break\n",
    "    print(f\"Idx {idx}/{len(all_html_files)}\")\n",
    "    loaded_docs = reader.load_data(file=f, split_documents=True)\n",
    "    \n",
    "    \n",
    "    loaded_doc = Document(\n",
    "        text=\"\\n\\n\".join([d.get_content() for d in loaded_docs[::]]),\n",
    "        metadata={\"path\": str(f)},\n",
    "    )\n",
    "    print(loaded_doc.metadata[\"path\"])\n",
    "    docs.append(loaded_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mloaded_docs\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loaded_docs' is not defined"
     ]
    }
   ],
   "source": [
    "loaded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='868cd63f-7c68-46ec-8105-c4bacbaa562a', embedding=None, metadata={'path': 'C:\\\\Users\\\\AjaykumarKV\\\\Documents\\\\LLM_Cookbooks\\\\LLM_Examples\\\\Llamaindex_cookbooks\\\\RAG\\\\docs.llamaindex.ai\\\\AdvancedRetrievalStrategiesLlamaIndex.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Advanced Retrieval Strategies#\\n\\nMain Advanced Retrieval Strategies#\\n\\nThere are a variety of more advanced retrieval strategies you may wish to try, each with different benefits:\\n\\nReranking\\n\\nRecursive retrieval\\n\\nEmbedded tables\\n\\nSmall-to-big retrieval\\n\\nSee our full retrievers module guide for a comprehensive list of all retrieval strategies, broken down into different categories.\\n\\nBasic retrieval from each index\\n\\nAdvanced retrieval and search\\n\\nAuto-Retrieval\\n\\nKnowledge Graph Retrievers\\n\\nComposed/Hierarchical Retrievers\\n\\nand more!\\n\\nMore resources are below.\\n\\nQuery Transformations#\\n\\nA user query can be transformed before it enters a flow (query engine, agent, and more). See resources below on query transformations:\\n\\nQuery Transform Cookbook\\n\\nQuery Transformations Docs\\n\\nComposable Retrievers#\\n\\nEvery retriever is capable of retrieving and running other objects, including\\n\\nother retrievers\\n\\nquery engines\\n\\nquery pipelines\\n\\nother nodes\\n\\nFor more details, check out the guide below.\\n\\nComposable Retrievers\\n\\nThird-Party Resources#\\n\\nHere are some third-party resources on advanced retrieval strategies.\\n\\nDeepMemory (Activeloop)\\n\\nWeaviate Hybrid Search\\n\\nPinecone Hybrid Search\\n\\nMilvus Hybrid Search', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bf5d083a-d5c0-47db-ac38-a2249ccb9b20', embedding=None, metadata={'path': 'C:\\\\Users\\\\AjaykumarKV\\\\Documents\\\\LLM_Cookbooks\\\\LLM_Examples\\\\Llamaindex_cookbooks\\\\RAG\\\\docs.llamaindex.ai\\\\BuildingPerformantRAGApplicationsforProductionLlamaIndex.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Building Performant RAG Applications for Production#\\n\\nPrototyping a RAG application is easy, but making it performant, robust, and scalable to a large knowledge corpus is hard.\\n\\nThis guide contains a variety of tips and tricks to improve the performance of your RAG workflow. We first outline some general techniques - they are loosely ordered in terms of most straightforward to most challenging. We then dive a bit more deeply into each technique, the use cases that it solves, and how to implement it with LlamaIndex!\\n\\nThe end goal is to optimize your retrieval and generation performance to answer more queries over more complex datasets accurately and without hallucinations.\\n\\nGeneral Techniques for Building Production-Grade RAG#\\n\\nHere are some top Considerations for Building Production-Grade RAG\\n\\nDecoupling chunks used for retrieval vs. chunks used for synthesis\\n\\nStructured Retrieval for Larger Document Sets\\n\\nDynamically Retrieve Chunks Depending on your Task\\n\\nOptimize context embeddings\\n\\nWe discussed this and more during our Production RAG Webinar. Check out this Tweet thread for more synthesized details.\\n\\nDecoupling Chunks Used for Retrieval vs. Chunks Used for Synthesis#\\n\\nA key technique for better retrieval is to decouple chunks used for retrieval with those that are used for synthesis.\\n\\nMotivation#\\n\\nThe optimal chunk representation for retrieval might be different than the optimal consideration used for synthesis. For instance, a raw text chunk may contain needed details for the LLM to synthesize a more detailed answer given a query. However, it may contain filler words/info that may bias the embedding representation, or it may lack global context and not be retrieved at all when a relevant query comes in.\\n\\nKey Techniques#\\n\\nThere’s two main ways to take advantage of this idea:\\n\\n1. Embed a document summary, which links to chunks associated with the document.\\n\\nThis can help retrieve relevant documents at a high-level before retrieving chunks vs. retrieving chunks directly (that might be in irrelevant documents).\\n\\nResources:\\n\\nTable Recursive Retrieval\\n\\nDocument Summary Index\\n\\n2. Embed a sentence, which then links to a window around the sentence.\\n\\nThis allows for finer-grained retrieval of relevant context (embedding giant chunks leads to “lost in the middle” problems), but also ensures enough context for LLM synthesis.\\n\\nResources:\\n\\nMetadata Replacement Postprocessor\\n\\nStructured Retrieval for Larger Document Sets#\\n\\nMotivation#\\n\\nA big issue with the standard RAG stack (top-k retrieval + basic text splitting) is that it doesn’t do well as the number of documents scales up - e.g. if you have 100 different PDFs. In this setting, given a query you may want to use structured information to help with more precise retrieval; for instance, if you ask a question that\\'s only relevant to two PDFs, using structured information to ensure those two PDFs get returned beyond raw embedding similarity with chunks.\\n\\nKey Techniques#\\n\\nThere’s a few ways of performing more structured tagging/retrieval for production-quality RAG systems, each with their own pros/cons.\\n\\n1. Metadata Filters + Auto Retrieval Tag each document with metadata and then store in a vector database. During inference time, use the LLM to infer the right metadata filters to query the vector db in addition to the semantic query string.\\n\\nPros ✅: Supported in major vector dbs. Can filter document via multiple dimensions.\\n\\nCons 🚫: Can be hard to define the right tags. Tags may not contain enough relevant information for more precise retrieval. Also tags represent keyword search at the document-level, doesn’t allow for semantic lookups.\\n\\nResources: 2. Store Document Hierarchies (summaries -> raw chunks) + Recursive Retrieval Embed document summaries and map to chunks per document. Fetch at the document-level first before chunk level.\\n\\nPros ✅: Allows for semantic lookups at the document level.\\n\\nCons 🚫: Doesn’t allow for keyword lookups by structured tags (can be more precise than semantic search). Also autogenerating summaries can be expensive.\\n\\nResources\\n\\nChroma Auto-Retrieval\\n\\nDocument Summary Index\\n\\nRecursive Retriever\\n\\nAuto-Retriever vs. Recursive Retriever\\n\\nDynamically Retrieve Chunks Depending on your Task#\\n\\nMotivation#\\n\\nRAG isn\\'t just about question-answering about specific facts, which top-k similarity is optimized for. There can be a broad range of queries that a user might ask. Queries that are handled by naive RAG stacks include ones that ask about specific facts e.g. \"Tell me about the D&I initiatives for this company in 2023\" or \"What did the narrator do during his time at Google\". But queries can also include summarization e.g. \"Can you give me a high-level overview of this document\", or comparisons \"Can you compare/contrast X and Y\". All of these use cases may require different retrieval techniques.\\n\\nKey Techniques#\\n\\nLlamaIndex provides some core abstractions to help you do task-specific retrieval. This includes our router module as well as our data agent module. This also includes some advanced query engine modules. This also include other modules that join structured and unstructured data.\\n\\nYou can use these modules to do joint question-answering and summarization, or even combine structured queries with unstructured queries.\\n\\nCore Module Resources\\n\\nQuery engine\\n\\nAgents\\n\\nRouter\\n\\nDetailed Guide Resources\\n\\nSub-Question Query Engine\\n\\nJoint QA-Summary\\n\\nRecursive Retriever Agents\\n\\nRouter Query Engine\\n\\nOpenAI Agent Cookbook\\n\\nOpenAIAgent Query Planning\\n\\nOptimize Context Embeddings#\\n\\nMotivation#\\n\\nThis is related to the motivation described above in \"decoupling chunks used for retrieval vs. synthesis\". We want to make sure that the embeddings are optimized for better retrieval over your specific data corpus. Pre-trained models may not capture the salient properties of the data relevant to your use case.\\n\\nKey Techniques#\\n\\nBeyond some of the techniques listed above, we can also try finetuning the embedding model. We can actually do this over an unstructured text corpus, in a label-free way.\\n\\nCheck out our guides here:\\n\\nEmbedding Fine-tuning Guide', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b6c0f914-e436-4754-938b-d22c3510b8b0', embedding=None, metadata={'path': 'C:\\\\Users\\\\AjaykumarKV\\\\Documents\\\\LLM_Cookbooks\\\\LLM_Examples\\\\Llamaindex_cookbooks\\\\RAG\\\\docs.llamaindex.ai\\\\index.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Welcome to LlamaIndex 🦙 !#\\n\\nLlamaIndex is the leading framework for building LLM-powered agents over your data with LLMs and workflows.\\n\\nIntroduction\\n\\nWhat is context augmentation? What are agents and workflows? How does LlamaIndex help build them?\\n\\nUse cases\\n\\nWhat kind of apps can you build with LlamaIndex? Who should use it?\\n\\nGetting started\\n\\nGet started in Python or TypeScript in just 5 lines of code!\\n\\nLlamaCloud\\n\\nManaged services for LlamaIndex including LlamaParse, the world\\'s best document parser.\\n\\nCommunity\\n\\nGet help and meet collaborators on Discord, Twitter, LinkedIn, and learn how to contribute to the project.\\n\\nRelated projects\\n\\nCheck out our library of connectors, readers, and other integrations at LlamaHub as well as demos and starter apps like create-llama.\\n\\nIntroduction#\\n\\nWhat are agents?#\\n\\nAgents are LLM-powered knowledge assistants that use tools to perform tasks like research, data extraction, and more. Agents range from simple question-answering to being able to sense, decide and take actions in order to complete tasks.\\n\\nLlamaIndex provides a framework for building agents including the ability to use RAG pipelines as one of many tools to complete a task.\\n\\nWhat are workflows?#\\n\\nWorkflows are multi-step processes that combine one or more agents, data connectors, and other tools to complete a task. They are event-driven software that allows you to combine RAG data sources and multiple agents to create a complex application that can perform a wide variety of tasks with reflection, error-correction, and other hallmarks of advanced LLM applications. You can then deploy these agentic workflows as production microservices.\\n\\nWhat is context augmentation?#\\n\\nLLMs offer a natural language interface between humans and data. LLMs come pre-trained on huge amounts of publicly available data, but they are not trained on your data. Your data may be private or specific to the problem you\\'re trying to solve. It\\'s behind APIs, in SQL databases, or trapped in PDFs and slide decks.\\n\\nContext augmentation makes your data available to the LLM to solve the problem at hand. LlamaIndex provides the tools to build any of context-augmentation use case, from prototype to production. Our tools allow you to ingest, parse, index and process your data and quickly implement complex query workflows combining data access with LLM prompting.\\n\\nThe most popular example of context-augmentation is Retrieval-Augmented Generation or RAG, which combines context with LLMs at inference time.\\n\\nLlamaIndex is the framework for Context-Augmented LLM Applications#\\n\\nLlamaIndex imposes no restriction on how you use LLMs. You can use LLMs as auto-complete, chatbots, agents, and more. It just makes using them easier. We provide tools like:\\n\\nData connectors ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more.\\n\\nData indexes structure your data in intermediate representations that are easy and performant for LLMs to consume.\\n\\nEngines provide natural language access to your data. For example:\\n\\nQuery engines are powerful interfaces for question-answering (e.g. a RAG flow).\\n\\nChat engines are conversational interfaces for multi-message, \"back and forth\" interactions with your data.\\n\\nAgents are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more.\\n\\nObservability/Evaluation integrations that enable you to rigorously experiment, evaluate, and monitor your app in a virtuous cycle.\\n\\nWorkflows allow you to combine all of the above into an event-driven system far more flexible than other, graph-based approaches.\\n\\nUse cases#\\n\\nSome popular use cases for LlamaIndex and context augmentation in general include:\\n\\nQuestion-Answering (Retrieval-Augmented Generation aka RAG)\\n\\nChatbots\\n\\nDocument Understanding and Data Extraction\\n\\nAutonomous Agents that can perform research and take actions\\n\\nMulti-modal applications that combine text, images, and other data types\\n\\nFine-tuning models on data to improve performance\\n\\nCheck out our use cases documentation for more examples and links to tutorials.\\n\\n👨\\u200d👩\\u200d👧\\u200d👦 Who is LlamaIndex for?#\\n\\nLlamaIndex provides tools for beginners, advanced users, and everyone in between.\\n\\nOur high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code.\\n\\nFor more complex applications, our lower-level APIs allow advanced users to customize and extend any module -- data connectors, indices, retrievers, query engines, and reranking modules -- to fit their needs.\\n\\nGetting Started#\\n\\nLlamaIndex is available in Python (these docs) and Typescript. If you\\'re not sure where to start, we recommend reading how to read these docs which will point you to the right place based on your experience level.\\n\\n30 second quickstart#\\n\\nSet an environment variable called OPENAI_API_KEY with an OpenAI API key. Install the Python library:\\n\\npip install llama-index\\r\\n\\nPut some documents in a folder called data, then ask questions about them with our famous 5-line starter:\\n\\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\\r\\n\\r\\ndocuments = SimpleDirectoryReader(\"data\").load_data()\\r\\nindex = VectorStoreIndex.from_documents(documents)\\r\\nquery_engine = index.as_query_engine()\\r\\nresponse = query_engine.query(\"Some question about the data should go here\")\\r\\nprint(response)\\r\\n\\nIf any part of this trips you up, don\\'t worry! Check out our more comprehensive starter tutorials using remote APIs like OpenAI or any model that runs on your laptop.\\n\\nLlamaCloud#\\n\\nIf you\\'re an enterprise developer, check out LlamaCloud. It is an end-to-end managed service for data parsing, ingestion, indexing, and retrieval, allowing you to get production-quality data for your production LLM application. It\\'s available both hosted on our servers or as a self-hosted solution.\\n\\nLlamaParse#\\n\\nLlamaParse is our state-of-the-art document parsing solution. It\\'s available as part of LlamaCloud and also available as a self-serve API. You can sign up and parse up to 1000 pages/day for free, or enter a credit card for unlimited parsing. Learn more.\\n\\nCommunity#\\n\\nNeed help? Have a feature suggestion? Join the LlamaIndex community:\\n\\nTwitter\\n\\nDiscord\\n\\nLinkedIn\\n\\nGetting the library#\\n\\nLlamaIndex Python\\n\\nLlamaIndex Python Github\\n\\nPython Docs (what you\\'re reading now)\\n\\nLlamaIndex on PyPi\\n\\nLlamaIndex.TS (Typescript/Javascript package):\\n\\nLlamaIndex.TS Github\\n\\nTypeScript Docs\\n\\nLlamaIndex.TS on npm\\n\\nContributing#\\n\\nWe are open-source and always welcome contributions to the project! Check out our contributing guide for full details on how to extend the core library or add an integration to a third party like an LLM, a vector store, an agent tool and more.\\n\\nLlamaIndex Ecosystem#\\n\\nThere\\'s more to the LlamaIndex universe! Check out some of our other projects:\\n\\nllama_deploy | Deploy your agentic workflows as production microservices\\n\\nLlamaHub | A large (and growing!) collection of custom data connectors\\n\\nSEC Insights | A LlamaIndex-powered application for financial research\\n\\ncreate-llama | A CLI tool to quickly scaffold LlamaIndex projects', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='865e8838-2755-4f24-a72a-796b5c7378ef', embedding=None, metadata={'path': 'C:\\\\Users\\\\AjaykumarKV\\\\Documents\\\\LLM_Cookbooks\\\\LLM_Examples\\\\Llamaindex_cookbooks\\\\RAG\\\\docs.llamaindex.ai\\\\NodeParserUsagePatternLlamaIndex.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Node Parser Usage Pattern#\\n\\nNode parsers are a simple abstraction that take a list of documents, and chunk them into Node objects, such that each node is a specific chunk of the parent document. When a document is broken into nodes, all of it\\'s attributes are inherited to the children nodes (i.e. metadata, text and metadata templates, etc.). You can read more about Node and Document properties here.\\n\\nGetting Started#\\n\\nStandalone Usage#\\n\\nNode parsers can be used on their own:\\n\\nfrom llama_index.core import Document\\nfrom llama_index.core.node_parser import SentenceSplitter\\n\\nnode_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\\n\\nnodes = node_parser.get_nodes_from_documents(\\n    [Document(text=\"long text\")], show_progress=False\\n)\\n\\nTransformation Usage#\\n\\nNode parsers can be included in any set of transformations with an ingestion pipeline.\\n\\nfrom llama_index.core import SimpleDirectoryReader\\nfrom llama_index.core.ingestion import IngestionPipeline\\nfrom llama_index.core.node_parser import TokenTextSplitter\\n\\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\\n\\npipeline = IngestionPipeline(transformations=[TokenTextSplitter(), ...])\\n\\nnodes = pipeline.run(documents=documents)\\n\\nIndex Usage#\\n\\nOr set inside a transformations or global settings to be used automatically when an index is constructed using .from_documents():\\n\\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\\nfrom llama_index.core.node_parser import SentenceSplitter\\n\\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\\n\\n# global\\nfrom llama_index.core import Settings\\n\\nSettings.text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\\n\\n# per-index\\nindex = VectorStoreIndex.from_documents(\\n    documents,\\n    transformations=[SentenceSplitter(chunk_size=1024, chunk_overlap=20)],\\n)\\n\\nModules#\\n\\nSee the full modules guide.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aaeb1b69-2c55-44f1-8fbf-7d00e7d88c7c', embedding=None, metadata={'path': 'C:\\\\Users\\\\AjaykumarKV\\\\Documents\\\\LLM_Cookbooks\\\\LLM_Examples\\\\Llamaindex_cookbooks\\\\RAG\\\\docs.llamaindex.ai\\\\QueryTransformationsLlamaIndex.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Query Transformations#\\n\\nLlamaIndex allows you to perform query transformations over your index structures. Query transformations are modules that will convert a query into another query. They can be single-step, as in the transformation is run once before the query is executed against an index.\\n\\nThey can also be multi-step, as in:\\n\\nThe query is transformed, executed against an index,\\n\\nThe response is retrieved.\\n\\nSubsequent queries are transformed/executed in a sequential fashion.\\n\\nWe list some of our query transformations in more detail below.\\n\\nUse Cases#\\n\\nQuery transformations have multiple use cases:\\n\\nTransforming an initial query into a form that can be more easily embedded (e.g. HyDE)\\n\\nTransforming an initial query into a subquestion that can be more easily answered from the data (single-step query decomposition)\\n\\nBreaking an initial query into multiple subquestions that can be more easily answered on their own. (multi-step query decomposition)\\n\\nHyDE (Hypothetical Document Embeddings)#\\n\\nHyDE is a technique where given a natural language query, a hypothetical document/answer is generated first. This hypothetical document is then used for embedding lookup rather than the raw query.\\n\\nTo use HyDE, an example code snippet is shown below.\\n\\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\\nfrom llama_index.core.indices.query.query_transform.base import (\\n    HyDEQueryTransform,\\n)\\nfrom llama_index.core.query_engine import TransformQueryEngine\\n\\n# load documents, build index\\ndocuments = SimpleDirectoryReader(\"../paul_graham_essay/data\").load_data()\\nindex = VectorStoreIndex(documents)\\n\\n# run query with HyDE query transform\\nquery_str = \"what did paul graham do after going to RISD\"\\nhyde = HyDEQueryTransform(include_original=True)\\nquery_engine = index.as_query_engine()\\nquery_engine = TransformQueryEngine(query_engine, query_transform=hyde)\\nresponse = query_engine.query(query_str)\\nprint(response)\\n\\nCheck out our example notebook for a full walkthrough.\\n\\nMulti-Step Query Transformations#\\n\\nMulti-step query transformations are a generalization on top of existing single-step query transformation approaches.\\n\\nGiven an initial, complex query, the query is transformed and executed against an index. The response is retrieved from the query. Given the response (along with prior responses) and the query, follow-up questions may be asked against the index as well. This technique allows a query to be run against a single knowledge source until that query has satisfied all questions.\\n\\nAn example image is shown below.\\n\\nHere\\'s a corresponding example code snippet.\\n\\nfrom llama_index.core.indices.query.query_transform.base import (\\n    StepDecomposeQueryTransform,\\n)\\n\\n# gpt-4\\nstep_decompose_transform = StepDecomposeQueryTransform(llm, verbose=True)\\n\\nquery_engine = index.as_query_engine()\\nquery_engine = MultiStepQueryEngine(\\n    query_engine, query_transform=step_decompose_transform\\n)\\n\\nresponse = query_engine.query(\\n    \"Who was in the first batch of the accelerator program the author started?\",\\n)\\nprint(str(response))\\n\\nCheck out our example notebook for a full walkthrough.\\n\\nHyDE Query Transform\\n\\nMultistep Query', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='09b4c24c-37f8-4e86-b87e-13ccbd5e5295', embedding=None, metadata={'path': 'C:\\\\Users\\\\AjaykumarKV\\\\Documents\\\\LLM_Cookbooks\\\\LLM_Examples\\\\Llamaindex_cookbooks\\\\RAG\\\\docs.llamaindex.ai\\\\UsagepatternLlamaIndex.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Usage pattern\\n\\nUsage Pattern#\\n\\nDefining a custom prompt#\\n\\nDefining a custom prompt is as simple as creating a format string\\n\\nfrom llama_index.core import PromptTemplate\\n\\ntemplate = (\\n    \"We have provided context information below. \\\\n\"\\n    \"---------------------\\\\n\"\\n    \"{context_str}\"\\n    \"\\\\n---------------------\\\\n\"\\n    \"Given this information, please answer the question: {query_str}\\\\n\"\\n)\\nqa_template = PromptTemplate(template)\\n\\n# you can create text prompt (for completion API)\\nprompt = qa_template.format(context_str=..., query_str=...)\\n\\n# or easily convert to message prompts (for chat API)\\nmessages = qa_template.format_messages(context_str=..., query_str=...)\\n\\nNote: you may see references to legacy prompt subclasses such as QuestionAnswerPrompt, RefinePrompt. These have been deprecated (and now are type aliases of PromptTemplate). Now you can directly specify PromptTemplate(template) to construct custom prompts. But you still have to make sure the template string contains the expected parameters (e.g. {context_str} and {query_str}) when replacing a default question answer prompt.\\n\\nYou can also define a template from chat messages\\n\\nfrom llama_index.core import ChatPromptTemplate\\nfrom llama_index.core.llms import ChatMessage, MessageRole\\n\\nmessage_templates = [\\n    ChatMessage(content=\"You are an expert system.\", role=MessageRole.SYSTEM),\\n    ChatMessage(\\n        content=\"Generate a short story about {topic}\",\\n        role=MessageRole.USER,\\n    ),\\n]\\nchat_template = ChatPromptTemplate(message_templates=message_templates)\\n\\n# you can create message prompts (for chat API)\\nmessages = chat_template.format_messages(topic=...)\\n\\n# or easily convert to text prompt (for completion API)\\nprompt = chat_template.format(topic=...)\\n\\nGetting and Setting Custom Prompts#\\n\\nSince LlamaIndex is a multi-step pipeline, it\\'s important to identify the operation that you want to modify and pass in the custom prompt at the right place.\\n\\nFor instance, prompts are used in response synthesizer, retrievers, index construction, etc; some of these modules are nested in other modules (synthesizer is nested in query engine).\\n\\nSee this guide for full details on accessing/customizing prompts.\\n\\nCommonly Used Prompts#\\n\\nThe most commonly used prompts will be the text_qa_template and the refine_template.\\n\\ntext_qa_template - used to get an initial answer to a query using retrieved nodes\\n\\nrefine_template - used when the retrieved text does not fit into a single LLM call with response_mode=\"compact\" (the default), or when more than one node is retrieved using response_mode=\"refine\". The answer from the first query is inserted as an existing_answer, and the LLM must update or repeat the existing answer based on the new context.\\n\\nAccessing Prompts#\\n\\nYou can call get_prompts on many modules in LlamaIndex to get a flat list of prompts used within the module and nested submodules.\\n\\nFor instance, take a look at the following snippet.\\n\\nquery_engine = index.as_query_engine(response_mode=\"compact\")\\nprompts_dict = query_engine.get_prompts()\\nprint(list(prompts_dict.keys()))\\n\\nYou might get back the following keys:\\n\\n[\\'response_synthesizer:text_qa_template\\', \\'response_synthesizer:refine_template\\']\\n\\nNote that prompts are prefixed by their sub-modules as \"namespaces\".\\n\\nUpdating Prompts#\\n\\nYou can customize prompts on any module that implements get_prompts with the update_prompts function. Just pass in argument values with the keys equal to the keys you see in the prompt dictionary obtained through get_prompts.\\n\\ne.g. regarding the example above, we might do the following\\n\\n# shakespeare!\\nqa_prompt_tmpl_str = (\\n    \"Context information is below.\\\\n\"\\n    \"---------------------\\\\n\"\\n    \"{context_str}\\\\n\"\\n    \"---------------------\\\\n\"\\n    \"Given the context information and not prior knowledge, \"\\n    \"answer the query in the style of a Shakespeare play.\\\\n\"\\n    \"Query: {query_str}\\\\n\"\\n    \"Answer: \"\\n)\\nqa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\\n\\nquery_engine.update_prompts(\\n    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\\n)\\n\\nModify prompts used in query engine#\\n\\nFor query engines, you can also pass in custom prompts directly during query-time (i.e. for executing a query against an index and synthesizing the final response).\\n\\nThere are also two equivalent ways to override the prompts:\\n\\nvia the high-level API\\n\\nquery_engine = index.as_query_engine(\\n    text_qa_template=custom_qa_prompt, refine_template=custom_refine_prompt\\n)\\n\\nvia the low-level composition API\\n\\nretriever = index.as_retriever()\\nsynth = get_response_synthesizer(\\n    text_qa_template=custom_qa_prompt, refine_template=custom_refine_prompt\\n)\\nquery_engine = RetrieverQueryEngine(retriever, response_synthesizer)\\n\\nThe two approaches above are equivalent, where 1 is essentially syntactic sugar for 2 and hides away the underlying complexity. You might want to use 1 to quickly modify some common parameters, and use 2 to have more granular control.\\n\\nFor more details on which classes use which prompts, please visit Query class references.\\n\\nCheck out the reference documentation for a full set of all prompts.\\n\\nModify prompts used in index construction#\\n\\nSome indices use different types of prompts during construction (NOTE: the most common ones, VectorStoreIndex and SummaryIndex, don\\'t use any).\\n\\nFor instance, TreeIndex uses a summary prompt to hierarchically summarize the nodes, and KeywordTableIndex uses a keyword extract prompt to extract keywords.\\n\\nThere are two equivalent ways to override the prompts:\\n\\nvia the default nodes constructor\\n\\nindex = TreeIndex(nodes, summary_template=custom_prompt)\\n\\nvia the documents constructor.\\n\\nindex = TreeIndex.from_documents(docs, summary_template=custom_prompt)\\n\\nFor more details on which index uses which prompts, please visit Index class references.\\n\\n[Advanced] Advanced Prompt Capabilities#\\n\\nIn this section we show some advanced prompt capabilities in LlamaIndex.\\n\\nRelated Guides:\\n\\nAdvanced Prompts\\n\\nPrompt Engineering for RAG\\n\\nPartial Formatting#\\n\\nPartially format a prompt, filling in some variables while leaving others to be filled in later.\\n\\nfrom llama_index.core import PromptTemplate\\n\\nprompt_tmpl_str = \"{foo} {bar}\"\\nprompt_tmpl = PromptTemplate(prompt_tmpl_str)\\npartial_prompt_tmpl = prompt_tmpl.partial_format(foo=\"abc\")\\n\\nfmt_str = partial_prompt_tmpl.format(bar=\"def\")\\n\\nTemplate Variable Mappings#\\n\\nLlamaIndex prompt abstractions generally expect certain keys. E.g. our text_qa_prompt expects context_str for context and query_str for the user query.\\n\\nBut if you\\'re trying to adapt a string template for use with LlamaIndex, it can be annoying to change out the template variables.\\n\\nInstead, define template_var_mappings:\\n\\ntemplate_var_mappings = {\"context_str\": \"my_context\", \"query_str\": \"my_query\"}\\n\\nprompt_tmpl = PromptTemplate(\\n    qa_prompt_tmpl_str, template_var_mappings=template_var_mappings\\n)\\n\\nFunction Mappings#\\n\\nPass in functions as template variables instead of fixed values.\\n\\nThis is quite advanced and powerful; allows you to do dynamic few-shot prompting, etc.\\n\\nHere\\'s an example of reformatting the context_str.\\n\\ndef format_context_fn(**kwargs):\\n    # format context with bullet points\\n    context_list = kwargs[\"context_str\"].split(\"\\\\n\\\\n\")\\n    fmtted_context = \"\\\\n\\\\n\".join([f\"- {c}\" for c in context_list])\\n    return fmtted_context\\n\\n\\nprompt_tmpl = PromptTemplate(\\n    qa_prompt_tmpl_str, function_mappings={\"context_str\": format_context_fn}\\n)\\n\\nprompt_tmpl.format(context_str=\"context\", query_str=\"query\")', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Advanced Retrieval Strategies#\\n\\nMain Advanced Retrieval Strategies#\\n\\nThere are a variety of more advanced retrieval strategies you may wish to try, each with different benefits:\\n\\nReranking\\n\\nRecursive retrieval\\n\\nEmbedded tables\\n\\nSmall-to-big retrieval\\n\\nSee our full retrievers module guide for a comprehensive list of all retrieval strategies, broken down into different categories.\\n\\nBasic retrieval from each index\\n\\nAdvanced retrieval and search\\n\\nAuto-Retrieval\\n\\nKnowledge Graph Retrievers\\n\\nComposed/Hierarchical Retrievers\\n\\nand more!\\n\\nMore resources are below.\\n\\nQuery Transformations#\\n\\nA user query can be transformed before it enters a flow (query engine, agent, and more). See resources below on query transformations:\\n\\nQuery Transform Cookbook\\n\\nQuery Transformations Docs\\n\\nComposable Retrievers#\\n\\nEvery retriever is capable of retrieving and running other objects, including\\n\\nother retrievers\\n\\nquery engines\\n\\nquery pipelines\\n\\nother nodes\\n\\nFor more details, check out the guide below.\\n\\nComposable Retrievers\\n\\nThird-Party Resources#\\n\\nHere are some third-party resources on advanced retrieval strategies.\\n\\nDeepMemory (Activeloop)\\n\\nWeaviate Hybrid Search\\n\\nPinecone Hybrid Search\\n\\nMilvus Hybrid Search'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e4d49e4634429dbdebb0a22aa0c2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "node_parser = SentenceSplitter()\n",
    "for idx, doc in enumerate(tqdm(docs)):\n",
    "        nodes = node_parser.get_nodes_from_documents([doc])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\",api_base=\"\")\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\", embed_batch_size=256\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
