{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Output with Groq and Instructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Large Language Models (LLMs) are often employed for building chatbots or conversational agents, numerous real-world applications require a different approach - one that goes beyond mere dialogue and involves producing structured, machine-readable outputs.\n",
    "\n",
    "Consider a typical scenario: we want to produce structured JSON data from an LLM. While tools like Python's json module allow us to handle this data, they also come with their own set of challenges, such as validating data types and ensuring consistency across outputs. Manually checking these aspects can be tedious and error-prone. LLMs also tend to forget to include a comma or a closing bracket ('}') somewhere in the produced JSON from time to time, which would invalidate the whole JSON output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A Very Simple Use Case\n",
    "Let's dive right into how you can set up the instructor library with models powered by Groq to generate structured JSON outputs. We'll keep it simple and straightforward so you can get up and running quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Dive into Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing the Necessary Libraries\n",
    "Install the required Python libraries. You'll need:\n",
    "<ul>\n",
    "<li>groq </li>\n",
    "<li>instructor </li>\n",
    "<li>python-dotenv (for loading environment variables) </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting instructor\n",
      "  Downloading instructor-1.7.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from instructor) (3.10.9)\n",
      "Collecting docstring-parser<1.0,>=0.16 (from instructor)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /home/codespace/.local/lib/python3.12/site-packages (from instructor) (3.1.4)\n",
      "Collecting jiter<0.9,>=0.6.1 (from instructor)\n",
      "  Downloading jiter-0.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.52.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from instructor) (1.59.6)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from instructor) (2.23.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from instructor) (2.9.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /home/codespace/.local/lib/python3.12/site-packages (from instructor) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from instructor) (13.9.4)\n",
      "Collecting tenacity<10.0.0,>=9.0.0 (from instructor)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from instructor) (0.15.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.13.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.4->instructor) (2.1.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from openai<2.0.0,>=1.52.0->instructor) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai<2.0.0,>=1.52.0->instructor) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.12/site-packages (from openai<2.0.0,>=1.52.0->instructor) (0.27.2)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from openai<2.0.0,>=1.52.0->instructor) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai<2.0.0,>=1.52.0->instructor) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai<2.0.0,>=1.52.0->instructor) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.8.0->instructor) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.3->instructor) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.3->instructor) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.3->instructor) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.3->instructor) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich<14.0.0,>=13.7.0->instructor) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich<14.0.0,>=13.7.0->instructor) (2.18.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.9.0->instructor) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.9.0->instructor) (1.5.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->instructor) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->instructor) (0.14.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor) (0.1.2)\n",
      "Downloading instructor-1.7.2-py3-none-any.whl (71 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading jiter-0.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: tenacity, jiter, docstring-parser, instructor\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.5.0\n",
      "    Uninstalling tenacity-8.5.0:\n",
      "      Successfully uninstalled tenacity-8.5.0\n",
      "  Attempting uninstall: jiter\n",
      "    Found existing installation: jiter 0.5.0\n",
      "    Uninstalling jiter-0.5.0:\n",
      "      Successfully uninstalled jiter-0.5.0\n",
      "\u001b[33m  WARNING: The script instructor is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-core 0.11.16 requires tenacity!=8.4.0,<9.0.0,>=8.2.0, but you have tenacity 9.0.0 which is incompatible.\n",
      "llama-index-readers-file 0.2.2 requires pypdf<5.0.0,>=4.0.1, but you have pypdf 5.1.0 which is incompatible.\n",
      "llama-index-legacy 0.9.48.post3 requires tenacity<9.0.0,>=8.2.0, but you have tenacity 9.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed docstring-parser-0.16 instructor-1.7.2 jiter-0.8.2 tenacity-9.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U  instructor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: John Doe\n",
      "Age: 35\n",
      "Email: johndoe@example.com\n"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from groq import Groq\n",
    "\n",
    "# Load the Groq API key from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Describe the desired output schema using pydantic models\n",
    "class UserInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    email: str\n",
    "\n",
    "# The text to extract data from\n",
    "text = \"\"\"\n",
    "Ajay, a 21-year-old software engineer from New York, has been working with large language models for several years.\n",
    "His email address is johndoe@example.com.\n",
    "\"\"\"\n",
    "\n",
    "# Patch Groq() with instructor, this is where the magic happens!\n",
    "client = instructor.from_groq(Groq(), mode=instructor.Mode.JSON)\n",
    "\n",
    "# Call the API\n",
    "user_info = client.chat.completions.create(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    response_model=UserInfo, # Specify the response model\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Your job is to extract user information from the given text.\"},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ],\n",
    "    temperature=0.65,\n",
    ")\n",
    "\n",
    "print(f\"Name: {user_info.name}\")\n",
    "print(f\"Age: {user_info.age}\")\n",
    "print(f\"Email: {user_info.email}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A more complex usecase\n",
    "\n",
    "Our goal is to create a structured dataset of realistic examples that simulate how a user might request weather information in various scenarios. We want to use a large language model (LLM) to generate these examples for us and use them as an evaluation set to test our agent's capabilities. Without such an evaluation, we lack a way to understand the effects of our prompt adjustments. These examples will not only help us evaluate the agent's ability to use the get_weather_info tool correctly but also make it easy to detect if any prompt changes have negative effects.\n",
    "\n",
    "Now, let's use the instructor library with Groq to generate synthetic examples for our weather agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import instructor\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from groq import Groq\n",
    "\n",
    "# Load the Groq API key from .env file\n",
    "load_dotenv()\n",
    "\n",
    "prompt = \"\"\"\n",
    "I am designing a weather agent. This agent can talk to the user and also fetch latest weather information.\n",
    "It has access to the `get_weather_info` tool with the following JSON schema:\n",
    "{json_schema}\n",
    "\n",
    "I want you to write some examples for `get_weather_info` and see if this functionality works correctly and can handle all the cases. \n",
    "Now given the information so far and the JSON schema of the provided tool, write {num} examples.\n",
    "Make sure each example is varied enough to cover common ways of requesting for this functionality.\n",
    "Make sure you fill the function parameters with the correct types when generating the output examples. \n",
    "Make sure your output is valid JSON.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example(BaseModel):\n",
    "    input_text: str = Field(description=\"The example text\")\n",
    "    tool_name: str = Field(description=\"The tool name to call for this example\")\n",
    "    tool_parameters: str = Field(description=\"An object containing the key-value pairs for the parameters of this tool as a JSON serializbale STRING, make sure it is valid JSON and parameter values are of the correct type according to the tool schema\")\n",
    "\n",
    "class ResponseModel(BaseModel):\n",
    "    examples: list[Example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.ResponseModel'>\n",
      "[Example(input_text=\"What's the weather like in New York?\", tool_name='get_weather_info', tool_parameters='{\"location\": \"New York\"}'),\n",
      " Example(input_text=\"I'm going to London tomorrow, what's the weather forecast?\", tool_name='get_weather_info', tool_parameters='{\"location\": \"London\"}'),\n",
      " Example(input_text='Can you tell me the current weather in Paris?', tool_name='get_weather_info', tool_parameters='{\"location\": \"Paris\"}'),\n",
      " Example(input_text=\"What's the weather like in Sydney, Australia?\", tool_name='get_weather_info', tool_parameters='{\"location\": \"Sydney\"}'),\n",
      " Example(input_text=\"I'm planning a trip to Tokyo, what's the weather forecast?\", tool_name='get_weather_info', tool_parameters='{\"location\": \"Tokyo\"}')]\n"
     ]
    }
   ],
   "source": [
    "# The schema for get_weather_info tool\n",
    "tool_schema = {\n",
    "    \"name\": \"get_weather_info\",\n",
    "    \"description\": \"Get the weather information for any location.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The location for which we want to get the weather information (e.g. New York)\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Patch Groq() with instructor, this is where the magic happens!\n",
    "client = instructor.from_groq(Groq(), mode=instructor.Mode.JSON)\n",
    "\n",
    "# Call the API with our custom prompt and ResponseModel\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    response_model=ResponseModel, # Specify the response model\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": prompt.format(json_schema=tool_schema, num=5), # Pass the tool schema and number of examples to the prompt\n",
    "        },\n",
    "    ],\n",
    "    temperature=0.65,\n",
    "    max_tokens=8000,\n",
    ")\n",
    "\n",
    "print(type(response))\n",
    "pprint(response.examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "So by this way we can get the structured output from the model.\n",
    "And also using instructor library we can make a data validation to adhere the response as user wanted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
