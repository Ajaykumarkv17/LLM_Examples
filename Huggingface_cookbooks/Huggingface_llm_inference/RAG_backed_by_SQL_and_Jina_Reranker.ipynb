{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with SQL Database, Jina AI Reranker, and Hugging Face LLM using LlamaIndex\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval Augmented Generation (RAG) pipeline using LlamaIndex, where:\n",
    "1.  **Data Source:** The knowledge base is stored in a SQL database (SQLite in this example).\n",
    "2.  **Retrieval:** LlamaIndex retrieves relevant information from the SQL database based on a user query.\n",
    "3.  **Reranking:** `JinaAIRerank` is used to rerank the retrieved documents for better relevance and quality before passing them to the language model.\n",
    "4.  **Generation:** A Hugging Face LLM (via `llama-index-llms-huggingface`) generates the final answer based on the reranked, retrieved context.\n",
    "\n",
    "This approach allows you to leverage structured data in SQL databases for your RAG system and improve its performance with sophisticated reranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU sqlalchemy transformers einops llama-index llama-index-postprocessor-jinaai-rerank llama-index-llms-huggingface \"huggingface_hub[inference]\" llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Set Up API Keys\n",
    "\n",
    "You'll need API keys for Jina AI (for reranking) and Hugging Face Hub (if you are using gated models or want to track usage, though many open models don't strictly require a key for download/inference via `HuggingFaceLLM`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Optional: Set up logging for LlamaIndex\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG) # DEBUG level for detailed logs\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# --- API Key Setup ---\n",
    "# Jina AI API Key for JinaAIRerank\n",
    "# Replace 'YOUR_JINA_API_KEY' with your actual Jina AI API key.\n",
    "os.environ['JINA_API_KEY'] = 'YOUR_JINA_API_KEY' \n",
    "\n",
    "# Hugging Face API Key (Token)\n",
    "# Replace 'YOUR_HUGGINGFACE_API_KEY' with your actual Hugging Face Hub token.\n",
    "# This is often needed for downloading models or using the Inference API.\n",
    "os.environ['HUGGINGFACE_API_KEY'] = 'YOUR_HUGGINGFACE_API_KEY'\n",
    "\n",
    "# --- LlamaIndex Core Imports ---\n",
    "from llama_index.core import ( # Corrected import path\n",
    "    SQLDatabase,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    Document,\n",
    "    QueryBundle,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter # Corrected import path\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine # Corrected import path\n",
    "from llama_index.core.retrievers import SQLRetriever # Corrected import path\n",
    "\n",
    "# --- LLM and Reranker Imports ---\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM # Corrected import path\n",
    "from llama_index.postprocessor.jinaai_rerank import JinaAIRerank # Corrected import path\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding # Corrected import path\n",
    "\n",
    "# --- Database Imports ---\n",
    "from sqlalchemy import create_engine, text, Column, Integer, String, MetaData, Table\n",
    "\n",
    "# Check if API keys are set (optional, for user feedback)\n",
    "if os.environ.get('JINA_API_KEY') == 'YOUR_JINA_API_KEY' or not os.environ.get('JINA_API_KEY'):\n",
    "    print(\"JINA_API_KEY is not set or is using the placeholder. JinaRerank might not work.\")\n",
    "if os.environ.get('HUGGINGFACE_API_KEY') == 'YOUR_HUGGINGFACE_API_KEY' or not os.environ.get('HUGGINGFACE_API_KEY'):\n",
    "    print(\"HUGGINGFACE_API_KEY is not set or is using the placeholder. Model downloads might be affected for certain models.\")\n",
    "\n",
    "print(\"Libraries imported and API key placeholders noted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Up SQLite Database and Populate with Data\n",
    "\n",
    "We'll create an in-memory SQLite database and a table named `documents` containing some text data that our RAG pipeline will query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SQLite database engine (in-memory)\n",
    "engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata = MetaData()\n",
    "\n",
    "# Define the 'documents' table\n",
    "documents_table = Table(\n",
    "    'documents',\n",
    "    metadata,\n",
    "    Column('id', Integer, primary_key=True, autoincrement=True),\n",
    "    Column('title', String(255)),\n",
    "    Column('content', String)\n",
    ")\n",
    "metadata.create_all(engine)\n",
    "\n",
    "# Sample data to insert\n",
    "sample_docs_data = [\n",
    "    {'title': 'Alpacas', 'content': 'Alpacas are South American camelids, smaller than llamas. They are known for their soft fleece.'},\n",
    "    {'title': 'Llamas', 'content': 'Llamas are also South American camelids, often used as pack animals. They are larger than alpacas and have banana-shaped ears.'},\n",
    "    {'title': 'Climate Change', 'content': 'Climate change refers to long-term shifts in temperatures and weather patterns, largely driven by human activities, especially the burning of fossil fuels.'},\n",
    "    {'title': 'Photosynthesis', 'content': 'Photosynthesis is the process used by plants, algae, and some bacteria to convert light energy into chemical energy, through a process that uses sunlight, water, and carbon dioxide.'},\n",
    "    {'title': 'Artificial Intelligence', 'content': 'Artificial intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, and self-correction.'}\n",
    "]\n",
    "\n",
    "# Insert data into the table\n",
    "with engine.connect() as connection:\n",
    "    for doc_data in sample_docs_data:\n",
    "        stmt = documents_table.insert().values(**doc_data)\n",
    "        connection.execute(stmt)\n",
    "    connection.commit()\n",
    "\n",
    "# Verify data insertion\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT title, content FROM documents\")).fetchall()\n",
    "    print(f\"Inserted {len(result)} documents into the SQLite database:\")\n",
    "    for row in result:\n",
    "        print(f\"  Title: {row[0]}, Content: {row[1][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connect LlamaIndex to the SQL Database\n",
    "\n",
    "We use LlamaIndex's `SQLDatabase` class to interface with our SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_database = SQLDatabase(engine, include_tables=[\"documents\"]) # Specify the table to consider\n",
    "print(\"LlamaIndex SQLDatabase initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure LLM, Embedding Model, and Reranker\n",
    "\n",
    "We'll set up the Hugging Face LLM for generation, a Hugging Face embedding model for retrieval, and the Jina AI Reranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configure LLM (Hugging Face) ---\n",
    "# Using a smaller, well-known model for faster demonstration.\n",
    "# Make sure you have accepted the terms for models like Llama-2 if you choose them.\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"google/flan-t5-small\", # A good small model for demonstration\n",
    "    # model_name=\"meta-llama/Llama-2-7b-chat-hf\", # Example for a larger model\n",
    "    tokenizer_name=\"google/flan-t5-small\",\n",
    "    query_wrapper_prompt=\"Question: {query_str}\\nAnswer:\", # Adjust based on model needs\n",
    "    context_window=2048, # Max input tokens for the model\n",
    "    max_new_tokens=256,  # Max tokens to generate\n",
    "    model_kwargs={\"torch_dtype\": \"auto\"}, # Use \"auto\" or torch.float16 for GPU\n",
    "    # generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    device_map=\"auto\" # Automatically select device (CPU/GPU)\n",
    ")\n",
    "\n",
    "# --- Configure Embedding Model (Hugging Face) ---\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\" # A good default embedding model\n",
    ")\n",
    "\n",
    "# --- Configure Reranker (Jina AI) ---\n",
    "jina_rerank = JinaAIRerank(\n",
    "    api_key=os.environ.get('JINA_API_KEY'), \n",
    "    model=\"jina-reranker-v1-base-en\", # Specify the Jina reranker model\n",
    "    top_n=3  # Rerank and return top 3 documents\n",
    ")\n",
    "\n",
    "# --- Set up ServiceContext / Settings ---\n",
    "# LlamaIndex has moved towards using global Settings for LLM, embed_model, etc.\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512 # Optional: configure chunk size for text splitting\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "\n",
    "print(\"LLM, Embedding Model, and Reranker configured.\")\n",
    "if not os.environ.get('JINA_API_KEY') or os.environ.get('JINA_API_KEY') == 'YOUR_JINA_API_KEY':\n",
    "    print(\"Warning: Jina API key is not set. Reranking will likely fail.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement RAG Pipeline from SQL Data\n",
    "\n",
    "For a RAG pipeline over SQL data where we want to apply reranking to text content, we'll first fetch the data from SQL, convert it into LlamaIndex `Document` objects, and then build a `VectorStoreIndex`. This allows us to use standard LlamaIndex retrievers and node postprocessors (like the reranker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from SQL and convert to LlamaIndex Documents\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT id, title, content FROM documents\")).fetchall()\n",
    "\n",
    "llama_documents = []\n",
    "for row_id, title, content in result:\n",
    "    doc = Document(\n",
    "        text=content, \n",
    "        metadata={'title': title, 'doc_id': row_id} # Add title and original ID as metadata\n",
    "    )\n",
    "    llama_documents.append(doc)\n",
    "\n",
    "print(f\"Converted {len(llama_documents)} SQL rows into LlamaIndex Document objects.\")\n",
    "\n",
    "# Build VectorStoreIndex from these documents\n",
    "# This will use the global Settings.embed_model for embeddings\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    llama_documents,\n",
    "    # service_context=ServiceContext.from_defaults(llm=llm, embed_model=embed_model) # Old way\n",
    "    # Settings are now used globally, so explicit service_context might not be needed here if already set\n",
    ")\n",
    "print(\"VectorStoreIndex built from SQL documents.\")\n",
    "\n",
    "# --- Construct Query Engine with Retriever and Reranker ---\n",
    "retriever = index.as_retriever(similarity_top_k=5) # Retrieve top 5 most similar documents initially\n",
    "\n",
    "# The query engine will use the global Settings.llm\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[jina_rerank] # Add Jina reranker here\n",
    "    # service_context=ServiceContext.from_defaults(llm=llm) # Old way\n",
    ")\n",
    "\n",
    "print(\"Query Engine with Jina Reranker created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Example Queries\n",
    "\n",
    "Let's test the RAG pipeline with a few queries. The Jina AI API key must be correctly set for the reranker to work. If not, this step might fail or the reranker might be skipped if it handles errors gracefully (often it will raise an error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query_str):\n",
    "    print(f\"\\n--- Query: {query_str} ---\")\n",
    "    try:\n",
    "        response = query_engine.query(query_str)\n",
    "        print(\"\\nResponse:\")\n",
    "        print(response)\n",
    "        \n",
    "        print(\"\\nSource Nodes (after reranking, if any):\")\n",
    "        for i, node in enumerate(response.source_nodes):\n",
    "            print(f\"  Node {i+1}: Score = {node.score:.4f}, Title = {node.metadata.get('title', 'N/A')}\")\n",
    "            print(f\"    Content: {node.text[:100]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during query processing: {e}\")\n",
    "        print(\"This might be due to an incorrect or missing JINA_API_KEY or HUGGINGFACE_API_KEY, or issues with model loading.\")\n",
    "\n",
    "# Example Query 1: About Alpacas\n",
    "run_query(\"Tell me about alpacas and their characteristics.\")\n",
    "\n",
    "# Example Query 2: Comparing Alpacas and Llamas\n",
    "run_query(\"How are alpacas different from llamas?\")\n",
    "\n",
    "# Example Query 3: About AI\n",
    "run_query(\"What is artificial intelligence?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This notebook demonstrated building a RAG pipeline that sources its information from a SQL database. Key steps included:\n",
    "*   Setting up a SQLite database with sample textual data.\n",
    "*   Fetching this data and converting it into LlamaIndex `Document` objects.\n",
    "*   Building a `VectorStoreIndex` on these documents to enable semantic retrieval.\n",
    "*   Configuring a Hugging Face LLM for text generation and a Hugging Face model for embeddings.\n",
    "*   Integrating the `JinaAIRerank` postprocessor to refine the retrieved results before they are passed to the LLM.\n",
    "*   Creating a LlamaIndex `RetrieverQueryEngine` that combines these components.\n",
    "\n",
    "**Important Considerations:**\n",
    "*   **API Keys:** Ensure your `JINA_API_KEY` and `HUGGINGFACE_API_KEY` (if needed for your chosen model) are correctly set up as environment variables.\n",
    "*   **Model Selection:** The choice of LLM and embedding model can significantly impact performance and resource requirements. The `google/flan-t5-small` and `all-MiniLM-L6-v2` models are chosen here for their relatively small size and speed, making them suitable for demonstration.\n",
    "*   **Data Scale:** For very large SQL databases, consider strategies like incremental indexing, more sophisticated SQL querying to pre-filter relevant data, or using LlamaIndex's `SQLTableRetrieverQueryEngine` if you want to perform natural language to SQL translation first, then RAG on the results.\n",
    "*   **Error Handling:** The Jina reranker will likely raise an error if the API key is invalid or missing. Robust applications should include proper error handling.\n",
    "*   **LlamaIndex Updates:** LlamaIndex is a rapidly evolving library. Some class names, import paths, or methods (like `ServiceContext` usage vs. global `Settings`) might change. Always refer to the latest LlamaIndex documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
