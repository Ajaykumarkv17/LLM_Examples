{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key features of Large Language Models (LLMs) is their context windowâ€”the maximum number of tokens they can process in a single request. As LLMs evolve, their context windows are becoming increasingly larger.\n",
    "\n",
    "Larger context windows unlock incredible possibilities:\n",
    "\n",
    "- **In-context retrieval**: Seamlessly referencing large amounts of text within a single query.\n",
    "- **In-context learning**: Adapting behavior to specific examples within the same session.\n",
    "- **Extended reasoning**: Handling very long chains of thought without breaking context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, these extended windows come with challenges:\n",
    "\n",
    "- Large memory requirements for KV Cache\n",
    "- Example: 1M tokens with Llama 3-70B (float16) needs 330GB\n",
    "- Makes deployment infeasible for many applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll address one solution for this problem: compressing the KV Cache for more efficient generation. To achieve this, we'll explore:\n",
    "\n",
    "- What the KV Cache is and why it matters.\n",
    "- KVPress, a powerful toolkit from NVIDIA designed to compress KV Cache effectively.\n",
    "- The inner workings of KVPress and how it achieves compression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In autoregressive models, text generation is sequential, with each prediction depending on all previous tokens. The model:\n",
    "\n",
    "- Must process tokens 1-999 to generate token 1000\n",
    "- Needs to reprocess the same information plus token 1000 for token 1001\n",
    "\n",
    "This becomes inefficient at scale. KV Cache solves this by storing intermediate attention layer results (keys and values), enabling reuse instead of recalculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from kvpress import ExpectedAttentionPress\n",
    "\n",
    "pipe = pipeline(\n",
    "\"kv-press-text-generation\",\n",
    "model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "device=\"cuda\",\n",
    "model_kwargs={\"attn_implementation\": \"sdpa\"}\n",
    ")\n",
    "\n",
    "context = \"A very long text you want to compress once and for all\"\n",
    "question = \"\\nA question about the compressed context\"  # optional\n",
    "\n",
    "press = ExpectedAttentionPress(compression_ratio=0.5)\n",
    "answer = pipe(context, question=question, press=press)[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using it directly in this [Hugging Face space](https://huggingface.co/spaces/nvidia/kvpress) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The growing context windows of LLMs enable new capabilities but face memory challenges due to KV Cache scaling. KVPress offers a solution through cache compression during pre-filling.\n",
    "\n",
    "Key points:\n",
    "- Higher compression ratios trade memory savings for accuracy\n",
    "- Seamless integration with transformers library \n",
    "- Modular design enables custom compression techniques\n",
    "- Makes long-context LLMs more accessible and deployable\n",
    "\n",
    "KVPress represents an important step toward efficient scaling of LLMs while managing memory constraints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
